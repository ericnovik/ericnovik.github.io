---
title: "Bayesian Inference"
subtitle: "NYU Applied Statistics for Social Science Research"
author: "Eric Novik | Spring 2024 | Lecture 4"
format:
  revealjs: 
    theme: ../../custom.scss
    scrollable: true
    slide-number: true
    chalkboard: 
      buttons: false
      chalk-width: 5
    preview-links: auto
    footer: <https://ericnovik.github.io/bayes-course.html>
editor: source
always_allow_html: true
---


##  MCMC, Posterior inference, and Prediction

::: incremental
- Metropolis-Hastings-Rosenbluth algorithm
- R implementation and testing
- Computational issues
- Why does MHR work
- Posterior predictive distribution
- Posterior predictive simulation in Stan
- Optimization and code breaking with MCMC
:::

```{r}
library(ggplot2)
library(dplyr)
library(janitor)
library(gridExtra)
library(purrr)
library(bayesrules)

thm <-
  theme_minimal() + theme(
    panel.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    plot.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    panel.grid.major = element_blank()
  )
theme_set(thm)

dbinom_theta <- function(theta, N, y) {
  choose(N, y) * theta^y * (1 - theta)^(N - y) 
}

dot_plot <- function(x, y, yc = NULL, dodge = 0.2) {
  x <- as.factor(x)
  p <- ggplot(data.frame(x, y), aes(x, y))
  p <- p + geom_point(aes(x = x, y = y), size = 0.5) +
    geom_segment(aes(
      x = x,
      y = 0,
      xend = x,
      yend = y
    ), linewidth = 0.2) +
    xlab(expression(theta)) + ylab(expression(f(theta)))
  
  if (!is.null(yc)) {
    xc <- as.numeric(x) + dodge
    p1 <- p + geom_point(aes(x = xc, y = yc), color = 'red', size = 0.5) +
      geom_segment(aes(
        x = xc,
        y = 0,
        xend = xc,
        yend = yc
      ), color = 'red', linewidth = 0.2)
    return(p1)
  }
  return(p)
}

plot_binom_lik <- function(N, y, MLE = TRUE) {
  theta <- seq(0, 1, len = 100)
  lik <- dbinom(y, N, theta)
  d <- data.frame(theta, lik)
  p <- ggplot(d, aes(theta, lik)) + geom_line(linewidth = 0.1) 
  if (MLE) {
    p <- p + 
      geom_point(x = y/N, y = 0, color = 'red', size = 0.5) +
      geom_point(x = y/N, y = dbinom_theta(y/N, N, y), 
                 color = 'blue', alpha = 0.5, size = 0.5) +
      geom_vline(xintercept = y/N, linewidth = 0.1, linetype = "dashed")
  }
  return(p)
}

plot_beta <- function(a, b, MLE = TRUE) {
  theta <- seq(0, 1, len = 100)
  lik <- dbeta(theta, a, b)
  d <- data.frame(theta, lik)
  p <- ggplot(d, aes(theta, lik)) + geom_line(linewidth = 0.1) 
  if (MLE) {
    mode <- (a - 1) / (a + b - 2)
    p <- p + 
      geom_point(x = mode, y = 0, color = 'red', size = 0.5) +
      geom_point(x = mode, y = dbeta(mode, a, b), 
                 color = 'blue', alpha = 0.5, size = 0.5) +
      geom_vline(xintercept = mode, linewidth = 0.1, linetype = "dashed")
  }
  return(p)
}

plot_gamma_pdf <- function(a, b, xlim = c(0, 10), MLE = TRUE) {
  y <- seq(xlim[1], xlim[2], len = 100)
  dens <- dgamma(y, shape = a, rate = b)
  d <- data.frame(y, dens)
  p <- ggplot(d, aes(y, dens)) + geom_line(linewidth = 0.1) 
  if (MLE) {
    mode <- (a - 1) / b
    p <- p + 
      geom_point(x = mode, y = 0, color = 'red', size = 0.5) +
      geom_point(x = mode, y = dgamma(mode, a, b), 
                 color = 'blue', alpha = 0.5, size = 0.5) +
      geom_vline(xintercept = mode, linewidth = 0.1, linetype = "dashed") + 
      xlim(c(xlim[1], xlim[2]))
  }
  return(p)
}

add_bin_vsegment <- function(p, N, y, theta, ...) {
  p + geom_segment(x = theta, y = 0, xend = theta, 
                   yend = dbinom(y, N, theta), 
                   linewidth = 0.1, ...) +
    geom_point(x = theta, y = 0, color = 'red', size = 0.5, ...)
}

post_beta_var <- function(a, b, n, y) {
  (a + y) * (b + n - y) / ((a + b + n)^2 * (a + b + n + 1))
}

normal_normal_post <- function(y, sd, prior_mu, prior_sd) {
# for the case where sd is known
  prior_prec <- 1/prior_sd^2 
  data_prec <- 1/sd^2
  n <- length(y)
  post_mu <- (prior_prec * prior_mu + data_prec * n * mean(y)) /
             (prior_prec + n * data_prec)
  post_prec <- prior_prec + n * data_prec
  post_sd <- sqrt(1/post_prec)
  return(list(post_mu = post_mu, post_sd = post_sd))
}
```

$$
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\L}{\mathcal{L}}
\DeclareMathOperator{\I}{\text{I}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

## High-Level Outline of MHR {.smaller}

:::: {.columns}

::: {.column width="50%"}
::: {.incremental}
- The idea is to spend "more time" in the area of high posterior volume
- Pick a random starting point at $i=1$ with $\theta^{(1)}$
- **Propose** a next possible value of $\theta$, call it  $\theta'$ from an (easy) proposal distribution
- Evaluate if you should **accept or reject** the proposal
- If accepted, go to $\theta^{'(2)}$, otherwise stay at $\theta^{(2)} = \theta^{(1)}$
- Rinse and repeat
:::

:::

::: {.column width="50%"}
::: {.incremental}
- If we can get an independant draw from $\theta$, we just take it
- That amounts to regular Monte Carlo sampling, like `rnorm(1, mean = 0, sd = 1)`
- Otherwise, we need a **rule** for evaluating when to accept and when to reject the proposal
- We still need a way to **evaluate the density** at the proposed value (it will be part of the rule)
- The big idea: if the proposed $\theta'$ has a higher plausibility than the current $\theta$, accept $\theta'$; if not, sometimes accept $\theta'$
:::

:::

::::

## Normal-Normal with Known $\sigma$ {.smaller}
::: {.incremental}
- We will start with a known posterior
- Let $y \sim \text{Normal}(\mu, 0.5)$
- Let the prior $\mu \sim \text{Normal}(0, 2)$
- Let's say we observe $y = 5$
- We can immediately compute the posterior $\mu \mid y \sim \text{Normal}(4.71, 0.49)$
:::

::: {.fragment}
```{r}
#| echo: true
normal_normal_post <- function(y, sd, prior_mu, prior_sd) {
# for the case where sd is known
  prior_prec <- 1/prior_sd^2 
  data_prec <- 1/sd^2
  n <- length(y)
  post_mu <- (prior_prec * prior_mu + data_prec * n * mean(y)) /
             (prior_prec + n * data_prec)
  post_prec <- prior_prec + n * data_prec
  post_sd <- sqrt(1/post_prec)
  return(list(post_mu = post_mu, post_sd = post_sd))
}

normal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2) |>
  unlist() |> round(2)
```
:::

## Metropolis-Hastings-Rosenbluth (MHR) {.smaller}

::: panel-tabset
## MHR
::: {.incremental}
- We are trying to generate draws: $\left( \theta^{(1)}, \theta^{(2)}, ..., \theta^{(N)}\right)$ implied by the target density $f$  
- Pick the first value of $\theta$ randomly or deterministically
- Draw $\theta'$ from a proposal distribution $q(\theta'\mid\theta)$
- Compute the unnormalized $f(\theta'\mid y) \propto f(y \mid \theta') f(\theta') = f_{\text{prop}}$
- Compute the unnormalized $f(\theta \mid y) \propto f(y \mid \theta) f(\theta) = f_{\text{current}}$
- Compute $\text{ratio}_f = \frac{f_{\text{prop}}}{f_{\text{current}}}$ and $\text{ratio}_q = \frac{q(\theta\mid\theta')}{q(\theta'\mid\theta)}$
- Asseptance probability $\alpha = \min\left\lbrace 1, \text{ratio}_f \cdot \text{ratio}_q \right\rbrace$
- If $q$ is symmetric, we can drop $\text{ratio}_q$, in which case $\alpha = \min\left\lbrace 1, \text{ratio}_f \right\rbrace$
- If $\text{ratio} \geq 1$, accept $\theta'$, else flip a coin with $\text{Pr}(X=1) = \alpha$ and accept $\theta'$ if $X=1$, stay with current $\theta$, if $X=0$
:::

## Normal Symmetry

::: incremental
- Why in case of normals, $q(\theta'\mid\theta) = q(\theta\mid\theta')$
:::

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 8
#| fig-height: 3
#| fig-align: center
#| echo: false
y <- seq(-3, 3, len = 100)
dy <- dnorm(y, mean = 0)
p1 <- ggplot(aes(y, dy), data = data.frame(y, dy)) +
  geom_line(linewidth = 0.2) +
  ggtitle("q(1|0) = dnorm(1, 0)") + xlab("") + ylab("") 
p1 <- p1 + geom_segment(x = 1, xend = 1, y = 0, yend = dnorm(1, 0), 
                  linewidth = 0.2, linetype = "dashed") +
  geom_point(x = 1, y = dnorm(1, 0), color = 'red')

y <- seq(-2, 4, len = 100)
dy <- dnorm(y, mean = 1)
p2 <- ggplot(aes(y, dy), data = data.frame(y, dy)) +
  geom_line(linewidth = 0.2) +
  ggtitle("q(0|1) = dnorm(0, 1)") + xlab("") + ylab("") 
p2 <- p2 + geom_segment(x = 0, xend = 0, y = 0, yend = dnorm(0, 1), 
                  linewidth = 0.2, linetype = "dashed") +
  geom_point(x = 0, y = dnorm(0, 1), color = 'red')

grid.arrange(p1, p2, nrow = 1)
```
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
dnorm(1, mean = 0)
dnorm(0, mean = 1)
```
:::

:::


## MHR in R {.smaller}

::: panel-tabset
## MHR Iteration
```{r}
#| echo: true
#| code-line-numbers: "|6|7|8|9|10|11|12|14|15|17|18|19|20|"

metropolis_iteration_1 <- function(y, current_mean, proposal_scale, 
                                   sd, prior_mean, prior_sd) {
# assume prior ~ N(prior_mean, prior_sd) and sd is known
# proposal sampling distribution q = N(current_mean, proposal_scale)

  proposal_mean <- rnorm(n = 1, mean = current_mean,  sd = proposal_scale) # q(mu' | mu)  
  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')
                         sd = prior_sd) *                  
                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')
  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)
                         sd = prior_sd) *        
                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)

  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]
  alpha <- min(ratio, 1)     
  
  if (alpha > runif(1)) {         # this is just another way of flipping a coint
    next_value <- proposal_mean  
  } else {
    next_value <- current_mean
  }
  return(next_value)
}
```

## MHR for N Iterations
```{r}
#| echo: true
#| code-line-numbers: "|8|9|11|12|18|19|"

mhr <- function(y, f, N, start, ...) {
# y: new observation
# f: function that implements one MHR iteration
# N: number of iterations
# start: initial value of the chain
# ...: additional arguments to f
  
  draws <- numeric(N)
  draws[1] <- f(y, current_mean = start, ...)
  
  for (i in 2:N) {
    draws[i] <- f(y, current_mean = draws[i - 1], ...)
  }
  
  return(draws)
}

y <- 5; N <- 5e3; start <- 3
d <- mhr(y, N, f = metropolis_iteration_1, start, proposal_scale = 2, sd = 0.5,  
         prior_mean = 0, prior_sd = 2)
```

:::

## Markov Chain Animation {.smaller}

![](anim/anim-0.1.gif){fig-align="left" height="255"}
![](anim/anim-10.gif){fig-align="left" height="255"}
![](anim/anim-2.gif){fig-align="left" height="255"}
Autocorrelation function (ACF)

![](anim/anim-0.1.png){fig-align="left" height="255"}
![](anim/anim-10.png){fig-align="left" height="255"}
![](anim/anim-2.png){fig-align="left" height="255"}


## Comparing Samples to the True Posterior {.smaller}

</br>

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| output-location: column
#| echo: true

np <- normal_normal_post(y = y, 
                         sd = 0.5, 
                         prior_mu = 0, 
                         prior_sd = 2)

theta <- seq(np$post_mu + 3*np$post_sd, 
             np$post_mu - 3*np$post_sd, 
             len = 100)

dn <- dnorm(theta, mean = np$post_mu, 
                     sd = np$post_sd)

p <- ggplot(aes(d), data = tibble(d = d))
p + geom_histogram(aes(y = after_stat(density)), 
                   bins = 25, alpha = 0.6) +
  geom_line(aes(theta, dn), linewidth = 0.5, 
            color = 'red', 
            data = tibble(theta, dn)) + 
  ylab("") + xlab(expression(theta)) + 
  ggtitle("MHR draws vs Normal(4.71, 0.49)")
```
:::

## What is Wrong with this Code {.smaller}

</br>

::: {.fragment}
```{r}
#| echo: true
#| eval: false

metropolis_iteration_1 <- function(y, current_mean, proposal_scale, 
                                   sd, prior_mean, prior_sd) {
# assume prior ~ N(prior_mean, prior_sd) and sd is known
# proposal sampling distribution q = N(current_mean, proposal_scale)

  proposal_mean <- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        
  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')
                         sd = prior_sd) *                  
                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')
  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)
                         sd = prior_sd) *        
                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)

  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]
  alpha <- min(ratio, 1)     
  
  if (alpha > runif(1)) {         # this is just another way of flipping a coint
    next_value <- proposal_mean  
  } else {
    next_value <- current_mean
  }
  return(next_value)
}
```
:::

## What if Y is a vector? {.smaller}

::: {.incremental}
- Recall the likelihood of many independent observations, is the product of their individual likelihoods
$$
\begin{eqnarray}
f(y \mid \mu) & = & \prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi} \
\sigma} \exp\left( - \, \frac{1}{2} \left(  \frac{y_i - \mu}{\sigma} \right)^2     \right) \\
f(\mu \mid y) & \propto & \text{prior} \cdot \prod_{i=1}^{n}\frac{1}{
\sigma} \exp\left( - \, \frac{1}{2} \left(  \frac{y_i - \mu}{\sigma} \right)^2     \right) \\
\end{eqnarray}
$$

- This suggests the following change:
:::

::: {.fragment}
```{r}
#| echo: true
#| code-line-numbers: "|6|8|10|11|"

metropolis_iteration_2 <- function(y, current_mean, proposal_scale, sd,
                                 prior_mean, prior_sd) {

  proposal_mean <- rnorm(1, current_mean,  proposal_scale)        
  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, sd = prior_sd) * 
                   prod(dnorm(y, mean = proposal_mean, sd = sd))   
  f_current     <- dnorm(current_mean, mean = prior_mean, sd = prior_sd) *        
                   prod(dnorm(y, mean = current_mean, sd = sd))

  if ((f_proposal || f_current) == 0) {
    return("Error: underflow") # on my computer double.xmin = 2.225074e-308
  }
  ratio <- f_proposal / f_current 
  alpha <- min(ratio, 1)     
  
  if (alpha > runif(1)) {         # definitely go if f_mu_prime > f_mu: alpha = 1
    next_value <- proposal_mean   # if alpha < 1, go if alpha > U(0, 1)
  } else {
    next_value <- current_mean
  }
  return(next_value)
}
```
:::

## Handling Underflow {.smaller}

::: {.fragment}
```{r}
#| cache: true
#| echo: true
set.seed(123)
y <- rnorm(300, mean = 2, sd = 0.55)
normal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>
  round(2)

replicate(20, metropolis_iteration_2(y = y, 
                                     current_mean = 1, 
                                     proposal_scale = 2, 
                                     sd = 0.55, 
                                     prior_mean = 0, 
                                     prior_sd = 1))
```
:::

## Always Compute on the Log Scale {.smaller}
::: {.incremental}
- For one observation $y$:
$$
\begin{eqnarray}
\log f(y \mid \mu) & \propto & \log \left( \frac{1}{ 
\sigma} \exp\left( - \, \frac{1}{2} \left(  \frac{y - \mu}{\sigma} \right)^2 \right) \right) \\
& = & -\log(\sigma) - \frac{1}{2} \left(  \frac{y - \mu}{\sigma} \right)^2
\end{eqnarray}
$$

::: {.fragment}
```{r}
#| echo: true

dnorm_log <- function(y, mean = 0, sd = 1) {
# dropping constant terms; not needed for MCMC
  -log(sd) - 0.5 * ((y - mean) / sd)^2
}
```
:::

- For multiple observations, you can just sum the log-likelihood
:::

## MHR on the Log Scale {.smaller}

```{r}
#| echo: true

metropolis_iteration_log <- function(y, current_mean, proposal_scale, 
                                     sd, prior_mean, prior_sd) {
  
  # draw a proposal q(proposal_mean | current_mean)
  proposal_mean  <- rnorm(1, current_mean,  proposal_scale)           
  
  # construct a proposal: f(mu') * \prod f(y_i | mu') on the log scale
  proposal_lik   <- sum(dnorm_log(y, mean = proposal_mean, sd = sd))
  proposal_prior <- dnorm_log(proposal_mean, mean = prior_mean, sd = prior_sd)
  f_proposal     <- proposal_prior + proposal_lik
  
  # construct a current: f(mu) * \prod f(y_i | mu) on the log scale
  current_lik    <- sum(dnorm_log(y, mean = current_mean, sd = sd))
  current_prior  <- dnorm_log(current_mean, mean = prior_mean, sd = prior_sd)
  f_current      <- current_prior + current_lik
  
  # ratio on the log scale = difference of the logs
  log_ratio <- f_proposal - f_current
  log_alpha <- min(log_ratio, 0)     
  
  if (log_alpha > log(runif(1))) {  
    next_value <- proposal_mean     
  } else {
    next_value <- current_mean
  }
  return(next_value)
}
```

## Checking the Work {.smaller}

::: {.fragment}
```{r}
#| cache: true
#| echo: true
set.seed(123)
y <- rnorm(300, mean = 2, sd = 0.55)
normal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>
  round(2)
```
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true

replicate(4, metropolis_iteration_2(y = y, 
                                     current_mean = 1, 
                                     proposal_scale = 2, 
                                     sd = 0.55, 
                                     prior_mean = 0, 
                                     prior_sd = 1))
```
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true

replicate(4, metropolis_iteration_log(y = y, 
                                     current_mean = 1, 
                                     proposal_scale = 2, 
                                     sd = 0.55, 
                                     prior_mean = 0, 
                                     prior_sd = 1))
```
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true

d <- mhr(y, N, f = metropolis_iteration_log, start, proposal_scale = 2, sd = 0.55,  prior_mean = 0, prior_sd = 1)
mean(d) |> round(2)
sd(d) |> round(2)
```
:::

## Why Does the Algorithm Work

::: {.incremental}
- To show why the algorithm works, we need to show:
  (a) The chain has stationary distribution and it is unique
  (b) The stationary distribution is our target distribution $f(\theta \mid y)$
- Condition (a) requires some theory of Markov Chains, but the conditions are mild and are generally satisfied in practice. (See Chapters 11 and 12 in Blitzstein and Hwang for more)
  - We will show an outline of the proof for (b)
:::

## Why Does the Algorithm Work {.smaller}

::: {.incremental}
- We will only consider the case of symmetric q
- Suppose you sample two points from the PMF $f(\theta \mid y)$, $\theta_a$ and $\theta_b$ and assume we are at time $t-1$
- Let the probability of going from $\theta_a$ to $\theta_b$ be: $\P(\theta^t = \theta_b) \mid \theta^{t-1} = \theta_a) := p_{a b}$ and the reverse jump: $\P(\theta^t = \theta_a) \mid \theta^{t-1} = \theta_b) := p_{b a}$
- We want to show:
$$
\begin{eqnarray}
\frac{p_{a  b}}{p_{b  a}} &=& \frac{f(\theta_b \mid y)}{f(\theta_a \mid y)} ,\, \text{where} \\
p_{a  b} &=& q(\theta_b \mid \theta_a) \cdot \min \left \lbrace 1, \frac{f(\theta_b \mid y)}{f(\theta_a \mid y)} \right \rbrace \\
p_{b  a} &=& q(\theta_a \mid \theta_b) \cdot \min \left \lbrace 1, \frac{f(\theta_a \mid y)}{f(\theta_b \mid y)} \right \rbrace
\end{eqnarray}
$$
:::

## Why Does the Algorithm Work {.smaller}

::: {.incremental}
- For symmetric q: $q(\theta_b | \theta_a) = q(\theta_a | \theta_b)$ and:
$$
\begin{eqnarray}
\frac{p_{a  b}}{p_{b  a}} &=& \frac{\min \left \lbrace 1, \frac{f(\theta_b \mid y)}{f(\theta_a \mid y)} \right \rbrace}{\min \left \lbrace 1, \frac{f(\theta_a \mid y)}{f(\theta_b \mid y)} \right \rbrace}
\end{eqnarray}
$$
- Consider the case when $f(\theta_b \mid y) > f(\theta_a \mid y)$
$$
\begin{eqnarray}
\frac{p_{a  b}}{p_{b  a}} &=& \frac{1}{\frac{f(\theta_a \mid y)}{f(\theta_b \mid y)}} = 
\frac{f(\theta_b \mid y)}{f(\theta_a \mid y)}
\end{eqnarray}
$$
- When $f(\theta_a \mid y) > f(\theta_b \mid y)$
$$
\frac{p_{a  b}}{p_{b  a}} = \frac{\frac{f(\theta_b \mid y)}{f(\theta_a \mid y)}}{1} = 
\frac{f(\theta_b \mid y)}{f(\theta_a \mid y)}
$$
:::

## Introducing Posterior Predictive Distribution {.smaller}
::: {.incremental}
- Recall the prior predictive distribution, before observing $y$, that appears in the denominator of Bayes's rule:
$$
f(y) = \int f(y, \theta) \, d\theta = \int f(\theta) f(y \mid \theta) \, d\theta
$$
- A posterior predictive distribution, $f(\tilde{y} | y)$ is obtained in a similar manner
$$
\begin{eqnarray}
f(\tilde{y} \mid y) &=& \int f(\tilde{y}, \theta \mid y) \, d\theta \\
&=& \int f(\theta \mid y) f(\tilde{y} \mid \theta, y) \, d\theta \\
&=& \int f(\theta \mid y) f(\tilde{y} \mid \theta) \, d\theta \\
\end{eqnarray}
$$

- $f(\tilde{y} \mid \theta, y) = f(\tilde{y} \mid \theta)$ since $y \perp\!\!\!\perp \tilde{y} \mid \theta$ (conditional indepedence)
- Two sources of variability are accounted for: sampling variability in $\tilde{y}$ weighted by posterior variability in $\theta$
- Given draws from $f(\theta \mid y)$, $\theta^{(m)} \sim f(\theta \mid y)$, we can compute the integral in a usual way: $f(\tilde{y} \mid y) \approx \frac{1}{M} \sum_{m = 1}^M f(\tilde{y} \mid \theta^{(m)})$
:::

## Introducing Posterior Predictive Distribution {.smaller}
::: {.incremental}
- For simple models we can often evalute $\int f(\theta \mid y) f(\tilde{y} \mid \theta) \, d\theta$ directly
- We will use an inderect approach using our example Normal-Normal model with known variance
- Since we already know $f(\theta | y)$, we will sample $\theta$ from it, and then sample $\tilde{y}$, from $f(\tilde{y} \mid \theta)$
- Recall, $y \sim \text{Normal}(\mu, 0.5)$ with prior $\mu \sim \text{Normal}(0, 2)$
- For $y = 5$, the posterior $\mu | y \sim \text{Normal}(4.71, 0.49)$
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true

ppd <- function(post_mu, post_sd) {
  mu <- rnorm(1, mean = post_mu, sd = post_sd)
  y  <- rnorm(1, mean = mu, sd = 0.5)
}
pd <- normal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2)
y <- replicate(1e5, ppd(pd$post_mu, pd$post_sd))
cat("y_tilde | y ~ Normal(", round(mean(y), 2), ",", round(sd(y), 2), ")", sep = "")
```
:::

::: incremental
- It can be shown that the posterior predictive distribution will have the same mean as the posterior distribution and the variance as the sum of the posterior variance and data variance:
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
ppd_mu <- pd$post_mu |> round(2)
ppd_sigma <- sqrt(pd$post_sd^2 + 0.5^2) |> round(2)
cat("y_tilde | y ~ Normal(", ppd_mu, ", ", ppd_sigma, ")", sep = "")
```
:::

## Posterior Predictions in Stan {.smaller}

::: {.incremental}
- You can compute posterior predictive distribution in R, Stan, and `rstanarm`
- Here, we will see how to do it in Stan
- We will show an example in `rstanarm` in the next lecture
:::

::: {.fragment}
```{stan, output.var = 'stan1', eval = FALSE, echo = TRUE}
data {
  real y;
  real<lower=0> sigma;
}
parameters {
  real mu;
}
model {
  mu ~ normal(0, 2);
  y ~ normal(mu, sigma);
}
generated quantities {
  real y_tilde = normal_rng(mu, sigma);
}
```
:::

## Posterior Predictions in Stan {.smaller}

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| echo: true
#| message: false
library(cmdstanr)

m1 <- cmdstan_model("stan/normal_pred.stan") # compile the model
data <- list(y = 5, sigma = 0.5)
f1 <- m1$sample(       # for other options to sample, help(sample)
  data = data,         # pass data as a list, match the vars name to Stan
  seed = 123,          # to reproduce results, Stan does not rely on R's seed
  chains = 4,          # total chains, the more, the better
  parallel_chains = 4, # for multi-processor CPUs
  refresh = 0,         # number of iterations printed on the screen
  iter_warmup = 500,   # number of draws for warmup (per chain)
  iter_sampling = 2000 # number of draws for samples (per chain)
)
f1$summary()
```
:::

## Breaking Codes with MCMC

::: panel-tabset
## Dianconis
![](images/prison0.png){fig-align="center" width="650"}

## Cypher text
![](images/prison1.png){fig-align="center" width="800"}

## Algorithm
![](images/prison3.png){fig-align="center" width="800"}

## Decoded text
![](images/prison2.png){fig-align="center" width="800"}
:::




