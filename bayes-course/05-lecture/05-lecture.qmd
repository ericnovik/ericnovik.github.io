---
title: "Bayesian Inference"
subtitle: "NYU Applied Statistics for Social Science Research"
author: "Eric Novik | Spring 2023 | Lecture 5"
format:
  revealjs: 
    theme: ../../custom.scss
    scrollable: true
    slide-number: true
    chalkboard: 
      buttons: false
      chalk-width: 5
    preview-links: auto
    footer: <https://ericnovik.github.io/bayes-course.html>
editor: source
always_allow_html: true
---


##  Linear Regression and Model Evaluation

::: columns
::: {.column width="60%"}
::: incremental
- Introducing linear regression
- Prior predictive simulations
- Sampling from the posterior
- Example of linear regression in Stan
- Evaluating the quality of the draws
- Posterior predictions
- Cross validation, ELPD, and LOO
:::
:::

::: {.column width="40%"}
![](images/stan.png){fig-align="center" height="500"}
:::
:::

```{r}
library(ggplot2)
library(dplyr)
library(janitor)
library(gridExtra)
library(purrr)

thm <-
  theme_minimal() + theme(
    panel.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    plot.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    panel.grid.major = element_blank()
  )
theme_set(thm)

dbinom_theta <- function(theta, N, y) {
  choose(N, y) * theta^y * (1 - theta)^(N - y) 
}

dot_plot <- function(x, y, yc = NULL, dodge = 0.2) {
  x <- as.factor(x)
  p <- ggplot(data.frame(x, y), aes(x, y))
  p <- p + geom_point(aes(x = x, y = y), size = 0.5) +
    geom_segment(aes(
      x = x,
      y = 0,
      xend = x,
      yend = y
    ), linewidth = 0.2) +
    xlab(expression(theta)) + ylab(expression(f(theta)))
  
  if (!is.null(yc)) {
    xc <- as.numeric(x) + dodge
    p1 <- p + geom_point(aes(x = xc, y = yc), color = 'red', size = 0.5) +
      geom_segment(aes(
        x = xc,
        y = 0,
        xend = xc,
        yend = yc
      ), color = 'red', linewidth = 0.2)
    return(p1)
  }
  return(p)
}

```

$$
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\L}{\mathcal{L}}
\DeclareMathOperator{\I}{\text{I}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

## Motivating Example {.smaller}
::: columns
::: {.column width="50%"}
::: incremental
- We borrow this example from Richard McElreath's Statistical Rethinking
- The data sets provided have been produced between 1969 to 2008, based on Nancy Howell's observations of the !Kung San
- From Wikipedia: "The ÇƒKung are one of the San peoples who live mostly on the western edge of the Kalahari desert, Ovamboland (northern Namibia and southern Angola), and Botswana."
:::

:::

::: {.column width="50%"}
![](images/kalahari.png){fig-align="right"}
:::
:::


::: footer
[Univercity of Toronto Data Sets](https://tspace.library.utoronto.ca/handle/1807/10395)
:::

## Howell Dataset {.smaller}

::: columns
::: {.column width="50%"}
::: {.fragment}
- Data sample and summary:
```{r}
#| echo: false
library(dplyr)
d <- readr::read_csv("data/howell.csv")
knitr::kable(d[1:6, ])
summary(d[, 1:3])
```
:::
:::

::: {.column width="50%"}
::: {.fragment}
```{r}
#| fig-width: 4.5
#| fig-height: 3.4
#| fig-align: center
#| echo: false
library(ggplot2)
w <- 199.8 / 2.2; h <- 69 * 2.54
p <- ggplot(aes(height, weight), data = d)
p + geom_point(aes(color = as.factor(male)), size = 0.2) + 
  geom_point(aes(x = h, y = w), color = '#00bfc4') + 
  ylab("Weight (kg)") + xlab("Height (cm)") +
  annotate("text", y = w - 5, x = h, label = "US male",
            color = '#00bfc4', size = 3) +
  annotate("text", y = 42, x = 131, label = "females",
            color = '#f8766d', size = 3) +
  annotate("text", y = 42, x = 173, label = "males",
            color = '#00bfc4', size = 3) +
  ggtitle("Kalahari !Kung San people") +
  theme(legend.position = "none")
```
:::

::: incremental
-   Notice a non-linearity
-   Thinking about why should this be, can give you an insight into how to model these data
:::

:::

:::

## Howell Dataset {.smaller}
::: incremental
- For now, we will focus on the linear subset of the data
- We will denonstrate the non-linear model at the end
- We will restrict our attention to adults (age > 18)
:::

::: {.fragment}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-align: center
#| echo: false
d <- d |> filter(age >= 18)
p <- ggplot(aes(height, weight), data = d)
p + geom_point(size = 0.2) +
  ylab("Weight (kg)") + xlab("Height (cm)") +
  ggtitle("Kalahari !Kung San people")

```
:::

## General Aproach {.smaller}
::: incremental
- Assess the scope of the inferences that you will get with this model
- In the case of linear regression, there is likely no causal mechanism and the coeficients should be intepreted as comparisons (RAOS, Page 84)
- Set up reasonable priors and likelihood
- Perform a prior predictive simualtion
- Adjust your priors
- Fit the model to data
- Assess quality of the inference and quality of the model
- Adjust your model
:::

## Howell Regression {.smaller}
::: incremental
- We will build a predictive model for adult Weight $y$ given Height $x$ using the Howell dataset
- Initial stab at the model:
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x_i \\
\alpha & \sim & \text{Normal}(\alpha_{l}, \, \alpha_s) \\
\beta & \sim & \text{Normal}(\beta_{l}, \, \beta_s) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- We have to specify $\alpha_{l}$ and $\alpha_s$, where l and s signify location and scale, and r, the rate of the exponential
- If we work on the original scale for $x$, it is awkward to choose a prior for the intercept \alpha: it correponds to the weight of the person with zero height
- This can be fixed by subtracting average height from $x$
:::

## Howell Regression {.smaller}
::: incremental
- We define a new variable, the centered version of $x$: $x^c_i = x_i - \bar{x}$
- Now $\alpha$ corresponds to the weight of an average person
- Checking [Wikipedia](https://en.wikipedia.org/wiki/Human_body_weight) reveals that average weight of a person in Africa is about 60 kg
- They don't state the standard deviation but it is unlikely that an African adult would weigh less than 30 kg and more than 120 kg and so we will set the prior sd = 10
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(\beta_{l}, \, \beta_s) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- What about the slope $\beta$?

:::

## Howell Regression {.smaller}
::: incremental
- In this dataset, the units of $\beta$ are $\frac{kg}{cm}$, since the units of height are $cm$
- First thing, $\beta$ should be positive. Why?
- Second, $beta$ is likely less than 1. Why?
- We can consult [height-weight tables](http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html) for the expected value and variance
- In the dataset, $\E(\beta) = 0.55$ with a standard error of 0.006, but since we are uncertain how applicable that is to !Kung, we will allow the prior to vary more
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(0.55, \, 0.1) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- What about the error term $\sigma$?
:::

## Howell Regression {.smaller}
::: incremental
- We know that $\sigma$ must be positive and so a possible choice for a the prior is $\text{Normal}^+$, Exponential, etc.
- At this stage they key is rule out implausable values, not to get something precise, partcularly since we have enough data (> 340 observations)
- From the background data, the residual standard error was 4.6, which implies the exponential rate parameter to be 0.2
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(0.55, \, 0.1) \\
\sigma & \sim & \text{Exp}(0.2) \\
\end{eqnarray}
$$
- We are now ready to perform a prior predictive simulation

:::

## Prior Predictive Simulation {.smaller}

::: incremental
- The simulation follows the generative process defined by the model
:::

::: columns
::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| echo: true

d <- d |>
  mutate(height_c = height - mean(height))
round(mean(d$height_c), 2)
head(d)

prior_pred <- function(data) {
  alpha <- rnorm(1, 60, 10)
  beta <- rnorm(1, 0.55, 0.1)
  sigma <- rexp(1, 0.2)
  l <- nrow(data); y <- numeric(l)
  for (i in 1:l) {
    mu <- alpha + beta * data$height_c[i]
    y[i] <- rnorm(1, mu, sigma)
  }
  return(y)
}
```
:::

:::

::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| echo: true
n <- 100
pr_p <- replicate(n = n, prior_pred(d))
# using library(purrr) functional primitives:
# pr_p <- map(1:n, \(i) prior_pred(d))
dim(pr_p)
round(pr_p[1:12, 1:8], 2)
```
:::

:::
:::



## Prior Predictive Simulation {.smaller}

::: incremental
- To get a sense for the possible regression lines implied by the prior, we fit a linear model to each simulation draw and plot the lines over observations
:::

::: {.fragment}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| output-location: column
#| echo: true

intercepts <- numeric(n)
slopes <- numeric(n)
for (i in 1:n) {
  coefs <- coef(lm(pr_p[, i] ~ d$height_c))
  intercepts[i] <- coefs[1]
  slopes[i] <- coefs[2]
}

# using library(purrr) functional primitives:
# df <- pr_p |> map_dfr(\(y) coef(lm(y ~ d$height_c)))

p <- ggplot(aes(height_c, weight), data = d)
p + geom_point(size = 0.5) + ylim(20, 90) + 
  geom_abline(slope = slopes, 
              intercept = intercepts, 
              alpha = 1/6) +
  ylab("Weight (kg)") + 
  xlab("Centered Height (cm)") +
  ggtitle("Kalahari !Kung San people", 
          subtitle = "Prior predictive simulation")
```
:::

::: incremental
- [What do you notice about this prior?](https://www.polleverywhere.com/free_text_polls/ktA65I0m27uUAnCn44Cui)
:::

## Deriving a Posterior Distribution {.smaller}

::: incremental
- We have seen how to derive posterior and posterior predictive distribution
- Three dimentional posterior: $f(\alpha, \beta, \sigma)$. What happened to $\mu$?
- We construct the posterior from the prior and data likelihood (for each $y_i$):
$$
\begin{eqnarray}
&\text{Prior: }f(\alpha, \beta, \sigma) = f_1(\alpha) f_2(\beta) f_3(\sigma) \\
&\text{Likelihood: }f(y \mid \alpha, \beta, \sigma) = \prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \\
&\text{Posterior: }f(\alpha,\beta,\sigma \mid y) = \frac{f_1(\alpha) f(_2\beta) f_3(\sigma) \cdot \left[\prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \right]}
 {\int\int\int f_1(\alpha) f_2(\beta) f_3(\sigma) \cdot \left[\prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \right] d\alpha \, d\beta \, d\sigma}
\end{eqnarray}
$$
- To be more precise, we would indicate that $f_1, f_2$ and $f_4$ are Normal with different parameters, and $f_3$ is $\text{Exp}(0.2)$
:::

## Fitting the Model {.smaller}

::: incremental
- Even though our prior is slighly off, 300+ observations is a lot in this case (big data!), and so we proceed to model fitting
- We will use `stan_glm()` function in `rstanarm`
- `rstanarm` has default priors, but you should specify your own:

```{r}
#| echo: false
library(rstanarm)
library(bayesplot)
color_scheme_set("viridis")
m1 <- readr::read_rds("models/m1.rds")
```
::: {.fragment}
```{r}
#| echo: true
#| cache: true
#| eval: false
library(rstanarm)
library(bayesplot)
options(mc.cores = parallel::detectCores())

m1 <- stan_glm(
  weight ~ height_c,
  data = d,
  family = gaussian,
  prior_intercept = normal(60, 10),
  prior = normal(0.55, 0.1),
  prior_aux = exponential(0.2),
  chains = 4,
  iter = 500,
  seed = 1234
)
```
::: 

- By default, `rstanarm` samples from the posterior. To get back the prior predictive distribution (instead of doing it in R) use `prior_PD = TRUE`
:::


## Looking at the Model Summary {.smaller}

::: {.fragment}
```{r}
#| fig-width: 4.5
#| fig-height: 4.5
#| fig-align: center
#| echo: true
summary(m1)
```
::: 

::: incremental
- If you can examine the priors by running `prior_summary(m1)`
:::


## Evaluting Quality of the Inferences {.smaller}

::: {.fragment}
```{r}
#| echo: true
neff_ratio(m1) |> round(2)
```
:::


::: {.fragment}
```{r}
#| echo: true
rhat(m1) |> round(2)
```
:::


::: {.fragment}
```{r}
#| fig-width: 12
#| fig-height: 4
#| fig-align: center
#| echo: true
mcmc_trace(m1, size = 0.3)
```
:::

## Same Model in Stan {.smaller}

::: {.fragment}
```{stan, output.var = 'stan1', eval = FALSE, echo = TRUE}
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
  int<lower=0, upper=1> prior_PD;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
transformed parameters {
  vector[N] mu = alpha + beta * x;
}
model {
  alpha ~ normal(60, 10);
  beta ~ normal(0.55, 0.1);
  sigma ~ exponential(0.2);
  if (!prior_PD) {
    y ~ normal(mu, sigma);
  }
}
generated quantities {
  array[N] real y_tilde = normal_rng(mu, sigma);
}
```
:::

::: incremental
- You can pass `prior_PD` as a flag to enable drawing from prior predictive distribution
:::

## Prior vs Posterior {.smaller}
::: incremental   
- Comparing the prior to the posterior tell us how much the model learned from data
- It also helps us to validate if our priors were reasonable
- In `rstanarm`, you can use `posterior_vs_prior` function
:::

::: {.fragment}
```{r} 
#| fig-width: 12
#| fig-height: 3
#| fig-align: center
#| echo: false

p1 <- posterior_vs_prior(m1, pars = "(Intercept)", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
p2 <- posterior_vs_prior(m1, pars = "height_c", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
p3 <- posterior_vs_prior(m1, pars = "sigma", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
grid.arrange(p1, p2, p3, ncol = 3) 
```
:::

::: incremental
- Your prior should cover the plausable range of parameter values
- When we don't have a lot of data and parameters are complex, setting good priors takes work, but there are [guidelines](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
:::

## Examining the Posterior {.smaller}

::: {.fragment}
```{r}
#| echo: true
library(tidybayes)

draws <- spread_draws(m1, `(Intercept)`, height_c, sigma)
knitr::kable(head(round(draws, 2)))

```
:::

::: incremental
-   `spread_draws` will arrange the inferences in columns (wide format)
-   `gather_draws` will arrange the inferences in rows (long format), which is usually more convenient for plotting and computation
:::

## Examining the Posterior {.smaller}

::: {.fragment}
```{r}
#| echo: true
options(digits = 3)
draws <- gather_draws(m1, `(Intercept)`, height_c, sigma)
knitr::kable(tail(draws, 4))
```
:::

::: {.fragment}
```{r}
#| echo: true
draws |> mean_qi(.width = 0.90) |> knitr::kable() # also see ?median_qi(), etc
```
:::

::: incremental
- From the above table: $\E(y|x^c) = 45 (\text{kg}) + 0.62 (\text{kg/cm})x^c (\text{cm})$
:::

## Variability in Parameter Inferences {.smaller}

::: incremental
- The code in section 9.4 in the book doesn't work as the function and variable names have changed
:::

::: {.fragment}
```{r}
#| echo: true
dpred <- d |> 
  # same as add_epred_draws for lin reg not not for other GLMs
  add_linpred_draws(m1, ndraws = 50)
head(dpred, 3)
```
:::

::: {.fragment}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| output-location: column-fragment
#| echo: true
p <- dpred |> ggplot(aes(x = height_c, y = weight)) +
  geom_line(aes(y = .linpred, group = .draw), 
            alpha = 0.1) + 
  geom_point(data = d, size = 0.05) +
  ylab("Weight (kg)") + 
  xlab("Centered Height (cm)") +
  ggtitle("100 draws from the slope/intercept posterior")
print(p)
```
:::


## Posterior Predictions {.smaller}

::: incremental
- Suppose we are interested in predicting the weight of a person with the height of 160 cm
- This corresponds to the centered height of 5.4: ($160 - \bar{x}$)
- We can now compute the distribution of the mean weight of a 160 cm person (reflecting variability in the slope and intercept only):
  - $\mu = \alpha + \beta \cdot 5.4$, for each posterior draw
- And a predictive distribution:
  -  $y_{\text{pred}}  \sim \text{Normal}(\mu, \sigma)$
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
draws <- spread_draws(m1, `(Intercept)`, height_c, sigma)
draws <- draws |>
  mutate(mu = `(Intercept)` + height_c * 5.4,
         y_pred = rnorm(nrow(draws), mu, sigma))
draws[1:3, 4:8]
```
:::

## Posterior Predictions {.smaller}

::: incremental
-   We can compare predictive and average densitives
-   Left panel showing the densities on their own
-   Right panel showing the same densities in the context of raw observations
:::

::: {.fragment}
```{r}
#| echo: true
mqi <- draws |> median_qi(.width = 0.90)
select(mqi, contains(c('mu', 'y_pred'))) |> round(2)
```
:::

::: columns
::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 5
#| fig-height: 3
#| echo: false
p1 <- ggplot(aes(mu), data = draws)
p1 + geom_density(color = 'red') + geom_density(aes(y_pred), color = 'blue') +
  ylab("") + annotate("text", 51, 0.15, label = expression(y_pred), color = 'blue') +
  annotate("text", 49.3, 0.5, label = expression(mu), color = 'red') +
  xlab("Weight (kg)")
```
:::

:::

::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 5
#| fig-height: 3
#| echo: false
p + 
  geom_segment(aes(x = 5.4, y = mu.lower, xend = 5.4, yend = mu.upper), 
               linewidth = 1.5, color = 'red', data = mqi) +
  geom_segment(aes(x = 5.4, y = y_pred.lower, xend = 5.4, yend = y_pred.upper), 
               linewidth = 2, color = 'blue', alpha = 1/5, data = mqi) + 
  ggtitle("Predicting the weight of a 160 cm person")
  
```
:::

:::
:::

## RStanArm Prediction Functions 

::: incremental
- `posterior_linpred` returns $D \times N$ matrix with D draws and N data points
   - $\eta_n = \alpha + \sum_{p=1}^P \beta_p x_{np}$, where $P$ is the total number of regression inputs

- `posterior_epred` returns an $D \times N$ matrix that applies the inverse link (in GLMs) to the linear predictor $\eta$
   - $\mu_n = \E(y | x_n)$; this is the same as $\eta$ in Lin Regression
   
- `posterior_predict` returns an $D \times N$ matrix of predictions: $y \mid \mu_n$
:::

## Predictions in RStanArm {.smaller}

::: incremental

- Posterior linear predictor

::: {.fragment}
```{r}
#| cache: true
#| echo: true
eta <- posterior_linpred(m1, newdata = data.frame(height_c = 5.4))
quantile(eta, probs = c(0.05, 0.50, 0.95)) |> round(2)
glue::glue('From the R simulation, eta = [{mqi$mu.lower |> round(2)}, {mqi$mu.upper |> round(2)}]')
```
:::

- Posterior conditional mean

::: {.fragment}
```{r}
#| cache: true
#| echo: true
mu <- posterior_epred(m1, newdata = data.frame(height_c = 5.4))
quantile(mu, probs = c(0.05, 0.50, 0.95)) |> round(2)
```
:::

- Posterior prediction

::: {.fragment}
```{r}
#| cache: true
#| echo: true
y_pred <- posterior_predict(m1, newdata = data.frame(height_c = 5.4))
quantile(y_pred, probs = c(0.05, 0.50, 0.95)) |> round(2)
glue::glue('From the R simulation, eta = [{mqi$y_pred.lower |> round(2)}, {mqi$y_pred.upper |> round(2)}]')
```
:::

:::

## Evaluting Quality of the Predictions
