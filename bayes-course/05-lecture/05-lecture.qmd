---
title: "Bayesian Inference"
subtitle: "NYU Applied Statistics for Social Science Research"
author: "Eric Novik | Spring 2023 | Lecture 5"
format:
  revealjs: 
    theme: ../../custom.scss
    scrollable: true
    slide-number: true
    chalkboard: 
      buttons: false
      chalk-width: 5
    preview-links: auto
    footer: <https://ericnovik.github.io/bayes-course.html>
editor: source
always_allow_html: true
---


##  Linear Regression and Model Evaluation

::: columns
::: {.column width="60%"}
::: incremental
- Introducing linear regression
- Prior predictive simulations
- Sampling from the posterior
- Example of linear regression in Stan
- Evaluating the quality of the draws
- Posterior predictions
- Cross validation, ELPD, and LOO
:::
:::

::: {.column width="40%"}
![](images/stan.png){fig-align="center" height="500"}
:::
:::

```{r}
library(ggplot2)
library(dplyr)
library(janitor)
library(gridExtra)
library(purrr)

thm <-
  theme_minimal() + theme(
    panel.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    plot.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    panel.grid.major = element_blank()
  )
theme_set(thm)

dbinom_theta <- function(theta, N, y) {
  choose(N, y) * theta^y * (1 - theta)^(N - y) 
}

dot_plot <- function(x, y, yc = NULL, dodge = 0.2) {
  x <- as.factor(x)
  p <- ggplot(data.frame(x, y), aes(x, y))
  p <- p + geom_point(aes(x = x, y = y), size = 0.5) +
    geom_segment(aes(
      x = x,
      y = 0,
      xend = x,
      yend = y
    ), linewidth = 0.2) +
    xlab(expression(theta)) + ylab(expression(f(theta)))
  
  if (!is.null(yc)) {
    xc <- as.numeric(x) + dodge
    p1 <- p + geom_point(aes(x = xc, y = yc), color = 'red', size = 0.5) +
      geom_segment(aes(
        x = xc,
        y = 0,
        xend = xc,
        yend = yc
      ), color = 'red', linewidth = 0.2)
    return(p1)
  }
  return(p)
}

```

$$
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\L}{\mathcal{L}}
\DeclareMathOperator{\I}{\text{I}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

## Motivating Example {.smaller}
::: columns
::: {.column width="50%"}
::: incremental
- We borrow this example from Richard McElreath's Statistical Rethinking
- The data sets provided have been produced between 1969 to 2008, based on Nancy Howell's observations of the !Kung San
- From Wikipedia: "The ǃKung are one of the San peoples who live mostly on the western edge of the Kalahari desert, Ovamboland (northern Namibia and southern Angola), and Botswana."
:::

:::

::: {.column width="50%"}
![](images/kalahari.png){fig-align="right"}
:::
:::


::: footer
[Univercity of Toronto Data Sets](https://tspace.library.utoronto.ca/handle/1807/10395)
:::

## Howell Dataset {.smaller}

::: columns
::: {.column width="50%"}
::: {.fragment}
- Data sample and summary:
```{r}
#| echo: false
library(dplyr)
d <- readr::read_csv("data/howell.csv")
knitr::kable(d[1:6, ])
summary(d[, 1:3])
```
:::
:::

::: {.column width="50%"}
::: {.fragment}
```{r}
#| fig-width: 4.5
#| fig-height: 3.4
#| fig-align: center
#| echo: false
library(ggplot2)
w <- 199.8 / 2.2; h <- 69 * 2.54
p <- ggplot(aes(height, weight), data = d)
p + geom_point(aes(color = as.factor(male)), size = 0.2) + 
  geom_point(aes(x = h, y = w), color = '#00bfc4') + 
  ylab("Weight (kg)") + xlab("Height (cm)") +
  annotate("text", y = w - 5, x = h, label = "US male",
            color = '#00bfc4', size = 3) +
  annotate("text", y = 42, x = 131, label = "females",
            color = '#f8766d', size = 3) +
  annotate("text", y = 42, x = 173, label = "males",
            color = '#00bfc4', size = 3) +
  ggtitle("Kalahari !Kung San people") +
  theme(legend.position = "none")
```
:::

::: incremental
-   Notice a non-linearity
-   Thinking about why should this be, can give you an insight into how to model these data
:::

:::

:::

## Howell Dataset {.smaller}
::: incremental
- For now, we will focus on the linear subset of the data
- We will denonstrate the non-linear model at the end
- We will restrict our attention to adults (age > 18)
:::

::: {.fragment}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| fig-align: center
#| echo: false
d <- d |> filter(age >= 18)
p <- ggplot(aes(height, weight), data = d)
p + geom_point(size = 0.2) +
  ylab("Weight (kg)") + xlab("Height (cm)") +
  ggtitle("Kalahari !Kung San people")

```
:::

## General Aproach {.smaller}
::: incremental
- Assess the scope of the inferences that you will get with this model
- In the case of linear regression, there is likely no causal mechanism and the coeficients should be intepreted as comparisons (RAOS, Page 84)
- Set up reasonable priors and likelihood
- Perform a prior predictive simualtion
- Adjust your priors
- Fit the model to data
- Assess quality of the inference and quality of the model
- Adjust your model
:::

## Howell Regression {.smaller}
::: incremental
- We will build a predictive model for adult Weight $y$ given Height $x$ using the Howell dataset
- Initial stab at the model:
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x_i \\
\alpha & \sim & \text{Normal}(\alpha_{l}, \, \alpha_s) \\
\beta & \sim & \text{Normal}(\beta_{l}, \, \beta_s) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- We have to specify $\alpha_{l}$ and $\alpha_s$, where l and s signify location and scale, and r, the rate of the exponential
- If we work on the original scale for $x$, it is awkward to choose a prior for the intercept \alpha: it correponds to the weight of the person with zero height
- This can be fixed by subtracting average height from $x$
:::

## Howell Regression {.smaller}
::: incremental
- We define a new variable, the centered version of $x$: $x^c_i = x_i - \bar{x}$
- Now $\alpha$ corresponds to the weight of an average person
- Checking [Wikipedia](https://en.wikipedia.org/wiki/Human_body_weight) reveals that average weight of a person in Africa is about 60 kg
- They don't state the standard deviation but it is unlikely that an African adult would weigh less than 30 kg and more than 120 kg and so we will set the prior sd = 10
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(\beta_{l}, \, \beta_s) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- What about the slope $\beta$?

:::

## Howell Regression {.smaller}
::: incremental
- In this dataset, the units of $\beta$ are $\frac{kg}{cm}$, since the units of height are $cm$
- First thing, $\beta$ should be positive. Why?
- Second, $beta$ is likely less than 1. Why?
- We can consult [height-weight tables](http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html) for the expected value and variance
- In the dataset, $\E(\beta) = 0.55$ with a standard error of 0.006, but since we are uncertain how applicable that is to !Kung, we will allow the prior to vary more
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(0.55, \, 0.1) \\
\sigma & \sim & \text{Exp}(r) \\
\end{eqnarray}
$$
- What about the error term $\sigma$?
:::

## Howell Regression {.smaller}
::: incremental
- We know that $\sigma$ must be positive and so a possible choice for a the prior is $\text{Normal}^+$, Exponential, etc.
- At this stage they key is rule out implausable values, not to get something precise, partcularly since we have enough data (> 340 observations)
- From the background data, the residual standard error was 4.6, which implies the exponential rate parameter to be 0.2
$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \, \sigma)\\
\mu_i & = & \alpha + \beta x^c_i \\
\alpha & \sim & \text{Normal}(60, \, 10) \\
\beta & \sim & \text{Normal}(0.55, \, 0.1) \\
\sigma & \sim & \text{Exp}(0.2) \\
\end{eqnarray}
$$
- We are now ready to perform a prior predictive simulation

:::

## Prior Predictive Simulation {.smaller}

::: incremental
- The simulation follows the generative process defined by the model
:::

::: columns
::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| echo: true

d <- d |>
  mutate(height_c = height - mean(height))
round(mean(d$height_c), 2)
head(d)

prior_pred <- function(data) {
  alpha <- rnorm(1, 60, 10)
  beta <- rnorm(1, 0.55, 0.1)
  sigma <- rexp(1, 0.2)
  l <- nrow(data); y <- numeric(l)
  for (i in 1:l) {
    mu <- alpha + beta * data$height_c[i]
    y[i] <- rnorm(1, mu, sigma)
  }
  return(y)
}
```
:::

:::

::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| echo: true
n <- 100
pr_p <- replicate(n = n, prior_pred(d))
# using library(purrr) functional primitives:
# pr_p <- map(1:n, \(i) prior_pred(d))
dim(pr_p)
round(pr_p[1:12, 1:8], 2)
```
:::

:::
:::



## Prior Predictive Simulation {.smaller}

::: incremental
- To get a sense for the possible regression lines implied by the prior, we fit a linear model to each simulation draw and plot the lines over observations
:::

::: {.fragment}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| output-location: column
#| echo: true

intercepts <- numeric(n)
slopes <- numeric(n)
for (i in 1:n) {
  coefs <- coef(lm(pr_p[, i] ~ d$height_c))
  intercepts[i] <- coefs[1]
  slopes[i] <- coefs[2]
}

# using library(purrr) functional primitives:
# df <- pr_p |> map_dfr(\(y) coef(lm(y ~ d$height_c)))

p <- ggplot(aes(height_c, weight), data = d)
p + geom_point(size = 0.5) + ylim(20, 90) + 
  geom_abline(slope = slopes, 
              intercept = intercepts, 
              alpha = 1/6) +
  ylab("Weight (kg)") + 
  xlab("Centered Height (cm)") +
  ggtitle("Kalahari !Kung San people", 
          subtitle = "Prior predictive simulation")
```
:::

::: incremental
- [What do you notice about this prior?](https://www.polleverywhere.com/free_text_polls/ktA65I0m27uUAnCn44Cui)
:::

## Deriving a Posterior Distribution {.smaller}

::: incremental
- We have seen how to derive posterior and posterior predictive distribution
- Three dimentional posterior: $f(\alpha, \beta, \sigma)$. What happened to $\mu$?
- We construct the posterior from the prior and data likelihood (for each $y_i$):
$$
\begin{eqnarray}
&\text{Prior: }f(\alpha, \beta, \sigma) = f_1(\alpha) f_2(\beta) f_3(\sigma) \\
&\text{Likelihood: }f(y \mid \alpha, \beta, \sigma) = \prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \\
&\text{Posterior: }f(\alpha,\beta,\sigma \mid y) = \frac{f_1(\alpha) f(_2\beta) f_3(\sigma) \cdot \left[\prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \right]}
 {\int\int\int f_1(\alpha) f_2(\beta) f_3(\sigma) \cdot \left[\prod_{i=1}^{n}f_4(y_i \mid \alpha, \beta, \sigma) \right] d\alpha \, d\beta \, d\sigma}
\end{eqnarray}
$$
- To be more precise, we would indicate that $f_1, f_2$ and $f_4$ are Normal with different parameters, and $f_3$ is $\text{Exp}(0.2)$
:::

## Fitting the Model {.smaller}

::: incremental
- Even though our prior is slighly off, 300+ observations is a lot in this case (big data!), and so we proceed to model fitting
- We will use `stan_glm()` function in `rstanarm`
- `rstanarm` has default priors, but you should specify your own:

```{r}
#| echo: false
library(rstanarm)
library(bayesplot)
color_scheme_set("viridis")
m1 <- readr::read_rds("models/m1.rds")
```
::: {.fragment}
```{r}
#| echo: true
#| cache: true
#| eval: false
library(rstanarm)
library(bayesplot)
options(mc.cores = parallel::detectCores())

m1 <- stan_glm(
  weight ~ height_c,
  data = d,
  family = gaussian,
  prior_intercept = normal(60, 10),
  prior = normal(0.55, 0.1),
  prior_aux = exponential(0.2),
  chains = 4,
  iter = 500,
  seed = 1234
)
```
::: 

- By default, `rstanarm` samples from the posterior. To get back the prior predictive distribution (instead of doing it in R) use `prior_PD = TRUE`
:::


## Looking at the Model Summary {.smaller}

::: {.fragment}
```{r}
#| fig-width: 4.5
#| fig-height: 4.5
#| fig-align: center
#| echo: true
summary(m1)
```
::: 

::: incremental
- If you can examine the priors by running `prior_summary(m1)`
:::


## Evaluting Quality of the Inferences {.smaller}

::: {.fragment}
```{r}
#| echo: true
neff_ratio(m1) |> round(2)
```
:::


::: {.fragment}
```{r}
#| echo: true
rhat(m1) |> round(2)
```
:::


::: {.fragment}
```{r}
#| fig-width: 12
#| fig-height: 4
#| fig-align: center
#| echo: true
mcmc_trace(m1, size = 0.3)
```
:::

## Same Model in Stan {.smaller}

::: {.fragment}
```{stan, output.var = 'stan1', eval = FALSE, echo = TRUE}
data {
  int<lower=0> N;
  vector[N] x;
  vector[N] y;
  int<lower=0, upper=1> prior_PD;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
transformed parameters {
  vector[N] mu = alpha + beta * x;
}
model {
  alpha ~ normal(60, 10);
  beta ~ normal(0.55, 0.1);
  sigma ~ exponential(0.2);
  if (!prior_PD) {
    y ~ normal(mu, sigma);
  }
}
generated quantities {
  array[N] real y_tilde = normal_rng(mu, sigma);
}
```
:::

::: incremental
- You can pass `prior_PD` as a flag to enable drawing from prior predictive distribution
:::

## Prior vs Posterior {.smaller}
::: incremental   
- Comparing the prior to the posterior tell us how much the model learned from data
- It also helps us to validate if our priors were reasonable
- In `rstanarm`, you can use `posterior_vs_prior` function
:::

::: {.fragment}
```{r} 
#| fig-width: 12
#| fig-height: 3
#| fig-align: center
#| echo: false

p1 <- posterior_vs_prior(m1, pars = "(Intercept)", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
p2 <- posterior_vs_prior(m1, pars = "height_c", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
p3 <- posterior_vs_prior(m1, pars = "sigma", color_by = "vs") + xlab("") + thm + theme(legend.position = "none")
grid.arrange(p1, p2, p3, ncol = 3) 
```
:::

::: incremental
- Your prior should cover the plausable range of parameter values
- When we don't have a lot of data and parameters are complex, setting good priors takes work, but there are [guidelines](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
:::

## Examining the Posterior {.smaller}

::: {.fragment}
```{r}
#| echo: true
library(tidybayes)

draws <- spread_draws(m1, `(Intercept)`, height_c, sigma)
knitr::kable(head(round(draws, 2)))

```
:::

::: incremental
-   `spread_draws` will arrange the inferences in columns (wide format)
-   `gather_draws` will arrange the inferences in rows (long format), which is usually more convenient for plotting and computation
:::

## Examining the Posterior {.smaller}

::: {.fragment}
```{r}
#| echo: true
options(digits = 3)
draws <- gather_draws(m1, `(Intercept)`, height_c, sigma)
knitr::kable(tail(draws, 4))
```
:::

::: {.fragment}
```{r}
#| echo: true
draws |> mean_qi(.width = 0.90) |> knitr::kable() # also see ?median_qi(), etc
```
:::

::: incremental
- From the above table: $\E(y|x^c) = 45 (\text{kg}) + 0.62 (\text{kg/cm})x^c (\text{cm})$
:::

## Variability in Parameter Inferences {.smaller}

::: incremental
- The code in section 9.4 in the book doesn't work as the function and variable names have changed
:::

::: {.fragment}
```{r}
#| echo: true
dpred <- d |> 
  # same as add_epred_draws for lin reg not not for other GLMs
  add_linpred_draws(m1, ndraws = 50)
head(dpred, 3)
```
:::

::: {.fragment}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| output-location: column-fragment
#| echo: true
p <- dpred |> ggplot(aes(x = height_c, y = weight)) +
  geom_line(aes(y = .linpred, group = .draw), 
            alpha = 0.1) + 
  geom_point(data = d, size = 0.05) +
  ylab("Weight (kg)") + 
  xlab("Centered Height (cm)") +
  ggtitle("100 draws from the slope/intercept posterior")
print(p)
```
:::


## Posterior Predictions {.smaller}

::: incremental
- Suppose we are interested in predicting the weight of a person with the height of 160 cm
- This corresponds to the centered height of 5.4: ($160 - \bar{x}$)
- We can now compute the distribution of the mean weight of a 160 cm person (reflecting variability in the slope and intercept only):
  - $\mu = \alpha + \beta \cdot 5.4$, for each posterior draw
- And a predictive distribution:
  -  $y_{\text{pred}}  \sim \text{Normal}(\mu, \sigma)$
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
draws <- spread_draws(m1, `(Intercept)`, height_c, sigma)
draws <- draws |>
  mutate(mu = `(Intercept)` + height_c * 5.4,
         y_pred = rnorm(nrow(draws), mu, sigma))
draws[1:3, 4:8]
```
:::

## Posterior Predictions {.smaller}

::: incremental
-   We can compare predictive and average densitives
-   Left panel showing the densities on their own
-   Right panel showing the same densities in the context of raw observations
:::

::: {.fragment}
```{r}
#| echo: true
mqi <- draws |> median_qi(.width = 0.90)
select(mqi, contains(c('mu', 'y_pred'))) |> round(2)
```
:::

::: columns
::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 5
#| fig-height: 3
#| echo: false
p1 <- ggplot(aes(mu), data = draws)
p1 + geom_density(color = 'red') + geom_density(aes(y_pred), color = 'blue') +
  ylab("") + annotate("text", 51, 0.15, label = expression(y_pred), color = 'blue') +
  annotate("text", 49.3, 0.5, label = expression(mu), color = 'red') +
  xlab("Weight (kg)")
```
:::

:::

::: {.column width="50%"}

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 5
#| fig-height: 3
#| echo: false
p + 
  geom_segment(aes(x = 5.4, y = mu.lower, xend = 5.4, yend = mu.upper), 
               linewidth = 1.5, color = 'red', data = mqi) +
  geom_segment(aes(x = 5.4, y = y_pred.lower, xend = 5.4, yend = y_pred.upper), 
               linewidth = 2, color = 'blue', alpha = 1/5, data = mqi) + 
  ggtitle("Predicting the weight of a 160 cm person")
  
```
:::

:::
:::

## RStanArm Prediction Functions 

::: incremental
- `posterior_linpred` returns $D \times N$ matrix with D draws and N data points
   - $\eta_n = \alpha + \sum_{p=1}^P \beta_p x_{np}$, where $P$ is the total number of regression inputs

- `posterior_epred` returns an $D \times N$ matrix that applies the inverse link (in GLMs) to the linear predictor $\eta$
   - $\mu_n = \E(y | x_n)$; this is the same as $\eta$ in Lin Regression
   
- `posterior_predict` returns an $D \times N$ matrix of predictions: $y \mid \mu_n$
:::

## Predictions in RStanArm {.smaller}

::: incremental

- Posterior linear predictor

::: {.fragment}
```{r}
#| cache: true
#| echo: true
eta <- posterior_linpred(m1, newdata = data.frame(height_c = 5.4))
quantile(eta, probs = c(0.05, 0.50, 0.95)) |> round(2)
glue::glue('From the R simulation, 90% interval for eta = [{mqi$mu.lower |> round(2)}, {mqi$mu.upper |> round(2)}]')
```
:::

- Posterior conditional mean

::: {.fragment}
```{r}
#| cache: true
#| echo: true
mu <- posterior_epred(m1, newdata = data.frame(height_c = 5.4))
quantile(mu, probs = c(0.05, 0.50, 0.95)) |> round(2)
```
:::

- Posterior prediction

::: {.fragment}
```{r}
#| cache: true
#| echo: true
y_pred <- posterior_predict(m1, newdata = data.frame(height_c = 5.4))
quantile(y_pred, probs = c(0.05, 0.50, 0.95)) |> round(2)
glue::glue('From the R simulation, 90% interval for y_pred = [{mqi$y_pred.lower |> round(2)}, {mqi$y_pred.upper |> round(2)}]')
```
:::

:::

## Evaluting Model Quality {.smaller}

</br>
</br>

![](images/data-quality.png){fig-align="center"}

## Evaluting Quality of the Model {.smaller}

::: incremental
- There are at least two stages of model evaluation: 1) the quality of the draws and 2) the quality of predictions
- There is also a question of model fairness
   - How were the data collected? 
   - People will likely interpret the results causually, even if not appropriate
   - How will the model be used?
- Just becuase the draws have good statistical properties (e.g., good mixing, low auto-correlation, etc.), it does not mean the model will perform well
- Model performance is assessed on how well it can make predictions for the types of questions that you are interested in
:::



## Evaluting Quality of the Model {.smaller}

::: incremental
- Once we are satisified that the draws are statistically well-behaved, we can focus on evaluating predictive performance
- We typically evalutate predictive performance in-sample and out-of-sample
- In general, a good model is well-calibrated and makes sharp predictions (Gneiting et al. 2007)[^1]
- For in-sample assessment, we perform Posterior Predictive Checks or PPCs
- To assess out-of-sample performance, we rely on cross-validation or its approximations
- If you are making causal (counterfactual) predictions, naive cross-validation will not work. Why?
:::

[^1]: Gneiting, T., Balabdaoui, F., and Raftery, A. E. (2007) Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2), 243–268.


## Model Evaluation {.smaller}

Here is the high level plan of attack:

::: incremental
- Fit a linear model to the full !Kung dataset (not just adults) and let `RStanArm` pick the priors
- We know that this model is not quite right
- Evaluate the model fit 
- Fix the model by thinking about the relationship between height and weight
- Evaluate the improved model
- Compare the linear model to the improved model
:::

## Model Evaluation {.smaller}

```{r}
#| cache: true
#| echo: false
d <- readr::read_csv("data/howell.csv")
m2 <- readr::read_rds("models/m2.rds")
```

```{r}
#| cache: true
#| echo: true
#| eval: false
m2 <- stan_glm(
  weight ~ height,
  data = d,
  family = gaussian,
  chains = 4,
  iter = 600,
  seed = 1234
)
summary(m2)
```

::: incremental
-   There were no sampling problems
-   And posterior draws look well-behaved
-   But how good is this model?
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: false
summary(m2)
```
:::

## Visual Posterior Predictive Checks {.smaller}

::: incremental
- The idea behind PPCs is to compare the distribution of of the observation to posterior predictions
- We already saw an example of how to do it by hand
- Here, we will do using functions in `RStanArm`
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
#| fig-width: 4.5
#| fig-height: 3.5
#| fig-align: center
#| output-location: column
library(bayesplot)
# bayesplot::pp_check(m2) <-- shortcut

# predict for every observed point
yrep <- posterior_predict(m2) 

# select 50 draws at random
s <- sample(nrow(yrep), 50)

# plot data against predictive densities
ppc_dens_overlay(d$weight, yrep[s, ]) 
```
:::

## Visual Posterior Predictive Checks {.smaller}

::: incremental
- We can also compute the distrubutions of test statistics:    
:::

::: {.fragment}
```{r}
#| cache: true
#| echo: true
#| fig-width: 11
#| fig-height: 2
#| fig-align: center
library(gridExtra)
p1 <- ppc_stat(d$weight, yrep, stat = "mean"); p2 <- ppc_stat(d$weight, yrep, stat = "sd")
q25 <- function(y) quantile(y, 0.25); q75 <- function(y) quantile(y, 0.75)
p3 <- ppc_stat(d$weight, yrep, stat = "q25"); p4 <- ppc_stat(d$weight, yrep, stat = "q75"); 
grid.arrange(p1, p2, p3, p4, ncol = 4)
```
:::

::: incremental
- `ppc_stat` is a shorthand for a computation on each posterior (predictive) draw
- For example, `ppc_stat(d$weight, yrep, stat = "mean")` is equivalent to:
   - Setting $T(y) = \text{mean(d\$weight)}$ and $T_{\text{yrep}} = \text{rowMeans(yrep)}$ 
:::

## Visual Posterior Predictive Checks {.smaller}

::: incremental
- Since we have a distribution at each observation point, we can plot the predictive distribution at each observation
- We will first randomly select a subset of 50 people
- Each column of `yrep` is a prediction for each observation point
:::

::: {.fragment}
```{r}
#| cache: true
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| echo: true
#| output-location: column

s <- sample(ncol(yrep), 50)

bayesplot::ppc_intervals(
  d$weight[s],
  yrep = yrep[, s],
  x = d$height[s],
  prob = 0.5,
  prob_outer = 0.90
) + xlab("Height (cm)")  + 
  ylab("Weight (kg)") +
  ggtitle("Predicted vs observed weight")

```
:::


## Quantifying Model Accuracy {.smaller}

::: columns
::: {.column width="60%"}

::: incremental
- We looked at some visual evidence for in-sample model accuracy
- The model is clearly doing a poor job at capturing observed data, particularly in the lower quantiles of the predictive distribution
- In a situation like this, we would typically proceed to model improvements
- Often, the model is not as bad as this one, and we would like to get some quantitative measures of model fit[^2]
- We do this, so we can assess the relative performance of the next set of models
:::


:::

::: {.column width="40%"}

::: {.fragment}
![](images/loo.png){fig-align="center"}
:::
:::
:::

[^2]: Vehtari, A., Gelman, A., & Gabry, J. (2017). [Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC](https://mc-stan.org/loo/index.html). Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4


## Quantifying Model Accuracy {.smaller}

::: incremental
- One way to assess model accuracy is to compute an average square deviation from point prediction: mean square error
   - $\text{MSE} = \frac{1}{N} \sum_{n=1}^{N} (y_n - \E(y_n|\theta))^2$
   - Or it's scaled version which devides the summand by $\V(y_i | \theta)$
- These measures do not work well for non-normal models 
- On the plus side, it is intuitive and computation is easy
- Note that if we just average the errors, we will get zero by definition
:::

::: {.fragment}
```{r}
#| echo: true

# avarage error
mean(d$weight - colMeans(yrep)) |> round(2)

# mean square error
mean((d$weight - colMeans(yrep))^2) |> round(2)

# your book reports median absolute error
median(abs(d$weight - colMeans(yrep)))
```
:::

## Quantifying Model Accuracy {.smaller}

::: incremental
- In a probabilistic prediction, we are able to assess model calibration
- Calibration says that, for example, 50% intervals contain approximately 50% of the observations, and so on
- A well-calibrated model may still be a bad model (see below) as the uncertainty may be too wide; for two models with the same calibration, we prefer the one with lower uncertainty
:::

::: {.fragment}
```{r}
#| echo: true
inside <- function(y, obs) return(obs >= y[1] & obs <= y[2])
calib  <- function(y, data, interval) {
  mid <- interval / 2
  l <- 0.5 - mid; u <- 0.5 + mid
  intervals <- apply(y, 2, quantile, probs = c(l, u))
  is_inside <- numeric(ncol(intervals))
  for (i in 1:ncol(intervals)) {
    is_inside[i] <- inside(intervals[, i], data[i])
  }
 return(mean(is_inside))
}
calib(yrep, d$weight, 0.50)
calib(yrep, d$weight, 0.90)
```
:::


## Quantifying Model Accuracy {.smaller}

::: incremental
- A popular "proper" scoring for probabilisitc prediction is log score
- This is equivalent to the log predictive density $\log f(y | \theta)$
- We would like to estimate the model's ability to predict data it has not seen, even though we do not observe the true data generating process
- This quantity is approximated by log-pointwise-predictive-density
$$
\text{llpd} = \sum_{n=1}^{N} \frac{1}{S} \sum_{s=1}^{S} \log f(y_n \mid \theta_{-n, s})
$$
:::

::: {.fragment}
```{r}
#| echo: true

loo(m2)
```
:::
