---
title: "SMaC: Statistics, Math, and Computing"
subtitle: "Applied Statistics for Social Science Research"
author: "Eric Novik | Summer 2023 | Session 5"
format:
  revealjs: 
    theme: ../../custom.scss
    scrollable: true
    slide-number: true
    chalkboard: 
      buttons: false
      chalk-width: 5
    preview-links: auto
    footer: <https://ericnovik.github.io/smac.html>
editor: source
always_allow_html: true
bibliography: ../references.bib
---

## Session 5 Outline

::: incremental
-   Random variables
-   Bernoulli, Binomial, Geometric
-   PDF and CDF
-   Exponential, Normal
-   Expectations, Variance
-   Joint, Marginal, and Conditional

:::

```{r}
library(ggplot2)
library(dplyr)
library(janitor)
library(gridExtra)
library(purrr)
library(MASS)
library(patchwork)
reticulate::use_condaenv("miniconda3")

thm <-
  theme_minimal() + theme(
    panel.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    plot.background = element_rect(fill = "#f0f1eb", color = "#f0f1eb"),
    panel.grid.major = element_blank()
  )
theme_set(thm)

dbinom_theta <- function(theta, N, y) {
  choose(N, y) * theta^y * (1 - theta)^(N - y) 
}

dot_plot <- function(x, y) {
  p <- ggplot(data.frame(x, y), aes(x, y))
  p + geom_point(aes(x = x, y = y), size = 0.5) +
    geom_segment(aes(x = x, y = 0, xend = x, yend = y), linewidth = 0.2) +
    xlab(expression(theta)) + ylab(expression(f(theta)))
}

```

$$
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\P}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\L}{\mathscr{L}}
\DeclareMathOperator{\I}{\text{I}}
$$

## Random Variables are Not Random {.smaller}

::: panel-tabset
### Story

::: incremental
-   It would be inconvenient to enumerate all possible events to describe a stochastic system

-   A more general approach is to introduce a function that maps sample space $S$ onto the Real line

-   For each possible outcome $s$, random variable $X(s)$ performs this mapping

-   This mapping is deterministic. The randomness comes from the experiment, not from the random variable (RV)

-   While it makes sense to talk about $\P(A)$, where $A$ is an event, it does not make sense to talk about $\P(X)$, but you can say $\P(X(s) = x)$, which we usually write as $\P(X = x)$

-   Let $X$ be the number of Heads in two coin flips. You flip the coin twice, and you get $HH$. In this case, $s = {HH}$, $X(s) = 2$, while $S = \{TT, TH, HT, HH\}$
:::

### Picture

-   Random variable $X$ for the number of Heads in two flips

![](images/nofheads.png){fig-align="center" width="800"}
:::

## Characterising Random Variables {.smaller}

::: panel-tabset
### Introduction

::: incremental
-   Two ways of describing an RV are CDF (Cumulative Distribution Function) and PMF (Probability Mass Function) for discrete RVs and PDF (Probability Density Function) for continuous RVs. There are other ways, but we will stick with CDF and P\[D/M\]F.
-   CDF $F_X(x)$ is a function of $x$ and is bounded between 0 and 1:
:::

::: {.fragment}
$$
F_X(x) = \P(X \leq x)
$$
:::

::: {.fragment}
-   PMF (for discrete RVs only) $p_X(x)$ is a function of $x$. Sometimes you will see it written as $f_X(x)$ to emphasize the connection to $F$.

$$
p_X(x) = \P(X = x)
$$
:::

::: {.fragment}
-   You can get from $p_X$ to $F_X$ by summing. Let's say $x = 4$. In that case:

$$
F_X(4) = \P(X \leq 4) = \sum_{i = 4,3,2,...}\P(X = i)
$$
:::

### R Conventions

::: incremental

-   In R, PMFs and PDFs start with the letter `d`. For example `dbinom()` and `dnormal()` refer to binomial PMF and normal PDF

-   CDFs start with `p`, so `pbinom()` and `pnorm()`

-   Inverse CDFs or quantile functions, start with `q` so `qbinom()` and so on

-   Random number generators start with `r`, so `rbinom()`

-   A binomial RV, which we will define later, represents the number of successes in N trials. In R, the PMF is `dbinom()` and CDF is `pbinom()`

-   Here is the full function signature: `dbinom(x, size, prob, log = FALSE)`

-   `x` is the number of successes, `size` is the number of trials, `prob` is the probability of success in each trial, and `log` is a flag asking if we want the results on the log scale.

::: 

:::

## Binomial RV {.smaller}

::: panel-tabset
### Binomial PMF

::: incremental
-   Bernoulli RV is one coin flip with a set probability of success (say Heads)

-   If $X \sim \text{Bernoulli}(\theta)$, the PMF can be written directly as $\P(X = x) = \theta^x (1 - \theta)^{1-x}, \, x \in \{0, 1\}$

-   Binomial can be thought of as the sum of $N$ independent Bernoulli trials. We can also write:
:::

::: {.fragment}
$$
\text{Bernoulli}(x~|~\theta) = \left\{ \begin{array}{ll} \theta &
\text{if } x = 1, \text{ and} \\ 1 - \theta & \text{if } x = 0
\end{array} \right.
$$
:::

::: {.fragment}
-   We can write the Binomial PMF, $X \sim \text{Binomial}(N, \theta)$ this way:

$$
\text{Binomial}(x~|~N,\theta) = \binom{N}{x}
\theta^x (1 - \theta)^{N - x}
$$
:::

### PMF and CDF Plots

$\text{Binomial}(x~|~N=4,\theta = 1/2)$

```{r}
#| fig-align: center
#| fig-width: 9
#| fig-height: 4

N <- 4

# compute and plot the PMF
pmf <- dbinom(x = 0:N, size = N, prob = 1/2)
d <- data.frame(x =  0:N, y = pmf)
p1 <- ggplot(d, aes(x, pmf))
p1 <- p1 + geom_col(width = .2) + 
  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +
  ylab("P(X = x)") + xlab("x = Number of Heads") +
  ggtitle("X ~ Binomial(4, 1/2)",
          subtitle = expression(PDF: p[X](x) == P(X == x)))

# compute and plot the CDF
x <- seq(-0.5, 4.5, length = 500)
cdf <- pbinom(q = x, size = N, prob = 1/2)
d <- data.frame(q = x, y = cdf)
dd <- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)
p2 <- ggplot(d, aes(x, cdf)) 
p2 <- p2 + geom_point(size = 0.2) + 
  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +
  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +
  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +
  ggtitle("X ~ Binomial(4, 1/2)",
          subtitle = expression(CDF: F[X](x) == P(X <= x))) +
  ylab(expression(P(X <= x))) + xlab("x = Number of Heads")

p1 + p2
```

### Code to Generate the Plots

```{r}
#| echo: true
#| eval: false
library(patchwork)
library(MASS)

N <- 4 # Number of successes out of x trials

# compute and plot the PMF
pmf <- dbinom(x = 0:N, size = N, prob = 1/2)
d <- data.frame(x =  0:N, y = pmf)
p1 <- ggplot(d, aes(x, pmf))
p1 <- p1 + geom_col(width = .2) + 
  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +
  ylab("P(X = x)") + xlab("x = Number of Heads") +
  ggtitle("X ~ Binomial(4, 1/2)",
          subtitle = expression(PDF: p[X](x) == P(X == x)))

# compute and plot the CDF
x <- seq(-0.5, 4.5, length = 500)
cdf <- pbinom(q = x, size = N, prob = 1/2)
d <- data.frame(q = x, y = cdf)
dd <- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)
p2 <- ggplot(d, aes(x, cdf)) 
p2 <- p2 + geom_point(size = 0.2) + 
  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +
  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +
  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +
  ggtitle("X ~ Binomial(4, 1/2)",
          subtitle = expression(CDF: F[X](x) == P(X <= x))) +
  ylab(expression(P(X <= x))) + xlab("x = Number of Heads")

p1 + p2
```
:::

## Binomial in R {.smaller}

```{r}
#| echo: true

# What is the probability of getting 2 Heads out of 5 fair trials?
N <- 5; x <- 2
dbinom(x = x, size = N, prob = 0.5) |> fractions()

# What is the binomial PMF: P(X = x), for N = 5, p = 0.5?
N <- 5; x <- -2:7 # notice we range x over any integers
dbinom(x = x, size = N, prob = 0.5) |> fractions()

# Verify that the PMF sums to 1
sum(dbinom(x = x, size = N, prob = 0.5))

# What is the probability of 3 heads or fewer
pbinom(3, size = N, prob = 0.5) |> fractions()

# compute the CDF: P(X <= x), for N = 5, p = 0.5
pbinom(x, size = N, prob = 0.5) |> fractions()

# get from the PMF to CDF; cumsum() is the cumulative sum function
dbinom(x = x, size = N, prob = 0.5) |> cumsum() |> fractions()
```

::: incremental
- **Your Turn**: Suppose the probability of success is 1/3, N = 10. What is the probability of 6 or more successes? Compute it with a PMF first and verify with the CDF.
:::

## Geometric RV {.smaller}

::: panel-tabset
### PMF

::: incremental
-   Geometric is a discrete waiting time distribution, and Exponential is its continuous analog
-   If $X$ is the number of failures before first success  $X \sim \text{Geometric}(\theta)$, where $\theta$ is probability of success 
-   Example: We keep flipping a coin until we get success, say Heads
    -   Say we flip five times, which means we get the following sequence: T T T T H
    -   The probability of this sequence is: $(\frac{1}{2})^4 (\frac{1}{2})^1$
    -   Notice this is the only way to get this sequence
-   If $x$ is the number of failures, the PMF is $P(X = x) = (1 - \theta)^x \theta$, where $x = 0, 1, 2, ...$
:::

### Check Convergence

-   To check if this is a valid PMF we need to sum over all $x$:

::: {.fragment}
$$
\begin{align}
\sum_{x = 0}^{\infty} \theta (1 - \theta)^x  = 
\theta \sum_{x = 0}^{\infty} (1 - \theta)^x \\
\text{Let } u = 1 - \theta \\
\theta \sum_{x = 0}^{\infty} u^x = \theta \frac{1}{1-u} = \theta \frac{1}{1-1 + \theta} = \frac{\theta}{\theta} = 1
\end{align}
$$
:::

::: {.fragment}
-   The last bit comes from geometric series for $|u| < 1$
:::

### Examples

-   The probability of T T T H (x = 3 failures) when $\theta = 1/2$, has to be $(1/2)^4$ or $1/16$

::: {.fragment}
```{r}
#| echo: true

x <- 3; theta <- 1/2
dgeom(x = x, prob = theta) |> fractions()
```
::: 

::: {.fragment}
-   If $\theta = 1/3$, the probability of the same sequence has to be $(2/3)^3 \cdot 1/3 = 8/81$

```{r}
#| echo: true

x <- 3; theta <- 1/3
dgeom(x = x, prob = theta) |> fractions()
```
:::

::: {.fragment}
-   The PMF is unbounded, but it converges to 1 as demonstrated before
:::

### PMF

```{r}
#| cache: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center
#| echo: true
#| output-location: column

x <-  0:15
theta <- 1/5
y <- dgeom(x = x, prob = theta)
d <- data.frame(x, y)
p <- ggplot(d, aes(x, y))
p + geom_col(width = 0.2) +
xlab("x = Number of failures before first success") +
 ylab(expression(P(X == x))) +
ggtitle("X ~ Geom(1/5)", 
 subtitle = expression(PDF: p[X](x) == P(X == x)))

```


:::

## Expectations {.smaller}

::: panel-tabset
### Introduction

::: incremental
-   An expectation is a kind of average, typically a weighted average
-   Expectation is a single number summary of the distribution
-   When computing a weighted average, the weights must add up to one
-   That's fortunate for us since the PMF satisfies this property
-   So, the expectation of a discrete RV is the sum of its values weighted by their respective probabilities
-   For a discrete RV, we have: $E(X) = \sum_{x}^{} x p_X(x)$
-   For continuous RVs we have: $E(X) = \int x p_X(x)\, dx$
:::

### Examples

-   Let's compute the expectation of the Geometric RV, $X \sim \text{Geom}(\theta)$
-   To solve it analytically, you would have to sum a slightly more complicated series
-   Here, we will just write the answer:

$$
E(X) = \sum_{x=0}^{\infty} x \theta (1 - \theta)^x = \frac{1-\theta}{\theta}
$$ 

- In particular, for $X \sim Geom(1/5)$, what is the expected number of failures before the first success?

-   The answer is $4 = (1 - 1/5) / 1/5$

### R Code

-   Let's check computationally:

```{r}
#| echo: true

theta <- 1/5
x <- 0:100
sum(x * dgeom(x = x, prob = theta))

theta <- 1/4
sum(x * dgeom(x = x, prob = theta))
```

-   We are summing up to a finite number (100), but higher terms are negligible for these choices of $\theta$

-   Now, given a value of $0 < \theta < 1$, you can compute the Geometric expectation in your head

### E(X) of Bernoulli and Binomial

::: incremental
-   If $X \sim \text{Bernoulli}(\theta)$, $E(X) = \sum_{x=0}^{1} x \theta^x (1 - \theta)^{1-x} = 0 + 1 \cdot \theta \cdot 1 = \theta$

-   For the Binomial, is it easier to think about it as a sum of iid Bernoulli RVs. Each Bernoulli RV has an expectation of $\theta$, and there are $n$ of them, so the expectation is $n \theta$.

-   This argument relies on the linearity of expectations: $E(X_1 + X_2 ... + X_n) = E(X_1) + E(X_2) + ... + E(X_n) = \theta + ... + \theta = n\theta$
:::
:::

## Indicator Random Variable {.smaller}

::: panel-tabset
### Recall our $\pi$ Simulation

$$
I_A(s) = I(s \in A) = \left\{\begin{matrix}
1 & \text{if } s \in A \\ 
0 & \text{if } s \notin A
\end{matrix}\right.
$$

Recall from Lecture 1:

$$
\begin{align}
X& \sim \text{Uniform}(-1, 1) \\
Y& \sim \text{Uniform}(-1, 1) \\
\pi& \approx \frac{4 \sum_{i=1}^{N} \text{I}(x_i^2 + y_i^2 < 1)}{N}
\end{align}
$$

### Fundamental Bridge

- There is a link between probability and indicator RVs
- Joe Blitzstein calls it the fundamental bridge

$$
P(A) = E(I_A)
$$
```{r}
#| echo: true
#| cache: true

# Pr of two heads in five trials
(P2 <- dbinom(2, 5, prob = 1/2))

# Let's create a realization Binom(5, 1/2)
x <- rbinom(1e6, size = 5, prob = 1/2)

# indicator RV: if x == 2 Ix = TRUE (1), else Ix = (0)
I2 <- x == 2

# compute E(Ix) == P2
mean(I2)
```

:::


## Continuous RVs and the Uniform {.smaller}

::: panel-tabset
### Definitions

-   We leave the discrete world and enter continuous RVs
-   We can no longer say, $P(X = x)$, since for a continuous RV $P(X = x) = 0$ for all $x$
-   Instead of PMFs, we will be working with PDFs, and we get probability out of them by integrating over the region that we care about
-   For a continuous RV: $\int_{-\infty}^{\infty} p_X(x)\, dx = 1$

$$
\begin{eqnarray}
P(a < X < b) & = & \int_{a}^{b} p_X(x)\, dx \\
F_X(x) & = & \int_{-\infty}^{x} p_X(u)\, du
\end{eqnarray}
$$

### Uniform

Uniform $X \sim \text{Uniform}(\alpha, \beta)$ has the following PDF: 

$$
\text{Uniform}(x|\alpha,\beta) =
\frac{1}{\beta - \alpha} 
$$

-   Where $\alpha \in \mathbb{R}$ and $\beta \in (\alpha,\infty)$

```{r}
#| echo: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center

x <- seq(-0.5, 1.5, length = 100)
pdf_x <- dunif(x, min = 0, max = 1)
p <- ggplot(data.frame(x, pdf_x), aes(x, pdf_x))
p + geom_line() + ylab(expression(p[X](x))) +
  ggtitle("X ~ Unif(0, 1)", subtitle = expression(PDF: p[X](x) == 1))
```

### Example: Stick breaking

-   You break a unit-length stick at a random point
-   Let $X$ be the breaking point so $X \sim \text{Uniform}(0, 1)$
-   Let $Y$ be the larger piece. What is E(Y)?
-   [LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician), says that we do not need $p_Y$, we can work with $p_X$.

$$
E(Y)= \int y(x) p_X(x) dx
$$ 

- This works only if $Y$ is a function of $X$. Here, $Y = \max\{X, 1 - X\}$. 
- We consider two cases: when $X$ is larger than $1/2$, it is between $1/2$ and one and when $1- X$ is larger, $X$ is between 0 and 1/2. Since $X$ is Uniform, $p_X(x) = 1$:

$$
y(x) = \left\{ 
\begin{array}{ll} x &
\text{if } 1/2 < x < 1, \text{ and} \\ 1 - x & 0 < x < 1/2
\end{array}  \right.
$$

-   In other words, $E(Y)$ can be computed as the sum of two integrals:

$$
E(Y) = \int_{0}^{1} y(x) p_X(x)\, dx =    \\
\int_{1/2}^{1} x\, dx + \int_{0}^{1/2} (1-x)\, dx = \\
\frac{x^2}{2} \Biggr|_{1/2}^{1} + \frac{1}{2} - \frac{x^2}{2} \Biggr|_{0}^{1/2} = \frac{3}{4}
$$

-   Let's do a quick simulation in R

```{r}
#| echo: true

x <- runif(1e4, min = 0, max = 1) # pick a random breaking point on (0, 1)
y <- ifelse(x > 0.5, x, 1 - x)    # generate y = max(x, 1-x)
mean(y) |> round(3)              # estimate the expectation E(Y)
```

:::

## Variance {.smaller}

::: panel-tabset
### Definitions

::: incremental
-   Variance is a measure of the spread of the distribution $E(X - E(X))^2 = E(X^2) - E(X)^2$ (Derive it?)
-   If we let $\mu = E(X)$, this becomes: $E(X - \mu)^2 = E(X^2) - \mu^2$ 
-   Unlike Expectation, Variance is not a linear operator. In particular $\text{Var}(cX) = c^2 \text{Var}(X)$ (Derive it?)
-   $\text{Var}(c + X) = \text{Var}(X)$: constants don't vary
-   If $X$ and $Y$ are independent $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$, but unlike for Expectations, this is not true in general
-   Square root of Variance is called a standard deviation $\text{sd} := \sqrt{\text{Var}}$, which is easier to interpret as it is expressed in the same units as data $x$.
:::

### Computation

-   In R, estimated Variance can be computed with `var()` and standard deviation with `sd()`
-   For reference, the variance of the Geometric distribution is: $(1 - \theta)/\theta^2$

```{r}
#| echo: true
n <- 1e5; theta1 <- 1/6; theta2 <- 1/3
x <- rgeom(n, prob = theta1)
var(x) |> round(2)
y <- rgeom(n, prob = theta2)
var(y) |> round(2)
(var(x) + var(y)) |> round(2)
var(x + y) |> round(2)
cov(x, y) |> round(2)
(var(x) + var(y) + 2*cov(x, y)) |> round(2)
# compare to the analytic result
(1 - theta1) / (theta1)^2 + (1 - theta2) / (theta2)^2 
```
:::

## Normal RV {.smaller}

::: panel-tabset
### Normal

-   $X \sim \text{Normal}(\mu, \sigma)$ has the following PDF:

$$
\text{Normal}(x \mid \mu,\sigma) = \frac{1}{\sqrt{2 \pi} \
\sigma} \exp\left( - \, \frac{1}{2} \left(  \frac{x - \mu}{\sigma} \right)^2    \right) \!
$$ 
- The bell shape comes from the $\exp(-x^2)$ part 

- The expected value is $E(X) = \mu$, the mode (highest peak) and median are also $\mu$. - Variance is $\text{Var}(X) = \sigma^2$ and standard deviation $\text{sd} = \sigma$ 

- A Normal RV can be converted to standard normal by subtracting $\mu$ and dividing by $\sigma$.

$$
\text{Normal}(x \mid 0, 1) \ = \
\frac{1}{\sqrt{2 \pi}} \, \exp \left( \frac{-x^2}{2} \right)\
$$

### Properties of Standard Normal

![](images/std-normal.png){fig-align="center"}

### Example: Heights of US adults

-   Sums of small contributions tend to have a normal distribution
-   Heights of people stratified by gender is a good example
-   The following data come from "Teaching Statistics, A Bag of Tricks" by Gelman and Nolan

```{r}
#| echo: true
#| fig-width: 5
#| fig-height: 4
#| fig-align: center


mu_m <- 69.1   # mean heights of US males in inches
sigma_m <- 2.9 # standard deviation for same
mu_w <- 63.7   # mean heights of US women in inches
sigma_w <- 2.7 # standard deviation of the same

x <- seq(50, 80, len = 1e3)
pdf_m <- dnorm(x, mean = mu_m, sd = sigma_m)
pdf_w <- dnorm(x, mean = mu_w, sd = sigma_w)
p <- ggplot(data.frame(x, pdf_m, pdf_w), aes(x, pdf_m))
p <- p + geom_line(size = 0.2, color = 'red') + 
  geom_line(aes(y = pdf_w), size = 0.2, color = 'blue') +
  xlab("Height (in)") + ylab("") +
  ggtitle("Distribution of heights of US adults") +
  annotate("text", x = 73.5, y = 0.10, label = "Men", color = 'red') +
  annotate("text", x = 58, y = 0.10, label = "Women", color = 'blue') +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

print(p)
```

-   The combined distribution is not Normal:

```{r}
#| fig-width: 5
#| fig-height: 4
#| fig-align: center

n_m <- 1e5 * 0.48
n_w <- 1e5 * 0.52

h_m <- rnorm(n_m, mu_m, sigma_m)
h_w <- rnorm(n_w, mu_w, sigma_w)
h <- c(h_m, h_w)

p <- ggplot(data.frame(h), aes(x = h))
p + geom_density(size = 0.2, bw = 0.5) + 
  xlab("Height (in)") + ylab("") + ylim(0, 0.15) +
  ggtitle("Combined distribution of men and women") +
  theme(axis.text.y = element_blank(),
  axis.ticks.y = element_blank())
```

-   You randomly sample a man from the population. What is $P(\text{Height}_m < 65)$

```{r}
#| echo: true

integrate(dnorm, lower = -Inf, upper = 65, 
          mean = mu_m, sd = sigma_m)$value |>
  round(2)
```

-   You randomly sample a woman from the population. What is $P(60 < \text{Height}_w < 70)$

```{r}
#| echo: true

integrate(dnorm, lower = 60, upper = 70, 
          mean = mu_w, sd = sigma_w)$value |>
  round(2)

(integrate(dnorm, lower = -Inf, upper = 70, 
          mean = mu_w, sd = sigma_w)$value - 
integrate(dnorm, lower = -Inf, upper = 60, 
          mean = mu_w, sd = sigma_w)$value) |>
  round(2)
```

-   What is the probability that a randomly chosen man is taller than a randomly chosen woman?

-   We have two distributions $M \sim \text{Normal}(\mu_m, \sigma_m)$ and $W \sim \text{Normal}(\mu_w, \sigma_w)$. We want $P(M > W) = P(Z > 0),\, \text{where }Z = M - W$

-   Sum of two normals is normal where both means and variances sum:

$$
\begin{eqnarray}
Z & \sim & \text{Normal}(\mu_m + \mu_w,\, \sigma_m^2 + \sigma_m^2) \\
E(Z) & = & E(M - W) = E(M) - E(W) = 69.1 - 63.7 = 5.4 \\
\text{Var}(Z) & = & \text{Var}(M - W) = \text{Var}(M) + \text{Var}(W) =  2.9^2 + 2.7^2 = 15.7 \\
\text{sd} & = & \sqrt{Var} = \sqrt{15.7} =3.96 \\
Z & \sim & \text{Normal}(5.4, 3.96)
\end{eqnarray}
$$ 
- To figure out when $Z > 0$, we can integrate the PDF from 0 to Infinity:

```{r}
#| echo: true

integrate(dnorm, lower = 0, upper = Inf, 
          mean = 5.4, sd = sqrt(15.7))$value |>
  round(2)
```

-   We don't have to integrate. We can evaluate the CDF instead:

```{r}
#| echo: true
1 - pnorm(0, mean = 5.4, sd = sqrt(15.7)) |>
  round(2)
```

-   By symmetry, the probability that a randomly chosen woman is taller than a randomly chosen man is 0.09

-   How can we check the analytic solution? We can do a simulation!

```{r}
#| echo: true
#| cache: true

n <- 1e5
taller_m <- numeric(n)
for (i in 1:n) {
  height_m <- rnorm(1, mu_m, sigma_m)
  height_w <- rnorm(1, mu_w, sigma_w)
  taller_m[i] <- height_m > height_w
}
mean(taller_m) |> round(2)
```

-   Suppose you wanted to compute the variance this way. How would you do it?
:::

## What We Did Not Cover

-   Poisson RV
-   Joint, Marginal, and Conditional
-   Covariance and correlation

## Homework {.smaller}

- Compute and plot a Binompial(10, 1/3) PMF. Compute the CDF. Sum the PMF to make sure it matches the CDF

- Take a look at the dataset `mtcars`

- Plot the relationship between `mpg` and `hp` and `mpg` and `wt`
- Now plot the same for each `cyl` (number of cylinders)
- What have you learned?

The following is from "All of Statistics" by Larry Wasserman.

- Consider tossing a fair six-sided die. Let A = {2, 4, 6} and B = {1, 2, 3, 4}. Compute P(A), P(B), and P(AB), where P(AB) means P(A and B). Are A and B independent? Why yes or why no?

- Simulate draws from the sample space and verify that P(AB) = P(A)P(B) where P(A) is the proportion of times A occurred in the simulation and similarly for P(AB) and P(B). Now find two events, A and B, that are not independent. Compute P(A), P(B), and P(AB). Compare the calculated values to their theoretical values.

## Homework Solutions {.smaller}

```{r}
#| echo: true

library(MASS)
library(dplyr)
library(purrr) # for the map() function

# question 1
pmf <- dbinom(x = 0:10, size = 10, prob = 1/3)
plot(pmf)

cumsum(pmf)
pbinom(0:10, size = 10, prob = 1/3)

# question 2
p <- ggplot(mtcars, aes(hp, mpg))
p + geom_point(aes(color = as.factor(cyl)))
p <- ggplot(mtcars, aes(wt, mpg))
p + geom_point(aes(color = as.factor(cyl)))

# question 3
die <- 1:6
A <- c(2, 4, 6)       # P(A) = 1/2
B <- c(1, 2, 3, 4)    # P(B) = 2/3
AB <- intersect(A, B) # P(AB) = 1/3
1/2 * 2/3 |> fractions() # events A and B are independent

roll <- function(n) {
  sample(die, size = n, prob = rep(1/6, 6), replace = TRUE)
}

n <- 1e6
x <- roll(n)

# check that we have the right convergence
# should be ~ 0.16 for each number
die |> map(~ mean(x == .x))
# avove is the same as:
for (i in seq_along(die)) {
  cat(mean(x == i))
}

# compute P(A), A = {2, 4, 6}
(P_A <- mean(x %in% A))

# compute P(B), B = {1, 2, 3, 4}
(P_B <- mean(x %in% B))

# compute P(AB), AB = {2, 4}
(P_AB <- mean(x %in% AB)) |> round(2)
(P_A * P_B) |> round(2)

# not independent: C = {1, 2, 3}, D = {3, 4, 5}
C <- c(1, 2, 3) # P(C) = 1/2
D <- c(3, 4, 5) # P(D) = 1/2
CD <- intersect(C, D) # P(CD) = 1/6 not equal to 1/2 * 1/2

(P_C <- mean(x %in% C))
(P_D <- mean(x %in% D))
(P_CD <- mean(x %in% CD))

# P(CD) = P(D|C) * P(C)
# We need to compute P(D|C): theoretically, it should be 1/6 * 2 = 1/3
(P_D_given_C <- P_CD / P_C)

# But we can compute P(D|C) directly from the sample:
mean(x[x %in% C] %in% D)
```

- What the last statement is saying: only look a the part of the sample space where C already happened. That means `x[x %in% C]` will contain only 1s, 2s, and 3s
- In that space, tell me the proportion of times when $D$ happens. ($D$ happens whenever you see a 3 or 4 or 5, so only 3 is relevant here)
- By definition this is $P(D | C)$




