[
  {
    "objectID": "smac.html",
    "href": "smac.html",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "",
    "text": "This is the home for SMaC: Statistics, Math, and Computing Bootcamp offered by NYU PRIISM, the Center for Practice and Research at the Intersection of Information, Society, and Methodology. Registered students will be given access to Brightspace, where we will host an online forum.\nSMaC aims to prepare students for the Applied Statistics for Social Science Research program at NYU. We will cover basic programming using the R language, including data manipulation and graphical displays; some key ideas from Calculus, including differentiation and integration; basic matrix algebra, including vector and matrix arithmetic; some core concepts in Probability, including random variables, discrete and continuous distributions, and expectations; and a few simple regression examples.\nThis is the current version of the syllabus, which is subject to change, so please check the date to make sure you have the most recent version.\n\nSession 01: The Big Picture and Introduction to R\nSession 02: Plotting, Exponentials, Logs, and Derivatives\nSession 03: Reshaping Data, Loops, and Maps; Introduction to Integration\nSession 04: Introduction to Probability 1\nSession 05: Introduction to Probability 2\nSession 06: Introduction to Probability 3\nSession 07: Introduction to Linear Algebra\nSession 08: Introduction to Inference and Regression 1\nSession 09: Introduction to Inference and Regression 2\nSession 10: Review, Discussion, and Q&A"
  },
  {
    "objectID": "bayes-course.html",
    "href": "bayes-course.html",
    "title": "APSTA-GE 2123: Bayesian Inference",
    "section": "",
    "text": "This is the home of the Spring 2024 Bayesian Inference class at NYU Steinhardt. If you are enrolled in the course, the assignment will be posted on Brightspace.\nThis is the current version of the syllabus. Please check the date, as it is subject to change.\n\nLectures\nThe following is a preliminary lecture plan. We will add the links after each lecture.\n\nLecture 01: Introduction and Bayesian Workflow\nLecture 02: Conjugate Models and Beta-Binomial\nLecture 03: More Conjugate Models and Introduction to Posterior Sampling\nLecture 04: MCMC, Posterior Inference, and Prediction\nLecture 05: Linear Regression and Model Evaluation\nLecture 06: Expanding the Linear Model and Modeling Counts\nLecture 07: Logistic regression and introduction to hierarchical models\n\nThe final project is due at the end of the semester, and the presentations will take place during finals week. Use these guidelines when working on your proposal and the final report.\n\n\nBackground Resources\nThe following resources may be helpful to those who need a refresher on the prerequisites.\n\nR for Data Science 2e, Wickham et al. \nCalculus Made Easy, Thompson\nCalculus, Strang et al.\nYouTube: Essence of Calculus, Sanderson\nIntoduction to Probability, Blitzstein et al.\nIntroduction to Probability Cheatsheet v2, Chen et al.\n\n\n\nAdditional Bayesian Resources\n\nStatistical Rethinking, McElreath\nA Student’s Guide to Bayesian Statistics, Lambert\nBayesian Data Analysis, Gelman et al.\nStan User’s Guide, Stan Development Team"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "href": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "title": "Bayesian Inference",
    "section": "MCMC, Posterior inference, and Prediction",
    "text": "MCMC, Posterior inference, and Prediction\n\n\nMetropolis-Hastings-Rosenbluth algorithm\nR implementation and testing\nComputational issues\nWhy does MHR work\nPosterior predictive distribution\nPosterior predictive simulation in Stan\nOptimization and code breaking with MCMC\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "title": "Bayesian Inference",
    "section": "High-Level Outline of MHR",
    "text": "High-Level Outline of MHR\n\n\n\n\nThe idea is to spend “more time” in the area of high posterior volume\nPick a random starting point at \\(i=1\\) with \\(\\theta^{(1)}\\)\nPropose a next possible value of \\(\\theta\\), call it \\(\\theta'\\) from an (easy) proposal distribution\nEvaluate if you should accept or reject the proposal\nIf accepted, go to \\(\\theta^{'(2)}\\), otherwise stay at \\(\\theta^{(2)} = \\theta^{(1)}\\)\nRinse and repeat\n\n\n\n\n\nIf we can get an independant draw from \\(\\theta\\), we just take it\nThat amounts to regular Monte Carlo sampling, like rnorm(1, mean = 0, sd = 1)\nOtherwise, we need a rule for evaluating when to accept and when to reject the proposal\nWe still need a way to evaluate the density at the proposed value (it will be part of the rule)\nThe big idea: if the proposed \\(\\theta'\\) has a higher plausibility than the current \\(\\theta\\), accept \\(\\theta'\\); if not, sometimes accept \\(\\theta'\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "href": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "title": "Bayesian Inference",
    "section": "Normal-Normal with Known \\(\\sigma\\)",
    "text": "Normal-Normal with Known \\(\\sigma\\)\n\n\nWe will start with a known posterior\nLet \\(y \\sim \\text{Normal}(\\mu, 0.5)\\)\nLet the prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nLet’s say we observe \\(y = 5\\)\nWe can immediately compute the posterior \\(\\mu \\mid y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\n\nnormal_normal_post &lt;- function(y, sd, prior_mu, prior_sd) {\n# for the case where sd is known\n  prior_prec &lt;- 1/prior_sd^2 \n  data_prec &lt;- 1/sd^2\n  n &lt;- length(y)\n  post_mu &lt;- (prior_prec * prior_mu + data_prec * n * mean(y)) /\n             (prior_prec + n * data_prec)\n  post_prec &lt;- prior_prec + n * data_prec\n  post_sd &lt;- sqrt(1/post_prec)\n  return(list(post_mu = post_mu, post_sd = post_sd))\n}\n\nnormal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2) |&gt;\n  unlist() |&gt; round(2)\n\npost_mu post_sd \n   4.71    0.49"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "title": "Bayesian Inference",
    "section": "Metropolis-Hastings-Rosenbluth (MHR)",
    "text": "Metropolis-Hastings-Rosenbluth (MHR)\n\nMHRNormal Symmetry\n\n\n\n\nWe are trying to generate draws: \\(\\left( \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)}\\right)\\) implied by the target density \\(f\\)\n\nPick the first value of \\(\\theta\\) randomly or deterministically\nDraw \\(\\theta'\\) from a proposal distribution \\(q(\\theta'\\mid\\theta)\\)\nCompute the unnormalized \\(f(\\theta'\\mid y) \\propto f(y \\mid \\theta') f(\\theta') = f_{\\text{prop}}\\)\nCompute the unnormalized \\(f(\\theta \\mid y) \\propto f(y \\mid \\theta) f(\\theta) = f_{\\text{current}}\\)\nCompute \\(\\text{ratio}_f = \\frac{f_{\\text{prop}}}{f_{\\text{current}}}\\) and \\(\\text{ratio}_q = \\frac{q(\\theta\\mid\\theta')}{q(\\theta'\\mid\\theta)}\\)\nAsseptance probability \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\cdot \\text{ratio}_q \\right\\rbrace\\)\nIf \\(q\\) is symmetric, we can drop \\(\\text{ratio}_q\\), in which case \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\right\\rbrace\\)\nIf \\(\\text{ratio} \\geq 1\\), accept \\(\\theta'\\), else flip a coin with \\(\\text{Pr}(X=1) = \\alpha\\) and accept \\(\\theta'\\) if \\(X=1\\), stay with current \\(\\theta\\), if \\(X=0\\)\n\n\n\n\n\n\nWhy in case of normals, \\(q(\\theta'\\mid\\theta) = q(\\theta\\mid\\theta')\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndnorm(1, mean = 0)\n\n[1] 0.2419707\n\ndnorm(0, mean = 1)\n\n[1] 0.2419707"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "title": "Bayesian Inference",
    "section": "MHR in R",
    "text": "MHR in R\n\nMHR IterationMHR for N Iterations\n\n\n\nmetropolis_iteration_1 &lt;- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean &lt;- rnorm(n = 1, mean = current_mean,  sd = proposal_scale) # q(mu' | mu)  \n  f_proposal    &lt;- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     &lt;- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio &lt;- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha &lt;- min(ratio, 1)     \n  \n  if (alpha &gt; runif(1)) {         # this is just another way of flipping a coint\n    next_value &lt;- proposal_mean  \n  } else {\n    next_value &lt;- current_mean\n  }\n  return(next_value)\n}\n\n\n\n\nmhr &lt;- function(y, f, N, start, ...) {\n# y: new observation\n# f: function that implements one MHR iteration\n# N: number of iterations\n# start: initial value of the chain\n# ...: additional arguments to f\n  \n  draws &lt;- numeric(N)\n  draws[1] &lt;- f(y, current_mean = start, ...)\n  \n  for (i in 2:N) {\n    draws[i] &lt;- f(y, current_mean = draws[i - 1], ...)\n  }\n  \n  return(draws)\n}\n\ny &lt;- 5; N &lt;- 5e3; start &lt;- 3\nd &lt;- mhr(y, N, f = metropolis_iteration_1, start, proposal_scale = 2, sd = 0.5,  \n         prior_mean = 0, prior_sd = 2)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "href": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "title": "Bayesian Inference",
    "section": "Markov Chain Animation",
    "text": "Markov Chain Animation\n   Autocorrelation function (ACF)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#comparing-samples-to-the-true-posterior",
    "href": "bayes-course/04-lecture/04-lecture.html#comparing-samples-to-the-true-posterior",
    "title": "Bayesian Inference",
    "section": "Comparing Samples to the True Posterior",
    "text": "Comparing Samples to the True Posterior\n\n\n\n\nnp &lt;- normal_normal_post(y = y, \n                         sd = 0.5, \n                         prior_mu = 0, \n                         prior_sd = 2)\n\ntheta &lt;- seq(np$post_mu + 3*np$post_sd, \n             np$post_mu - 3*np$post_sd, \n             len = 100)\n\ndn &lt;- dnorm(theta, mean = np$post_mu, \n                     sd = np$post_sd)\n\np &lt;- ggplot(aes(d), data = tibble(d = d))\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 25, alpha = 0.6) +\n  geom_line(aes(theta, dn), linewidth = 0.5, \n            color = 'red', \n            data = tibble(theta, dn)) + \n  ylab(\"\") + xlab(expression(theta)) + \n  ggtitle(\"MHR draws vs Normal(4.71, 0.49)\")"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "href": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "title": "Bayesian Inference",
    "section": "What is Wrong with this Code",
    "text": "What is Wrong with this Code\n\n\n\nmetropolis_iteration_1 &lt;- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean &lt;- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        \n  f_proposal    &lt;- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     &lt;- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio &lt;- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha &lt;- min(ratio, 1)     \n  \n  if (alpha &gt; runif(1)) {         # this is just another way of flipping a coint\n    next_value &lt;- proposal_mean  \n  } else {\n    next_value &lt;- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "href": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "title": "Bayesian Inference",
    "section": "What if Y is a vector?",
    "text": "What if Y is a vector?\n\n\nRecall the likelihood of many independent observations, is the product of their individual likelihoods \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\nf(\\mu \\mid y) & \\propto & \\text{prior} \\cdot \\prod_{i=1}^{n}\\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\end{eqnarray}\n\\]\nThis suggests the following change:\n\n\n\n\nmetropolis_iteration_2 &lt;- function(y, current_mean, proposal_scale, sd,\n                                 prior_mean, prior_sd) {\n\n  proposal_mean &lt;- rnorm(1, current_mean,  proposal_scale)        \n  f_proposal    &lt;- dnorm(proposal_mean, mean = prior_mean, sd = prior_sd) * \n                   prod(dnorm(y, mean = proposal_mean, sd = sd))   \n  f_current     &lt;- dnorm(current_mean, mean = prior_mean, sd = prior_sd) *        \n                   prod(dnorm(y, mean = current_mean, sd = sd))\n\n  if ((f_proposal || f_current) == 0) {\n    return(\"Error: underflow\") # on my computer double.xmin = 2.225074e-308\n  }\n  ratio &lt;- f_proposal / f_current \n  alpha &lt;- min(ratio, 1)     \n  \n  if (alpha &gt; runif(1)) {         # definitely go if f_mu_prime &gt; f_mu: alpha = 1\n    next_value &lt;- proposal_mean   # if alpha &lt; 1, go if alpha &gt; U(0, 1)\n  } else {\n    next_value &lt;- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "href": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "title": "Bayesian Inference",
    "section": "Handling Underflow",
    "text": "Handling Underflow\n\n\nset.seed(123)\ny &lt;- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |&gt; unlist() |&gt;\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\nreplicate(20, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n [1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n [5] \"Error: underflow\" \"1.66235834591796\" \"Error: underflow\" \"2.55673000751367\"\n [9] \"Error: underflow\" \"Error: underflow\" \"2.51354952759192\" \"Error: underflow\"\n[13] \"Error: underflow\" \"Error: underflow\" \"3.02905649937656\" \"Error: underflow\"\n[17] \"Error: underflow\" \"2.82278258359193\" \"2.67485301455861\" \"Error: underflow\""
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "Always Compute on the Log Scale",
    "text": "Always Compute on the Log Scale\n\n\nFor one observation \\(y\\): \\[\n\\begin{eqnarray}\n\\log f(y \\mid \\mu) & \\propto & \\log \\left( \\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\right) \\\\\n& = & -\\log(\\sigma) - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2\n\\end{eqnarray}\n\\]\n\n\n\ndnorm_log &lt;- function(y, mean = 0, sd = 1) {\n# dropping constant terms; not needed for MCMC\n  -log(sd) - 0.5 * ((y - mean) / sd)^2\n}\n\n\n\nFor multiple observations, you can just sum the log-likelihood"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "MHR on the Log Scale",
    "text": "MHR on the Log Scale\n\nmetropolis_iteration_log &lt;- function(y, current_mean, proposal_scale, \n                                     sd, prior_mean, prior_sd) {\n  \n  # draw a proposal q(proposal_mean | current_mean)\n  proposal_mean  &lt;- rnorm(1, current_mean,  proposal_scale)           \n  \n  # construct a proposal: f(mu') * \\prod f(y_i | mu') on the log scale\n  proposal_lik   &lt;- sum(dnorm_log(y, mean = proposal_mean, sd = sd))\n  proposal_prior &lt;- dnorm_log(proposal_mean, mean = prior_mean, sd = prior_sd)\n  f_proposal     &lt;- proposal_prior + proposal_lik\n  \n  # construct a current: f(mu) * \\prod f(y_i | mu) on the log scale\n  current_lik    &lt;- sum(dnorm_log(y, mean = current_mean, sd = sd))\n  current_prior  &lt;- dnorm_log(current_mean, mean = prior_mean, sd = prior_sd)\n  f_current      &lt;- current_prior + current_lik\n  \n  # ratio on the log scale = difference of the logs\n  log_ratio &lt;- f_proposal - f_current\n  log_alpha &lt;- min(log_ratio, 0)     \n  \n  if (log_alpha &gt; log(runif(1))) {  \n    next_value &lt;- proposal_mean     \n  } else {\n    next_value &lt;- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "href": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "title": "Bayesian Inference",
    "section": "Checking the Work",
    "text": "Checking the Work\n\n\nset.seed(123)\ny &lt;- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |&gt; unlist() |&gt;\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\n\n\n\n\nreplicate(4, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n\n\n\n\n\nreplicate(4, metropolis_iteration_log(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] 1.000000 1.000000 1.423961 2.379762\n\n\n\n\n\nd &lt;- mhr(y, N, f = metropolis_iteration_log, start, proposal_scale = 2, sd = 0.55,  prior_mean = 0, prior_sd = 1)\nmean(d) |&gt; round(2)\n\n[1] 2.02\n\nsd(d) |&gt; round(2)\n\n[1] 0.03"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nTo show why the algorithm works, we need to show:\n\nThe chain has stationary distribution and it is unique\nThe stationary distribution is our target distribution \\(f(\\theta \\mid y)\\)\n\nCondition (a) requires some theory of Markov Chains, but the conditions are mild and are generally satisfied in practice. (See Chapters 11 and 12 in Blitzstein and Hwang for more)\n\nWe will show an outline of the proof for (b)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nWe will only consider the case of symmetric q\nSuppose you sample two points from the PMF \\(f(\\theta \\mid y)\\), \\(\\theta_a\\) and \\(\\theta_b\\) and assume we are at time \\(t-1\\)\nLet the probability of going from \\(\\theta_a\\) to \\(\\theta_b\\) be: \\(\\P(\\theta^t = \\theta_b) \\mid \\theta^{t-1} = \\theta_a) := p_{a b}\\) and the reverse jump: \\(\\P(\\theta^t = \\theta_a) \\mid \\theta^{t-1} = \\theta_b) := p_{b a}\\)\nWe want to show: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} ,\\, \\text{where} \\\\\np_{a  b} &=& q(\\theta_b \\mid \\theta_a) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace \\\\\np_{b  a} &=& q(\\theta_a \\mid \\theta_b) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nFor symmetric q: \\(q(\\theta_b | \\theta_a) = q(\\theta_a | \\theta_b)\\) and: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{\\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace}{\\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace}\n\\end{eqnarray}\n\\]\nConsider the case when \\(f(\\theta_b \\mid y) &gt; f(\\theta_a \\mid y)\\) \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{1}{\\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)}} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\end{eqnarray}\n\\]\nWhen \\(f(\\theta_a \\mid y) &gt; f(\\theta_b \\mid y)\\) \\[\n\\frac{p_{a  b}}{p_{b  a}} = \\frac{\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}}{1} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nRecall the prior predictive distribution, before observing \\(y\\), that appears in the denominator of Bayes’s rule: \\[\nf(y) = \\int f(y, \\theta) \\, d\\theta = \\int f(\\theta) f(y \\mid \\theta) \\, d\\theta\n\\]\nA posterior predictive distribution, \\(f(\\tilde{y} | y)\\) is obtained in a similar manner \\[\n\\begin{eqnarray}\nf(\\tilde{y} \\mid y) &=& \\int f(\\tilde{y}, \\theta \\mid y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta, y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta \\\\\n\\end{eqnarray}\n\\]\n\\(f(\\tilde{y} \\mid \\theta, y) = f(\\tilde{y} \\mid \\theta)\\) since \\(y \\perp\\!\\!\\!\\perp \\tilde{y} \\mid \\theta\\) (conditional indepedence)\nTwo sources of variability are accounted for: sampling variability in \\(\\tilde{y}\\) weighted by posterior variability in \\(\\theta\\)\nGiven draws from \\(f(\\theta \\mid y)\\), \\(\\theta^{(m)} \\sim f(\\theta \\mid y)\\), we can compute the integral in a usual way: \\(f(\\tilde{y} \\mid y) \\approx \\frac{1}{M} \\sum_{m = 1}^M f(\\tilde{y} \\mid \\theta^{(m)})\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nFor simple models we can often evalute \\(\\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta\\) directly\nWe will use an inderect approach using our example Normal-Normal model with known variance\nSince we already know \\(f(\\theta | y)\\), we will sample \\(\\theta\\) from it, and then sample \\(\\tilde{y}\\), from \\(f(\\tilde{y} \\mid \\theta)\\)\nRecall, \\(y \\sim \\text{Normal}(\\mu, 0.5)\\) with prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nFor \\(y = 5\\), the posterior \\(\\mu | y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\n\nppd &lt;- function(post_mu, post_sd) {\n  mu &lt;- rnorm(1, mean = post_mu, sd = post_sd)\n  y  &lt;- rnorm(1, mean = mu, sd = 0.5)\n}\npd &lt;- normal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2)\ny &lt;- replicate(1e5, ppd(pd$post_mu, pd$post_sd))\ncat(\"y_tilde | y ~ Normal(\", round(mean(y), 2), \",\", round(sd(y), 2), \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71,0.7)\n\n\n\n\n\nIt can be shown that the posterior predictive distribution will have the same mean as the posterior distribution and the variance as the sum of the posterior variance and data variance:\n\n\n\n\nppd_mu &lt;- pd$post_mu |&gt; round(2)\nppd_sigma &lt;- sqrt(pd$post_sd^2 + 0.5^2) |&gt; round(2)\ncat(\"y_tilde | y ~ Normal(\", ppd_mu, \", \", ppd_sigma, \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71, 0.7)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\n\nYou can compute posterior predictive distribution in R, Stan, and rstanarm\nHere, we will see how to do it in Stan\nWe will show an example in rstanarm in the next lecture\n\n\n\n\ndata {\n  real y;\n  real&lt;lower=0&gt; sigma;\n}\nparameters {\n  real mu;\n}\nmodel {\n  mu ~ normal(0, 2);\n  y ~ normal(mu, sigma);\n}\ngenerated quantities {\n  real y_tilde = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\n\nlibrary(cmdstanr)\n\nm1 &lt;- cmdstan_model(\"stan/normal_pred.stan\") # compile the model\ndata &lt;- list(y = 5, sigma = 0.5)\nf1 &lt;- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 2000 # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -3.47  -3.18 0.745 0.326 -4.95 -2.94  1.00    3507.    4238.\n2 mu        4.71   4.71 0.498 0.498  3.89  5.53  1.00    2899.    4051.\n3 y_tilde   4.71   4.71 0.706 0.706  3.52  5.87  1.00    4572.    5962."
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#breaking-codes-with-mcmc",
    "href": "bayes-course/04-lecture/04-lecture.html#breaking-codes-with-mcmc",
    "title": "Bayesian Inference",
    "section": "Breaking Codes with MCMC",
    "text": "Breaking Codes with MCMC\n\nDianconisCypher textAlgorithmDecoded text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#more-linear-models-and-modeling-counts",
    "href": "bayes-course/06-lecture/06-lecture.html#more-linear-models-and-modeling-counts",
    "title": "Bayesian Inference",
    "section": "More Linear Models and Modeling Counts",
    "text": "More Linear Models and Modeling Counts\n\n\n\n\nImproving the model by thinking about the DGP\nMore on model evaluation and comparison\nModeling count data with Poisson\nModel evaluation and overdispersion\nNegative binomial model for counts\nGeneralized linear models\n\n\n\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#motivating-example",
    "href": "bayes-course/06-lecture/06-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nAt the end of the last lecture, we saw that the linear model did not capture the relationship between height and weight very well\nThat’s not surprising: the process can’t be linear as it has a natural lower and upper bound\nTo remedy this situation, we have to think generatively: either biologically or geometrically/physically\nThe biology of growth is very complex – we would have to think about what causes primate (or animal) growth and how growth translates into height and weight, which is likely affected by genetic and environmental factors\nFortunately, there is a more straightforward geometrical approach\nRichard McElreath has a nice presentation in Chapter 16 of his book (2nd edition) – we reproduce a simplified version here"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#deriving-the-model",
    "href": "bayes-course/06-lecture/06-lecture.html#deriving-the-model",
    "title": "Bayesian Inference",
    "section": "Deriving the model",
    "text": "Deriving the model\n\n\n\n\nIn the spirit of the spherical cow, we can think of a person as a cylinder\nThe volume of the cylinder is: \\(V = \\pi r^2 h\\), where \\(r\\) is a person’s radius and \\(h\\) is the height\nIt seems reasonable to assume that a person’s width (\\(2r\\)) is proportional to the height \\(h\\): \\(r = kh\\) where \\(k\\) is the proportionality constant\nTherefore: \\(V = \\pi r^2 h = \\pi (kh)^2 h = \\theta h^3\\) where \\(\\theta\\) absorbed other constant terms\nIf the human body has approximately the same density, weight should be proportional to Volume: \\(w = kV\\), \\(w = k\\theta h^3\\)\nWe will absorbe \\(k\\) into \\(\\theta\\), and so \\(w = \\theta h^3\\), so the weight is proportional to the cube of height\n\n\n\n\n\n\nKeenan Crane; GIF by username:Nepluno - https://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/#spot"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#deriving-the-model-1",
    "href": "bayes-course/06-lecture/06-lecture.html#deriving-the-model-1",
    "title": "Bayesian Inference",
    "section": "Deriving the model",
    "text": "Deriving the model\n\n\nWe can write the model in the following way: \\[\n\\begin{eqnarray}\nw_i & \\sim & \\text{LogNormal}(\\mu_i, \\sigma)  \\\\\n\\exp(\\mu_i) & = & \\theta h_i^3 \\\\\n\\theta & \\sim & \\text{prior}_{\\theta}(.) \\\\\n\\sigma & \\sim & \\text{Exponetial}(1)\n\\end{eqnarray}\n\\]\nWeight is a positive quantity, and we give it a LogNormal distribution\n\\(\\exp(\\mu_i)\\) is the median of a LogNormal, which is where we specify our cubic relationship between weight and height\nNotice that the model for the conditional median is \\(\\mu_i = \\log(\\theta) + 3 \\log(h_i)\\), in other words, we do not need to estimate the coefficient on height, we only need the intercept\nIn RStanArm, we can estimate a similar model as a linear regression of log weight on log height; (in Stan, we can write this model directly)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#choosing-priors",
    "href": "bayes-course/06-lecture/06-lecture.html#choosing-priors",
    "title": "Bayesian Inference",
    "section": "Choosing Priors",
    "text": "Choosing Priors\n\n\nIn our log-log linear regression, we have an intercept and coefficient on log height, which we said was 3\nInstead of fixing it at 3, we will estimate it and give it an informative prior, where most of the mass is between 2 and 4\nThe implies something like \\(\\beta \\sim \\text{Normal}(3, 0.3)\\)\nWe will leave our \\(\\sigma \\sim \\text{Exponetial}(1)\\)\nWe have less intuition about the intercept, so we will give it a wider prior on a scale of centered predictors (RStanArm centers by default): \\(\\alpha \\sim \\text{Normal}(0, 5)\\)\nHow do we know these priors are reasonable on the predictive scale (weight)?\nWe will perform another prior predictive simulation"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nCompute the new log variables:\n\n\n\n\nd &lt;- readr::read_csv(\"../05-lecture/data/howell.csv\")\nd &lt;- d |&gt;\n  mutate(log_h = log(height),\n         log_w = log(weight))\n\n\n\n\nRun prior predictive simulation in RStanARM. You should write the R code directly.\n\n\n\n\nm3 &lt;- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 5),\n  prior_PD = 1,  # don't evaluate the likelihood\n  seed = 1234,\n  chains = 4,\n  iter = 600\n)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-1",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-1",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nlibrary(tidybayes)\nd |&gt;\n  add_epred_draws(m3, ndraws = 100) |&gt;\n  ggplot(aes(y = log_w, x = log_h)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = .epred, group = .draw), alpha = 0.25) +\n  xlab(\"Log Height\") + ylab(\"Log Weight\") + ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-2",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-2",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nWe can examine what this looks like on the original scale by exponentiating the predictions:\n\n\n\n\nd |&gt;\n  add_epred_draws(m3, ndraws = 100) |&gt;\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = exp(.epred), group = .draw), color = 'green', alpha = 0.25) +\n  xlab(\"Height\") + ylab(\"Weight\") + ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-3",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-3",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nOur intercept scale seems too wide, so we will make some adjustments:\n\n\n\n\n\nm3 &lt;- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 2.5),\n  prior_PD = 1,  # don't evaluate the likelihood\n  seed = 1234,\n  refresh = 0,\n  chains = 4,\n  iter = 600\n)\nd |&gt;\n  add_epred_draws(m3, ndraws = 100) |&gt;\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = exp(.epred), group = .draw), \n            alpha = 0.25, color = 'green') +\n  xlab(\"Height\") + ylab(\"Weight\") + \n  ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#fitting-the-model",
    "href": "bayes-course/06-lecture/06-lecture.html#fitting-the-model",
    "title": "Bayesian Inference",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nWe can likely do better with these priors, but most of the simulations are covering the data and so we proceed to model fitting\n\n\n\n\nm3 &lt;- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 2.5), \n  seed = 1234,\n  refresh = 0,\n  chains = 4,\n  iter = 600\n)\nprior_summary(m3)\n\nPriors for model 'm3' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = 3, scale = 0.3)\n\nAuxiliary (sigma)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\nsummary(m3)\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      log_w ~ log_h\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 544\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -8.0    0.1 -8.1  -8.0  -7.8 \nlog_h        2.3    0.0  2.3   2.3   2.4 \nsigma        0.1    0.0  0.1   0.1   0.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.4    0.0  3.4   3.4   3.5  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   619 \nlog_h         0.0  1.0   621 \nsigma         0.0  1.0   923 \nmean_PPD      0.0  1.0  1436 \nlog-posterior 0.1  1.0   529 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#comparing-to-the-linear-model",
    "href": "bayes-course/06-lecture/06-lecture.html#comparing-to-the-linear-model",
    "title": "Bayesian Inference",
    "section": "Comparing to the Linear Model",
    "text": "Comparing to the Linear Model\n\n\n\nm2 &lt;- readr::read_rds(\"../05-lecture/models/m2.rds\")\np1 &lt;- pp_check(m2) + xlab(\"Weight (kg)\") + ggtitle(\"Linear Model\")\np2 &lt;- pp_check(m3) + xlab(\"Log Weight\") + ggtitle(\"Log-Log Model\")\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#plotting-prediction-intervals",
    "href": "bayes-course/06-lecture/06-lecture.html#plotting-prediction-intervals",
    "title": "Bayesian Inference",
    "section": "Plotting Prediction Intervals",
    "text": "Plotting Prediction Intervals\n\n\nd |&gt;\n  add_predicted_draws(m3) |&gt;\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5, alpha = 0.2) +\n  stat_lineribbon(aes(y = exp(.prediction)), .width = c(0.90, 0.50), alpha = 0.25) +\n  xlab(\"Height (cm)\") + ylab(\"Weight (kg)\") + ggtitle(\"In Sample Predictions of Weight from Height\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#predicting-for-new-data",
    "href": "bayes-course/06-lecture/06-lecture.html#predicting-for-new-data",
    "title": "Bayesian Inference",
    "section": "Predicting For New Data",
    "text": "Predicting For New Data\n\n\nlog_h &lt;- seq(0, 5.2, len = 500)\nnew_data &lt;- tibble(log_h)\npred &lt;- add_predicted_draws(new_data, m3)\npred |&gt;\n  ggplot(aes(x = exp(log_h), y = exp(.prediction))) +\n  stat_lineribbon(.width = c(0.90, 0.50), alpha = 0.25) +\n  xlab(\"Height (cm)\") + ylab(\"Weight (kg)\") + ggtitle(\"Predictions of Weight from Height\") + \n  geom_point(aes(y = weight, x = height), size = 0.5, alpha = 0.2, data = d)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nTo build up a larger regression model, we will take a look at the quality of wine dataset from the UCI machine learning repository"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-1",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-1",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nOur task is to predict the (subjective) quality of wine from measurements like acidity, sugar, and chlorides\nThe outcome is ordinal, which should be analyzed using ordinal regression, but we will start with linear regression\n\n\n\n\n\nd &lt;- readr::read_delim(\"data/winequality-red.csv\")\n\n# remove duplicates\nd &lt;- d[!duplicated(d), ]\np1 &lt;- ggplot(aes(x = quality), data = d)\np1 &lt;- p1 + geom_histogram() + \n  ggtitle(\"Red wine quality ratings\")\np2 &lt;- ggplot(aes(quality, alcohol), data = d)\np2 &lt;- p2 + \n  geom_point(position = \n               position_jitter(width = 0.2),\n             size = 0.3)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-2",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-2",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nAs before, we will center the predictors, but this time, we will also divide by standard deviation\nThis will make the coefficients comparable\nIf you have binary inputs, it may make sense to divide by two standard deviations (Page 186 in Regresion and Other Stories)\nWe will also center the quality score\n\n\n\n\nds &lt;- d |&gt;\n  scale() |&gt;\n  as_tibble() |&gt;\n  mutate(quality = d$quality - 5.5)\nhead(ds)\n\n# A tibble: 6 × 12\n  fixed_acidity volatile_acidity citric_acid residual_sugar chlorides\n          &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n1        -0.524            0.932       -1.39        -0.461    -0.246 \n2        -0.294            1.92        -1.39         0.0566    0.200 \n3        -0.294            1.26        -1.19        -0.165     0.0785\n4         1.66            -1.36         1.47        -0.461    -0.266 \n5        -0.524            0.713       -1.39        -0.535    -0.266 \n6        -0.236            0.385       -1.09        -0.683    -0.387 \n# ℹ 7 more variables: free_sulfur_dioxide &lt;dbl&gt;, total_sulfur_dioxide &lt;dbl&gt;,\n#   density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;, alcohol &lt;dbl&gt;, quality &lt;dbl&gt;"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-3",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-3",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can now fit our first regression to alcohol only\nAfter standardization, and since we don’t know much about wine, we can set weakly informative priors\n\n\n\n\nm1 &lt;- stan_glm(quality ~ alcohol, \n               data = ds,\n               family = gaussian,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               iter = 500,\n               chains = 4)\nsummary(m1)\n\n\n\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ alcohol\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.1    0.0  0.1   0.1   0.2  \nalcohol     0.4    0.0  0.4   0.4   0.4  \nsigma       0.7    0.0  0.7   0.7   0.7  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.1    0.0  0.1   0.1   0.2  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  759  \nalcohol       0.0  1.0  720  \nsigma         0.0  1.0  905  \nmean_PPD      0.0  1.0  862  \nlog-posterior 0.1  1.0  373  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#priors-in-rstanarm",
    "href": "bayes-course/06-lecture/06-lecture.html#priors-in-rstanarm",
    "title": "Bayesian Inference",
    "section": "Priors in RStanArm",
    "text": "Priors in RStanArm\n\n\nWhen we say prior = normal(0, 1) in RStanArm, every \\(\\beta\\), except for the intercept will be given this prior\nWhen setting informative priors, you may want to set a specific prior for each \\(\\beta\\)\nSuppose your model is: \\[\ny_i \\sim \\mathsf{Normal}\\left(\\alpha + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}, \\, \\sigma\\right)\n\\]\n\n\n\n\nAnd you want to put a \\(\\text{Normal}(-3, 1)\\) on \\(\\beta_1\\) and \\(\\text{Normal}(2, 0.1)\\) on \\(\\beta_2\\)\n\n\n\n\nmy_prior &lt;- normal(location = c(-3, 2), scale = c(1, 0.1))\nstan_glm(y ~ x1 + x2, data = dat, prior = my_prior)\n\n\n\n\nRefer to this vignette for more information about this topic"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-4",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-4",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can look at the inference using mcmc_areas\n\n\n\n\nlibrary(bayesplot)\nmcmc_areas(m1)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-5",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-5",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nLet’s predict the rating at the high and low alcohol content\nOn a standardized scale, that would correspond to an alcohol measurement of 4 and -2 (or about 8 and 15 on the original scale)\n\n\n\n\n\nlibrary(bayesplot)\nd_new &lt;- tibble(alcohol = c(-2, 4))\npred &lt;- m1 |&gt;\n  posterior_predict(newdata = d_new) |&gt;\n  data.frame()\ncolnames(pred) &lt;- c(\"low_alc\", \"high_alc\") \npred &lt;- tidyr::pivot_longer(pred, everything(), \n                            names_to = \"alc\",\n                            values_to = \"value\")\np &lt;- ggplot(aes(x = value), \n            data = pred)\np + geom_density(aes(fill = alc, color = alc), \n                 alpha = 1/4) +\n  geom_histogram(aes(x = quality, \n                     y = after_stat(density)), \n                 alpha = 1/2,\n                 data = ds) +\n  xlab(\"Quality Score (-2.5, +2.5)\") + ylab(\"\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-6",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-6",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nNaive way to show the posterior predictive check\n\n\n\n\nyrep1 &lt;- posterior_predict(m1) # predict at every observation\nppc_dens_overlay(ds$quality, yrep1[sample(nrow(yrep1), 50), ])"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-7",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-7",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can classify each prediction based on the distance to the nearest rating category\n\n\n\n\n\nmap_real_number &lt;- function(x) {\n  if (x &lt; -2) {\n    return(-2.5)\n  } else if (x &gt;= -2 && x &lt; -1) {\n    return(-1.5)\n  } else if (x &gt;= -1 && x &lt; 0) {\n    return(-0.5)\n  } else if (x &gt;= 0 && x &lt; 1) {\n    return(0.5)\n  } else if (x &gt;= 1 && x &lt; 2) {\n    return(1.5)\n  } else if (x &gt;= 2) {\n    return(2.5)\n  }\n}\nmap_real_number &lt;- Vectorize(map_real_number)\nyrep_cat &lt;- map_real_number(yrep1) |&gt;\n  matrix(nrow = nrow(yrep1), ncol = ncol(yrep1))\nppc_dens_overlay(ds$quality, \n            yrep_cat[sample(nrow(yrep1), 50), ])"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-8",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-8",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can take a look at the distribution of a few statistics to check where the model is particularly strong or weak\n\n\n\n\np1 &lt;- ppc_stat(ds$quality, yrep1, stat = \"max\")\np2 &lt;- ppc_stat(ds$quality, yrep1, stat = \"min\")\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-9",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-9",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can also look at predictions directly, and compare them to observed data\n\n\n\n\ns &lt;- sample(nrow(yrep1), 50); \np1 &lt;- ppc_ribbon(ds$quality[s], yrep1[, s])\np2 &lt;- ppc_intervals(ds$quality[s], yrep1[, s])\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-10",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-10",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can now fit a larger model and compare the results\n\n\n\n\nm2 &lt;- stan_glm(quality ~ ., \n               data = ds,\n               family = gaussian,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               iter = 700,\n               chains = 4)\nsummary(m2)\n\n\n\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ .\n algorithm:    sampling\n sample:       1400 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   12\n\nEstimates:\n                       mean   sd   10%   50%   90%\n(Intercept)           0.1    0.0  0.1   0.1   0.1 \nfixed_acidity         0.0    0.1  0.0   0.0   0.1 \nvolatile_acidity     -0.2    0.0 -0.2  -0.2  -0.2 \ncitric_acid           0.0    0.0 -0.1   0.0   0.0 \nresidual_sugar        0.0    0.0  0.0   0.0   0.0 \nchlorides            -0.1    0.0 -0.1  -0.1  -0.1 \nfree_sulfur_dioxide   0.0    0.0  0.0   0.0   0.1 \ntotal_sulfur_dioxide -0.1    0.0 -0.1  -0.1  -0.1 \ndensity               0.0    0.0 -0.1   0.0   0.0 \npH                   -0.1    0.0 -0.1  -0.1   0.0 \nsulphates             0.2    0.0  0.1   0.2   0.2 \nalcohol               0.3    0.0  0.3   0.3   0.4 \nsigma                 0.7    0.0  0.6   0.7   0.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.1    0.0  0.1   0.1   0.2  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                     mcse Rhat n_eff\n(Intercept)          0.0  1.0  1827 \nfixed_acidity        0.0  1.0   560 \nvolatile_acidity     0.0  1.0  1056 \ncitric_acid          0.0  1.0  1057 \nresidual_sugar       0.0  1.0   926 \nchlorides            0.0  1.0  1373 \nfree_sulfur_dioxide  0.0  1.0  1025 \ntotal_sulfur_dioxide 0.0  1.0  1044 \ndensity              0.0  1.0   551 \npH                   0.0  1.0   722 \nsulphates            0.0  1.0  1244 \nalcohol              0.0  1.0   678 \nsigma                0.0  1.0  1887 \nmean_PPD             0.0  1.0  1464 \nlog-posterior        0.1  1.0   636 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-11",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-11",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can look at all the parameters in one plot, excluding sigma\n\n\n\n\nmcmc_areas(m2, pars = vars(!sigma))"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-12",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-12",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nDid we improve the model?\nWe will check accuracy using MSE for both models\nWe will also check the width of posterior intervals\nFinally, we will compare the models using PSIS-LOO CV (preferred)\n\n\n\n\nyrep2 &lt;- posterior_predict(m2)\np3 &lt;- ppc_intervals(ds$quality[s], yrep2[, s])\ngrid.arrange(p2, p3, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-13",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-13",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nComparing (Root) Mean Square Errors\n\n\n\n\n(colMeans(yrep1) - ds$quality)^2 |&gt; mean() |&gt; sqrt() |&gt; round(2)\n\n[1] 0.72\n\n(colMeans(yrep2) - ds$quality)^2 |&gt; mean() |&gt; sqrt() |&gt; round(2)\n\n[1] 0.66\n\n\n\n\n\nComparing posterior intervals\n\n\n\n\nwidth &lt;- function(yrep, q1, q2) {\n  q &lt;- apply(yrep, 2, function(x) quantile(x, probs = c(q1, q2)))\n  width &lt;- apply(q, 2, diff)\n  return(mean(width))\n}\n\nwidth(yrep1, 0.25, 0.75) |&gt; round(2)\n\n[1] 0.97\n\nwidth(yrep2, 0.25, 0.75) |&gt; round(2)\n\n[1] 0.89"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-14",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-14",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nLet’s estimate PSIS-LOO CV, a measure of out-of-sample predictive performance\n\n\n\n\n\nlibrary(loo)\noptions(mc.cores = 4)\nloo1 &lt;- loo(m1)\nloo2 &lt;- loo(m2)\npar(mar = c(3,3,2,1), \n    mgp = c(2,.7,0), \n    tck = -.01, \n    bg = \"#f0f1eb\")\npar(mfrow = c(2, 1))\nplot(loo1)\nplot(loo2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-15",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-15",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nFinally, we can compare the models using loo_compare\n\n\n\n\nloo_compare(loo1, loo2)\n\n   elpd_diff se_diff\nm2    0.0       0.0 \nm1 -117.9      17.7"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#glms-and-models-for-count-data",
    "href": "bayes-course/06-lecture/06-lecture.html#glms-and-models-for-count-data",
    "title": "Bayesian Inference",
    "section": "GLMs and Models for Count Data",
    "text": "GLMs and Models for Count Data\n\n\nModeling count data is typically part of a general GLM framework\nThe general setup is that we have:\n\nResponse vector \\(y\\), and predictor matrix \\(X\\)\nLinear predictor: \\(\\eta = X\\beta\\), where \\(X\\) is \\(N\\text{x}P\\) and \\(\\beta\\) is \\(P\\text{x}1\\). What are the dimensions of \\(\\eta\\)?\n\\(\\E(y \\mid X) = g^{-1}(\\eta)\\), where \\(g\\) is the link function that maps the linear predictor onto the observational scale\nFor linear regression, \\(g\\) is the identity function (i.e., no transformation)\nThe Poisson data model is \\(y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\(\\lambda_i = \\exp(X_i\\beta)\\), and so our link function \\(g(x) = \\log(x)\\)\nAs stated before, for one observation \\(y\\), \\(f(y \\mid \\lambda) = \\frac{1}{y!} \\lambda^y e^{-\\lambda}\\)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#poisson-posterior",
    "href": "bayes-course/06-lecture/06-lecture.html#poisson-posterior",
    "title": "Bayesian Inference",
    "section": "Poisson Posterior",
    "text": "Poisson Posterior\n\n\nTo derive the posterior distribution for Poisson, we consider K regression inputs and independent priors on all \\(K+1\\): \\(\\alpha\\) and \\(\\beta_1, \\beta_2, ..., \\beta_k\\) \\[\n\\begin{eqnarray}\nf\\left(\\alpha,\\beta \\mid y,X\\right) & \\propto &\nf_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot\n\\prod_{i=1}^N {\\frac{g^{-1}(\\eta_i)^{y_i}}{y_i!} e^{-g^{-1}(\\eta_i)}} \\\\\n& \\propto & f_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot\n\\prod_{i=1}^N {\\frac{\\exp(\\alpha + X_i\\beta)^{y_i}}{y_i!} e^{-\\exp(\\alpha + X_i\\beta)}}\n\\end{eqnarray}\n\\]\nWhen the rate is observed at different time scales or unit scales, we introduce an exposure \\(u_i\\), which multiplies the rate \\(\\lambda_i\\)\nThe data model then becomes \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}\\left(u_i e^{X_i\\beta}\\right) \\\\\n& = & \\text{Poisson}\\left(e^{\\log(u_i)} e^{X_i\\beta}\\right) \\\\\n& = &\\text{Poisson}\\left(e^{X_i\\beta + \\log(u_i)}\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#poisson-simulation",
    "href": "bayes-course/06-lecture/06-lecture.html#poisson-simulation",
    "title": "Bayesian Inference",
    "section": "Poisson Simulation",
    "text": "Poisson Simulation\n\n\nWe can set up a forward simulation to generate Poisson data\nIt’s a good practice to fit simulated data and see if you can recover the parameters from a known data-generating process\n\n\n\n\n\nset.seed(123)\nn &lt;- 100\na &lt;- 1.5\nb &lt;- 0.5\nx &lt;- runif(n, -5, 5)\neta &lt;- a + x * b   # could be negative\nlambda &lt;- exp(eta) # always positive\ny &lt;- rpois(n, lambda)\nsim &lt;- tibble(y, x, lambda)\np &lt;- ggplot(aes(x, y), data = sim)\np + geom_point(size = 0.5) + \n  geom_line(aes(y = lambda), \n            col = 'red', \n            linewidth = 0.2) +\n  ggtitle(\"Simulated Poission Data\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#fitting-simulated-data",
    "href": "bayes-course/06-lecture/06-lecture.html#fitting-simulated-data",
    "title": "Bayesian Inference",
    "section": "Fitting Simulated Data",
    "text": "Fitting Simulated Data\n\n\nComplex and non-linear models may have a hard time recovering parameters from forward simulations\nThe process for fitting simulated data may give some insight into the data-generating process and priors\n\n\n\n\n\n# fitting from eta = 1.5 +  0.5 * x\nm3 &lt;- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = sim,\n               chains = 4,\n               refresh = 0,\n               iter = 1000)\nsummary(m3)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ x\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 1.5    0.1  1.4   1.5   1.6  \nx           0.5    0.0  0.5   0.5   0.5  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 10.7    0.5 10.1  10.7  11.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   451 \nx             0.0  1.0   504 \nmean_PPD      0.0  1.0  1434 \nlog-posterior 0.0  1.0   667 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#checking-poission-assumption",
    "href": "bayes-course/06-lecture/06-lecture.html#checking-poission-assumption",
    "title": "Bayesian Inference",
    "section": "Checking Poission Assumption",
    "text": "Checking Poission Assumption\n\n\nWe know that for Poisson model, \\(\\E(y_i) = \\V(y_i)\\), or equivalently \\(\\sqrt{\\E(y_i)} = \\text{sd}(y_i)\\)\nWe can check that the prediction errors follow this trend since we have a posterior predictive distribution at each \\(y_i\\)\n\n\n\n\n\nlibrary(latex2exp)\nyrep &lt;- posterior_predict(m3)\nd &lt;- tibble(y_mu_hat = sqrt(colMeans(yrep)),\n            y_var = apply(yrep, 2, sd))\np &lt;- ggplot(aes(y_mu_hat, y_var), data = d)\np + geom_point(size = 0.5) +\n  geom_abline(slope = 1, intercept = 0,\n              linewidth = 0.2) +\n  xlab(TeX(r'($\\sqrt{\\widehat{E(y_i)}}$)')) +\n  ylab(TeX(r'($\\widehat{sd(y_i)}$)'))"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#posterior-predictive-checks",
    "href": "bayes-course/06-lecture/06-lecture.html#posterior-predictive-checks",
    "title": "Bayesian Inference",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\n\npred &lt;- add_predicted_draws(sim, m3)\np1 &lt;- pred |&gt;\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, alpha = 0.2)\np2 &lt;- pp_check(m3)\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#adding-exposure",
    "href": "bayes-course/06-lecture/06-lecture.html#adding-exposure",
    "title": "Bayesian Inference",
    "section": "Adding Exposure",
    "text": "Adding Exposure\n\n\nLet’s check the effect of adding an exposure variable to the DGP\n\n\n\n\n\nn &lt;- 100\na &lt;- 1.5\nb &lt;- 0.5\nx &lt;- runif(n, -5, 5)\nu &lt;- rexp(n, 0.2)\neta  &lt;- a + x * b + log(u) \n# or &lt;- a + x * b\n\nlambda &lt;- exp(eta)\ny &lt;- rpois(n, lambda)\n# or rpois(n, u * lambda)\n\nsim_exposure &lt;- tibble(y, x, lambda, \n                       exposure = u)\np &lt;- ggplot(aes(x, y), data = sim_exposure)\np + geom_point(size = 0.5) + \nggtitle(\"Simulated Poission Data with Exposure\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#checking-predictions",
    "href": "bayes-course/06-lecture/06-lecture.html#checking-predictions",
    "title": "Bayesian Inference",
    "section": "Checking Predictions",
    "text": "Checking Predictions\n\nCodePlot\n\n\n\n\nSuppose we fit the model with and without the exposure term\n\n\n\n\n\n\nm4 &lt;- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = sim_exposure, refresh = 0,\n               iter = 1200)\nm5 &lt;- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               offset = log(exposure),\n               refresh = 0,\n               data = sim_exposure, iter = 1200)\nyrep_m4 &lt;- posterior_predict(m4)\nyrep_m5 &lt;- posterior_predict(m5)\ns &lt;- sample(nrow(yrep_m4), 100)\np1 &lt;- ppc_dens_overlay(log(sim_exposure$y + 1), \n                       log(yrep_m4[s, ] + 1)) + \n  ggtitle(\"No Exposure (log scale)\")\np2 &lt;- ppc_dens_overlay(log(sim_exposure$y + 1), \n                       log(yrep_m5[s, ] + 1)) + \n  ggtitle(\"With Exposure (log scale)\")\n\n\n\n\n\npred4 &lt;- add_predicted_draws(sim_exposure, m4)\npred5 &lt;- add_predicted_draws(sim_exposure, m5, \n          offset = log(sim_exposure$exposure))\np3 &lt;- pred4 |&gt;\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), \n                  alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, \n             alpha = 0.2)\np4 &lt;- pred5 |&gt;\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), \n                  alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, \n             alpha = 0.2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches!",
    "text": "Example: Trapping Roaches!\n\n\nThis example comes from Gelman and Hill (2007)\nThese data come from a pest management program aimed at reducing the number of roaches in the city apartments\nThe outcome \\(y\\), is the number of roaches caught\nThere is a pre-treatment number of roaches, roach1, a treatment indicator, and senior indicator for only elderly residents in a building\nThere is also exposure2, a number of days for which the roach traps were used\n\n\n\n\n# rescale to make sure coefficients are approximately \n# on the same scale\nroaches &lt;- roaches |&gt;\n  mutate(roach100 = roach1 / 100) |&gt;\n  as_tibble()\nhead(roaches)\n\n# A tibble: 6 × 6\n      y roach1 treatment senior exposure2 roach100\n  &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1   153 308            1      0      0.8    3.08  \n2   127 331.           1      0      0.6    3.31  \n3     7   1.67         1      0      1      0.0167\n4     7   3            1      0      1      0.03  \n5     0   2            1      0      1.14   0.02  \n6     0   0            1      0      1      0"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-1",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-1",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nOur model has three inputs and an intercept term\nSince the traps were set for a different number of days, we will include an exposure offset \\(u_i\\)\n\\(b_t\\) is the treatment coefficient, \\(b_r\\) is the baseline roach level, and \\(b_s\\) is the senior coefficient\nWe need to consider reasonable priors on all those \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(?, \\, ?) \\\\\n\\beta & \\sim & \\text{Normal}(?, \\, ?)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-2",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-2",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nIf we look at the exposure, the average number of days that the traps were set was about 1\nHow many roaches do we expect to trap during a whole day? Hundreds would probably be on the high side, so our prior model should not be predicting, say 10s of thousands\nWhat is the interpretation of the intercept in this regression?\nThere is no way (to my knowledge) to put a half-normal or exponential distribution on the intercept in rstanarm, and if we put Normal(3, 1), it’s unlikely to be negative, and the number of roaches can be as high as Exp(5) ~ 150 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(3, 1) \\\\\n\\beta & \\sim & \\text{Normal}(?, \\, ?)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-3",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-3",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow large can we expect the effects to be in this regression?\nLet’s just consider treatment\nSuppose we estimate the coefficient to be -0.05\nThat means it reduces roach infestation by 5% on average (exp(-0.05) = 0.95)\nWhat if it’s -2; that would mean an 86% reduction, an unlikely but possible outcome\nWith this in mind, we will set the betas to Normal(0, 1) \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(3, 1) \\\\\n\\beta & \\sim & \\text{Normal}(0, \\, 1)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-4",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-4",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe could do a quick sanity check using the prior predictive distribution\n\n\n\n\nm6 &lt;- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               prior_PD = 1,\n               seed = 123)\nyrep_m6 &lt;- posterior_predict(m6)\nsummary(colMeans(yrep_m6))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    9.88    42.69    47.71   296.27    57.70 52941.62 \n\n\n\n\n\nThe median is not unreasonable, but we would not expect the max (of the average!) to be 52,000\nThe numbers or not in another universe, however, so we will go with it\nTry to do what people usually do, which is put the scale on the intercept at 10 or more and scale of the betas at 5 or more, and see what you get"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-5",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-5",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe will now fit the model and evaluate the inferences\n\n\n\n\nm7 &lt;- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               seed = 123)\nsummary(m7)\n\n\nModel Info:\n\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ roach100 + treatment + senior\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  3.1    0.0  3.1   3.1   3.1 \nroach100     0.7    0.0  0.7   0.7   0.7 \ntreatment   -0.5    0.0 -0.5  -0.5  -0.5 \nsenior      -0.4    0.0 -0.4  -0.4  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 25.6    0.4 25.1  25.6  26.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   899 \nroach100      0.0  1.0  1074 \ntreatment     0.0  1.0   950 \nsenior        0.0  1.0  1045 \nmean_PPD      0.0  1.0  1219 \nlog-posterior 0.1  1.0   574 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-6",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-6",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow good is this model?\nLet’s look a the basic posterior predictive check\n\n\n\n\n\nyrep_m7 &lt;- posterior_predict(m7)\ns &lt;- sample(nrow(yrep_m7), 100)\n\n# on the log scale,\n# so we can better see the data\np1 &lt;- ppc_dens_overlay(log(roaches$y + 1), \n                 log(yrep_m7[s, ] + 1)) \n\nprop_zero &lt;- function(y) mean(y == 0)\np2 &lt;- pp_check(m7, plotfun = \"stat\", \n               stat = \"prop_zero\", \n               binwidth = .005)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#negative-binomial",
    "href": "bayes-course/06-lecture/06-lecture.html#negative-binomial",
    "title": "Bayesian Inference",
    "section": "Negative Binomial",
    "text": "Negative Binomial\n\n\nPPCs suggest there is overdispersion in the data\nWe can introduce a likelihood that doesn’t force the mean to be equal to the variance\nThe following is one of the parameterizations that is used in Stan \\[\n\\begin{eqnarray}\n\\text{NegBinomial2}(n \\, | \\, \\mu, \\phi)  &=& \\binom{n + \\phi - 1}{n} \\,\n\\left( \\frac{\\mu}{\\mu+\\phi} \\right)^{\\!n} \\, \\left(\n\\frac{\\phi}{\\mu+\\phi} \\right)^{\\!\\phi} \\\\\n\\E(n) &=& \\mu \\ \\\n\\text{ and } \\ \\V(n) = \\mu + \\frac{\\mu^2}{\\phi}\n\\end{eqnarray}\n\\]\nNotice that the variance term includes \\(\\mu^2 / \\phi &gt; 0\\) allowing for more flexibility than in the case of Poisson"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-7",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-7",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe will now fit the model using a Negative Binomial instead of a Poisson\n\n\n\n\nm8 &lt;- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               family = neg_binomial_2, \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               seed = 123)\nsummary(m8)\n\n\nModel Info:\n\n function:     stan_glm\n family:       neg_binomial_2 [log]\n formula:      y ~ roach100 + treatment + senior\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n                        mean   sd   10%   50%   90%\n(Intercept)            2.8    0.2  2.6   2.8   3.1 \nroach100               1.2    0.2  1.0   1.2   1.6 \ntreatment             -0.7    0.2 -1.0  -0.7  -0.4 \nsenior                -0.3    0.3 -0.7  -0.3   0.0 \nreciprocal_dispersion  0.3    0.0  0.2   0.3   0.3 \n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD  68.8   82.0  24.3  42.5 138.8\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                      mcse Rhat n_eff\n(Intercept)           0.0  1.0  1742 \nroach100              0.0  1.0  1708 \ntreatment             0.0  1.0  1204 \nsenior                0.0  1.0  1573 \nreciprocal_dispersion 0.0  1.0  1604 \nmean_PPD              2.6  1.0  1017 \nlog-posterior         0.1  1.0   515 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-8",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-8",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow good is this model?\nLet’s look at the basic posterior predictive check\n\n\n\n\n\nyrep_m8 &lt;- posterior_predict(m8)\ns &lt;- sample(nrow(yrep_m8), 100)\n\n# on the log scale,\n# so we can better see the data\np1 &lt;- ppc_dens_overlay(log(roaches$y + 1), \n                 log(yrep_m8[s, ] + 1)) \n\np2 &lt;- pp_check(m8, plotfun = \"stat\", \n               stat = \"prop_zero\", \n               binwidth = .005)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-9",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-9",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nLet’s check the comparison of the out-of-sample predictive performance relative to the Poisson model\n\n\n\n\nloo_pois &lt;- loo(m7)\nloo_nb &lt;- loo(m8)\nloo_compare(loo_pois, loo_nb)\n\n   elpd_diff se_diff\nm8     0.0       0.0\nm7 -5329.4     703.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "title": "Bayesian Inference",
    "section": "Conjugate Models and Posterior Sampling",
    "text": "Conjugate Models and Posterior Sampling\n\n\nGamma-Poisson family\nNormal-Normal family\nIntroduction to posterior sampling on a grid\nIntroduction to Stan\nBasic Markov Chain diagnostics\nEffective sample size\nComputing the R-Hat statistic\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson",
    "title": "Bayesian Inference",
    "section": "Poisson",
    "text": "Poisson\n\n\nPoisson distribution arises when we are interested in counts\nIn practice, we rarely use Poisson due to its restrictive nature\nPoisson RV \\(Y\\) is paremeterized with a rate of occurance \\(\\lambda\\): \\(Y|\\lambda \\sim \\text{Pois}(\\lambda)\\)\n\\[\nf(y \\mid \\lambda) =  \\frac{e^{-\\lambda} \\lambda^y}{y!}\\;\\; \\text{ for } y \\in \\{0,1,2,\\ldots\\}\n\\]\nNotice, the Tailor series for \\(e^\\lambda = \\sum_{y=0}^{\\infty} \\frac{\\lambda^y}{y!}\\) immediately validates that \\(f\\) is a PDF\nAlso, \\(\\E(Y | \\lambda) = \\V(Y | \\lambda) = \\lambda\\), which is the restrictive case mentioned about — real count data seldom satisfied this constraint"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "title": "Bayesian Inference",
    "section": "Visualizing Poission",
    "text": "Visualizing Poission\n\n\nNotice that as the rate increases, so does the expected number of events as well as variance, which immediately follows from \\(\\E(Y) = \\V(Y) = \\lambda\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#example-hourse-kicks",
    "href": "bayes-course/03-lecture/03-lecture.html#example-hourse-kicks",
    "title": "Bayesian Inference",
    "section": "Example: Hourse Kicks",
    "text": "Example: Hourse Kicks\n\n\nServing in Prussian cavalry in the 1800s was a perilous affair\nAside from the usual dangers of military service, you were at risk of being killed by a horse kick\nData from the book The Law of Small Numbers by Ladislaus Bortkiewicz (1898)\nBortkiewicz was a Russian economist and statistician of Polish ancestry"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "title": "Bayesian Inference",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\n\n\nAssume we observe \\(Y_1, Y_2, ..., Y_n\\) independant Poisson random variables\nThe joint lilelihood, a function of the parameter \\(\\lambda\\) for \\(y_i \\in \\mathbb{Z}^+\\) and \\(\\lambda &gt; 0\\), is given by: \\[\n\\begin{eqnarray}\nf(y \\mid \\lambda) & = & \\prod_{i=1}^n f(y_i \\mid \\lambda) = f(y_1 \\mid \\lambda)  f(y_2 \\mid \\lambda) \\cdots f(y_n \\mid \\lambda)  =  \\prod_{i=1}^{n}\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\\\\n& = &\\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\cdot \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\cdots \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n& =  &\\frac{\\left(\\lambda^{y_1}\\lambda^{y_2} \\cdots \\lambda^{y_n}\\right) \\left(e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}\\right)}{y_1! y_2! \\cdots y_n!} \\\\\n& = &\\frac{\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i!} \\propto\n\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\n\\end{eqnarray}\n\\]\nWe call \\(\\sum_{i=1}^{n} y_i\\) a sufficient statistic"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "title": "Bayesian Inference",
    "section": "Conjugate Prior for Poisson",
    "text": "Conjugate Prior for Poisson\n\n\nThe likelihood has a form of \\(\\lambda^{a} e^{-b\\lambda}\\) so we expect the conjugate prior to be of the same form\nGamma PDF satisfied this condition: \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nMatching up the exponents, we can interpret \\((\\alpha - 1)\\) as the total number of incidents \\(\\sum y_i\\) out of \\(\\beta\\) prior observations\nFull Gamma density is \\(\\text{Gamma}(\\lambda|\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}   {\\Gamma(\\alpha)} \\, \\lambda^{\\alpha - 1}e^{-\\beta \\, \\lambda}\\) for \\(\\lambda, \\alpha, \\beta \\in \\mathbb{R}^+\\) \\[\n\\begin{equation}\n\\begin{split}\n\\E(\\lambda) & = \\frac{\\alpha}{\\beta} \\\\\n\\text{Mode}(\\lambda) & = \\frac{\\alpha - 1}{\\beta} \\;\\; \\text{ for } \\alpha \\ge 1 \\\\\n\\V(\\lambda) & = \\frac{\\alpha}{\\beta^2} \\\\\n\\end{split}\n\\end{equation}\n\\]\nWhen \\(\\alpha = 1\\), \\(\\lambda \\sim \\text{Dist}(\\beta)\\). What is \\(\\text{Dist}\\)? Work in pairs."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "title": "Bayesian Inference",
    "section": "Visualizing Gamma",
    "text": "Visualizing Gamma\n\n\nNotice that variance, mean, and mode are increasing with \\(\\alpha\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "title": "Bayesian Inference",
    "section": "Gamma Posterior",
    "text": "Gamma Posterior\n\n\nPrior is \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nLikelihood is \\(f(y | \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\\) \\[\n\\begin{eqnarray}\nf(\\lambda \\mid y) & \\propto & \\text{prior} \\cdot \\text{likelihood} \\\\\n& = & \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\cdot  \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda} \\\\\n& = & \\lambda^{\\alpha + \\sum_{i=1}^{n} y_i - 1} \\cdot e^{-\\beta\\lambda - n\\lambda} \\\\\n& = & \\lambda^{\\color{red}{\\alpha + \\sum_{i=1}^{n} y_i} - 1} \\cdot e^{-\\color{red}{(\\beta + n)} \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma}\\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + n \\right)\n\\end{eqnarray}\n\\]\nAs before, we can match the Gamma kernel without doing the integration"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "title": "Bayesian Inference",
    "section": "Checking the Constant",
    "text": "Checking the Constant\n\n\nGamma prior integration constant: \\(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\)\nThe posterior kernel integration constant is the reciprocal of: \\(\\frac{\\Gamma(\\alpha + \\sum y_i)}{(\\beta + n)^{\\alpha + \\sum y_i}}\\). Why?\nWe can sanity check this in Mathematica where \\(t = \\sum y_i\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#posterior-mean-and-variance",
    "href": "bayes-course/03-lecture/03-lecture.html#posterior-mean-and-variance",
    "title": "Bayesian Inference",
    "section": "Posterior Mean and Variance",
    "text": "Posterior Mean and Variance\nPosterior mean and variance follow from the updated prior mean and variance:\n\n\\[\n\\begin{eqnarray}\n\\E(\\lambda \\mid y) &=& \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + \\sum_{i=1}^{n} y_i}{\\beta + n} \\\\\n\\V(\\lambda \\mid y) &=& \\frac{\\alpha'}{(\\beta')^2} = \\frac{\\alpha + \\sum_{i=1}^{n} y_i}{(\\beta + n)^2}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n\nFrom 1875 to 1894, there were 14 different cavalry corps\nEach reported a number of deaths by horse kick every year\nThere are 20 years x 14 corps making 280 observations\n\n\n\n\nlibrary(gt)\nd &lt;- vroom::vroom(\"data/horsekicks.csv\")\nd |&gt; filter(Year &lt; 1883) |&gt; gt()\n\n\n\n\n\n\n\nYear\nGC\nC1\nC2\nC3\nC4\nC5\nC6\nC7\nC8\nC9\nC10\nC11\nC14\nC15\n\n\n\n\n1875\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n\n\n1876\n2\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n1877\n2\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\n2\n0\n\n\n1878\n1\n2\n2\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n1879\n0\n0\n0\n1\n1\n2\n2\n0\n1\n0\n0\n2\n1\n0\n\n\n1880\n0\n3\n2\n1\n1\n1\n0\n0\n0\n2\n1\n4\n3\n0\n\n\n1881\n1\n0\n0\n2\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n1882\n1\n2\n0\n0\n0\n0\n1\n0\n1\n1\n2\n1\n4\n1"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n\n# this flattens the data frame into a vector\ndd &lt;- unlist(d[, -1]) \np &lt;- ggplot(aes(y), data = data.frame(y = dd))\np1 &lt;- p + geom_histogram() +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Total reported counts of deaths\")\np2 &lt;- p + geom_histogram(aes(y = ..count../sum(..count..))) +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Proportion of reported counts of deaths\")\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\nPrior and PosteriorPlots\n\n\n\n\nLet’s assume that before seeing the data, your friend told you that last year, in 1874, there were no deaths reported in his corps\nThat would imply \\(\\beta = 1\\) and \\(\\alpha - 1 = 0\\) or \\(\\alpha = 1\\)\nThe prior on lambda would therefore be \\(\\text{Gamma}(1, 1)\\)\n\n\n\nN &lt;- length(dd)\nsum_yi &lt;- sum(dd)\ny_bar &lt;- sum_yi/N\ncat(\"Total number of observations N =\", N)\n\nTotal number of observations N = 280\n\ncat(\"Total number of deaths =\", sum_yi)\n\nTotal number of deaths = 196\n\ncat(\"Average number of deaths =\", y_bar)\n\nAverage number of deaths = 0.7\n\n\n\n\nThe posterior is \\(\\text{Gamma}\\left( \\alpha + \\sum y_i, \\, \\beta + N \\right) = \\text{Gamma}(197, \\, 281)\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "href": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "title": "Bayesian Inference",
    "section": "Likelihood Dominates",
    "text": "Likelihood Dominates\n\n\n\n\nWith so much data relative to prior observations, the likelihood completely dominates the prior\n\n\n\n\nplot_gamma_poisson(1, 1, sum_y = sum_yi, n = N) + \n  xlim(0, 3)\n\n\n\n\n\n\n\n\n\n\n\n\nmean_post &lt;- (1 + sum_yi) / (1 + N) \nsd_post &lt;- sqrt((1 + sum_yi) / (1 + N)^2) \ncat(\"Posterior mean =\", mean_post |&gt; round(2))\n\nPosterior mean = 0.7\n\ncat(\"Average rate =\", y_bar)\n\nAverage rate = 0.7\n\ncat(\"Posterior sd =\", sd_post |&gt; round(2))\n\nPosterior sd = 0.05"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "title": "Bayesian Inference",
    "section": "Checking the Fit",
    "text": "Checking the Fit\n\n\nWe can plug the posterior mean for \\(\\lambda\\) into the Poisson PMF\n\n\n\n\nmean_post &lt;- (1 + sum_yi) / (1 + N)\ndeaths &lt;- 0:4\npred &lt;- dpois(deaths, lambda = mean_post)\nactual &lt;- as.numeric(table(dd) / N)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "href": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "title": "Bayesian Inference",
    "section": "Adding Exposure",
    "text": "Adding Exposure\n\n\nPoisson likelihood seems to work well for these data\nIt is likely that each of the 16 corps had about the same number of soldier-horses, a common military practice\nSuppose each corps has a different number of cavalrymen\nWe need to introduce an exposure variable \\(x_i\\) for each corps unit \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(x_i \\lambda)\\\\\n\\lambda & \\sim & \\text{Gamma}(\\alpha, \\beta) \\\\\nf(y \\mid \\lambda) & \\propto & \\lambda^{\\sum_{i=1}^{n} y_i}e^{- (\\sum_{i=1}^{n} x_i) \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma} \\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + \\sum_{i=1}^{n} x_i \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\nThe last conjugate distribution we will introduce is Normal\nWe will only consider a somewhat unrealistic case of known variance \\(\\sigma \\in \\mathbb{R}^+\\) and unknown mean \\(\\mu \\in \\mathbb{R}\\)\nNormal PDF is for one observation \\(y\\) is given by: \\[\n\\begin{eqnarray}\n\\text{Normal}(y \\mid \\mu,\\sigma) & = &\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\E(Y) & = & \\text{ Mode}(Y) = \\mu \\\\\n\\V(Y) & = & \\sigma^2 \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\nNormal arises when many independent small contributions are added up\nIt is a limiting distribution of means of an arbitrary distribution\n\n\n\n\n\nplot_xbar &lt;- function(n_repl, n_samples) {\n  x &lt;- seq(0.6, 1.4, len = 100)\n  xbar &lt;- replicate(n_repl, \n                    mean(rexp(n_samples, rate = 1)))\n  mu &lt;- dnorm(x, mean = 1, sd = 1/sqrt(n_samples))\n  p &lt;- ggplot(aes(x = xbar), \n              data = tibble(xbar))\n  p + geom_histogram(aes(y = ..density..), \n                     bins = 30, alpha = 0.6) +\n    geom_line(aes(x = x, y = mu), \n              color = 'red', \n              linewidth = 0.3, \n              data = tibble(x, y)) +\n    ylab(\"\") + theme(axis.text.y = element_blank())\n}\n\np1 &lt;- plot_xbar(1e4, 100) + \n  ggtitle(\"Sampling means from rexp(100, 1)\")\np2 &lt;- plot_xbar(1e4, 300) + \n  ggtitle(\"Sampling means from rexp(300, 1)\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "title": "Bayesian Inference",
    "section": "Joint Normal Likelihood",
    "text": "Joint Normal Likelihood\n\n\nAfter observing data \\(y\\), we can compute the joint normal likelihood, assuming \\(\\sigma\\) is known \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n& \\propto & \\prod_{i=1}^{n} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2 \\right) \\\\  \n& = & \\exp \\left( {-\\frac{\\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2}}\\right) \\\\\n&\\propto& \\exp\\left({-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\\right)\n\\end{eqnarray}\n\\]\nThe last line is derived by expanding the square and dropping terms that don’t depend on \\(\\mu\\); \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "title": "Bayesian Inference",
    "section": "Normal Prior",
    "text": "Normal Prior\n\n\nWe can now define the prior on \\(\\mu\\)\nWe will choose \\(\\mu\\) to be normal: \\(\\mu \\sim \\text{Normal}(\\theta, \\tau^2)\\) \\[\n\\begin{eqnarray}\nf(\\mu \\mid \\theta, \\tau) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "title": "Bayesian Inference",
    "section": "Normal Posterior",
    "text": "Normal Posterior\n\nFor one observation \\(y\\):\n\n\\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\nf(\\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\nf(\\mu \\mid y) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\n& = & \\exp \\left( -\\frac{1}{2} \\left(  \\frac{(\\mu - \\theta)^2}{\\tau^2} + \\frac{(y - \\mu)^2}{\\sigma^2} \\right)\\right) \\\\\n& = &  \\exp \\left(  -\\frac{1}{2\\tau_1^2} \\left( \\mu - \\mu_1 \\right)^2    \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-posterior-1",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-posterior-1",
    "title": "Bayesian Inference",
    "section": "Normal Posterior",
    "text": "Normal Posterior\n\nFor one observation: \\[\n\\begin{eqnarray}\nf(\\mu \\mid y) &\\propto& \\exp \\left(  -\\frac{1}{2\\tau_1^2} \\left( \\mu - \\mu_1 \\right)^2    \\right) \\text{ i.e. } \\mu \\mid y \\sim \\text{Normal}(\\mu_1, \\tau_1) \\\\\n\\mu_1 &=& \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{1}{\\sigma^2} y}{\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} &=& \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}\n\\end{eqnarray}\n\\]\nFor multiple observations: \\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{n}{\\sigma^2} \\overline{y}}{\\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "title": "Bayesian Inference",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nAriannaThe Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArianna Rosenbluth Dies at 93, The New York Times"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\n\n\nMost posterior distributions do not have an analytical form\nIn those cases, we must resort to sampling methods\nSampling in high dimensions requires specialized algorithms\nHere, we will look at one-dimensional sampling on the grid (of parameter values)\nAt the end of this lecture, we look at some output of a state-of-the-art HMC NUTS sampler\nNext week, we will examine the Metropolis-Hastings-Rosenbluth algorithm"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\nWe already saw how given samples from the target distribution, we could compute quantities of interest\nThe grid approach to getting samples:\n\nGenerate discreet points in parameter space \\(\\theta\\)\nDefine our likelihood function \\(f(y|\\theta)\\) and prior \\(f(\\theta)\\)\nFor each point on the grid, compute the product \\(f(y|\\theta)f(\\theta)\\)\nNormalize the product to sum to 1\nSample from the resulting distribution in proportion to the posterior probability\n\nHere, \\(\\theta\\) is continuous, so we can not use the technique we used for a discrete, one-dimensional prior from the clinical trial example.\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "href": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "title": "Bayesian Inference",
    "section": "Early Clinical Trial Example",
    "text": "Early Clinical Trial Example\n\nDeriving \\(f(\\theta, y)\\)R ImplementationGraphs\n\n\n\n\nFor simplicity we will assume uniform \\(\\text{Beta}(1, 1)\\) prior \\[\n\\begin{eqnarray}\nf(\\theta)f(y\\mid \\theta) &\\propto& \\theta^{a -1}(1 - \\theta)^{b-1} \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^0(1 - \\theta)^0 \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^{\\sum_{i=1}^{n} y_i} \\cdot (1 - \\theta)^{\\sum_{i=1}^{n} (1- y_i)}\n\\end{eqnarray}\n\\]\nOn the log scale: \\(\\log f(\\theta, y) \\propto \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\n\nRecall we had 3 responders out of 5 patients\nModel: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\nlp &lt;- function(theta, data) {\n# log(theta) * sum(data$y) + log(1 - theta) * sum(1 - data$y)\n  lp &lt;- 0\n  for (i in 1:data$N) {\n    lp &lt;- lp + log(theta) * data$y[i] + log(1 - theta) * (1 - data$y[i])\n  }\n  return(lp)\n}\ndata &lt;- list(N = 5, y = c(0, 1, 1, 0, 1))\n# generate theta parameter grid\ntheta &lt;- seq(0.01, 0.99, len = 100)\n# compute log likelihood and prior for every value of the grid\nlog_lik &lt;- lp(theta, data); log_prior &lt;- log(dbeta(theta, 1, 1))\n# compute log posterior\nlog_post &lt;- log_lik + log_prior\n# convert back to the original scale and normalize\npost &lt;- exp(log_post); post &lt;- post / sum(post)\n# sample theta in proportion to the posterior probability\ndraws &lt;- sample(theta, size = 1e5, replace = TRUE, prob = post)\n\n\n\n\n\n\nFrom the first lecture, we know the posterior is \\(\\text{Beta}(1 + 3, 1 + 5 - 3) = \\text{Beta}(4, 3)\\)\nWe can compare this density to the posterior draws\n\n\n\n\nbeta_dens &lt;- dbeta(theta, 4, 3)\n(mle &lt;- sum(data$y) / data$N)\n\n[1] 0.6"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "href": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "title": "Bayesian Inference",
    "section": "Monte Carlo Integration and MCMC",
    "text": "Monte Carlo Integration and MCMC\n\nIntroductionExample 1Example 2MCMC\n\n\n\n\nWe already saw a special case of MC integration\nSuppose we can draw samples from PDF \\(f\\), \\((\\theta^{(1)}, \\theta^{(2)},..., \\theta^{(N)})\\)\nIf we want to compute an expectation of some function \\(h\\): \\[\n\\begin{eqnarray}\n\\E_f[h(\\theta)] &=& \\int h(\\theta)f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} h \\left( \\theta^{(i)} \\right)\n\\end{eqnarray}\n\\]\nThe Law of Large Numbers tells us that these approximations improve with \\(N\\)\nIn practice, the challenge is obtaining draws from \\(f\\)\nThat’s where MCMC comes in\n\n\n\n\n\n\nSuppose we want to estimate the mean and variance of standard normal\nIn case of the mean, \\(h(\\theta) := \\theta\\) and variance, \\(h(\\theta) := \\theta^2\\), and \\(f(\\theta) = \\frac{1}{\\sqrt{2 \\pi} \\ } e^{-\\theta^2/2}\\) \\[\n\\begin{eqnarray}\n\\E[\\theta] &=& \\int_{-\\infty}^{\\infty} \\theta f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\theta^{(i)} \\\\\n\\E[\\theta^2] &=& \\int_{-\\infty}^{\\infty} \\theta^2 f(\\theta)\\, d\\theta &\\approx&\n\\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\theta^{(i)} \\right)^2\n\\end{eqnarray}\n\\]\n\n\n\n\nN &lt;- 1e5\ntheta &lt;- rnorm(N, 0, 1)          # draw theta from N(0, 1)\n(1/N * sum(theta)) |&gt; round(2)   # E(theta),   same as mean(theta)\n\n[1] 0\n\n(1/N * sum(theta^2)) |&gt; round(2) # E(theta^2), same as mean(theta^2)\n\n[1] 1\n\n\n\n\n\n\n\nSuppose we want to estimate a CDF of Normal(0, 1) at some point \\(t\\)\nWe let \\(h(\\theta) = \\I(\\theta &lt; t)\\), where \\(\\I\\) is an indicator function that returns 1 when \\(\\theta &lt; t\\) and \\(0\\) otherwise \\[\n\\begin{eqnarray}\n\\E[h(\\theta)] = \\E[\\I(\\theta &lt; t)] &=& \\int_{-\\infty}^{\\infty} \\I(\\theta &lt; t) f(\\theta)\\, d\\theta =\n\\int_{-\\infty}^{t}f(\\theta)\\, d\\theta = \\Phi(t) \\\\\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\I(\\theta^{(i)} &lt; t) \\\\\n\\end{eqnarray}\n\\]\n\n\n\n\npnorm(1, 0, 1) |&gt; round(2)          # Evalute N(0, 1) CDF at 1\n\n[1] 0.84\n\nN &lt;- 1e5\ntheta &lt;- rnorm(N, 0, 1)             # draw theta from N(0, 1)\n(1/N * sum(theta &lt; 1)) |&gt; round(2)  # same as mean(theta &lt; 1)\n\n[1] 0.84\n\n\n\n\n\n\n\nMCMC is a very general method for computing expectations (integrals)\nIt produces dependant (autocorrelated) samples, but for a good algorithm, the dependence is manageable\nStan’s MCMC algorithm is very efficient (more on that later) and requires all parameters to be continuous (data can be discrete)\nIt solves the problem of drawing from distribution \\(f(\\theta)\\) where \\(f\\) is not one of the fundamental distributions and \\(\\theta\\) is high dimentional\nWhat is high-dimensional? Modern algorithms like NUTS can jointly sample tens of thousands and more parameters\nThat’s 10,000+ dimensional integrals of complicated functions!\n\n\n\n\n\n\nInclude an example where h(x) is an indicator function, and we want to evaluate Normal CDF at some point t."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "title": "Bayesian Inference",
    "section": "Introduction to Stan",
    "text": "Introduction to Stan\n\n\nStan is a procedural, statically typed, Turning complete, probabilistic programming language\nStan language expresses a probabilistic model\nStan transpiler converts it to C++\nStan inference algorithms perform parameter estimation\nOur model: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\n\n// 01-bernoulli.stan\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}\n\n\n\n\n\n// 02-bernoulli.stan\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  theta ~ beta(1, 1);\n  y ~ bernoulli(theta);\n}"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "title": "Bayesian Inference",
    "section": "Running Stan",
    "text": "Running Stan\n\n\nlibrary(cmdstanr)\n\nm1 &lt;- cmdstan_model(\"stan/01-bernoulli.stan\") # compile the model\ndata &lt;- list(N = 5, y = c(0, 1, 1, 0, 1))\nf1 &lt;- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 500  # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -5.27  -4.99  0.683 0.291 -6.67  -4.78   1.00     865.    1012.\n2 theta     0.579  0.584 0.167 0.182  0.288  0.843  1.00     749.     933."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "href": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "title": "Bayesian Inference",
    "section": "Working With Posterior Draws",
    "text": "Working With Posterior Draws\n\n\n\n\nlibrary(tidybayes)\n\ndraws &lt;- gather_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy long format\n\n# A tibble: 6 × 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable .value\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n1      4        495  1995 lp__       -5.27\n2      4        496  1996 lp__       -4.90\n3      4        497  1997 lp__       -4.78\n4      4        498  1998 lp__       -4.90\n5      4        499  1999 lp__       -4.90\n6      4        500  2000 lp__       -5.70\n\ndraws |&gt; \n  group_by(.variable) |&gt; \n  summarize(mean = mean(.value))\n\n# A tibble: 2 × 2\n  .variable   mean\n  &lt;chr&gt;      &lt;dbl&gt;\n1 lp__      -5.27 \n2 theta      0.579\n\nmedian_qi(draws, .width = 0.90)\n\n# A tibble: 2 × 7\n  .variable .value .lower .upper .width .point .interval\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 lp__      -4.99  -6.67  -4.78     0.9 median qi       \n2 theta      0.584  0.288  0.843    0.9 median qi       \n\n\n\n\n\n\ndraws &lt;- spread_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy wide format\n\n# A tibble: 6 × 5\n  .chain .iteration .draw theta  lp__\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1      4        495  1995 0.386 -5.27\n2      4        496  1996 0.661 -4.90\n3      4        497  1997 0.576 -4.78\n4      4        498  1998 0.477 -4.90\n5      4        499  1999 0.659 -4.90\n6      4        500  2000 0.322 -5.70\n\ntheta &lt;- seq(0.01, 0.99, len = 100)\np &lt;- ggplot(aes(theta), data = draws)\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 30, alpha = 0.6) +\n    geom_line(aes(theta, beta_dens), \n              linewidth = 0.5, color = 'red',\n              data = tibble(theta, beta_dens)) +\n  ylab(\"\") + xlab(expression(theta)) +\n  ggtitle(\"Posterior draws from Stan\")"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "title": "Bayesian Inference",
    "section": "What’s a Chain",
    "text": "What’s a Chain"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "title": "Bayesian Inference",
    "section": "Dynamic Simulation",
    "text": "Dynamic Simulation\nMCMC Demo"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "href": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "title": "Bayesian Inference",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\n\n\n\nIn general, you want to assess (1) the quality of the draws and (2) the quality of predictions\nThere are no guarantees in either case, but the former is easier than the latter\nThe folk theorem of statistical computing: computational problems often point to problems in the model (AG)\nWe will address the quality of the draws now and the quality of predictions later in the course"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "title": "Bayesian Inference",
    "section": "Good Chain Bad Chain",
    "text": "Good Chain Bad Chain\n\nCodePlot\n\n\n\n\nlibrary(bayesplot)\ndraws &lt;- f1$draws(\"theta\")\ncolor_scheme_set(\"viridis\")\np1 &lt;- mcmc_trace(draws, pars = \"theta\") + ylab(expression(theta)) +\n  ggtitle(\"Good Chain\")\nbad_post &lt;- readRDS(\"data/bad_post.rds\")\nbad_draws &lt;- bad_post$draws(\"mu\")\np2 &lt;- mcmc_trace(bad_draws, pars = \"mu\") + ylab(expression(theta)) +\n  ggtitle(\"Bad Chain\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "href": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "title": "Bayesian Inference",
    "section": "Effective Sample Size and Autocorrelation",
    "text": "Effective Sample Size and Autocorrelation\n\nESSACF\n\n\n\n\nMCMC generates dependent draws from the target distribution\nDependent samples are less efficient as you need more of them to estimate the quantity of interest\nESS or \\(N_{eff}\\) is approximately how many independent samples you have\nTypically, \\(N_{eff} &lt; N\\) for MCMC, as there is some autocorrelation\nESS should be considered relative to \\(N\\): \\(\\frac{N_{eff}}{N}\\)\nGenerally, we don’t like to see \\(\\text{ratio} &lt; 0.10\\)\n\n\n\n\nneff_ratio(f1, pars = \"theta\") |&gt; round(2)\n\ntheta \n 0.38 \n\nneff_ratio(bad_post, pars = \"mu\") |&gt; round(2)\n\n  mu \n0.09"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "href": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "title": "Bayesian Inference",
    "section": "Computing R-Hat",
    "text": "Computing R-Hat\n\n\nAutocorrelation assesses the quality of a single chain\nSplit R-Hat estimates the extent to which the chains are consistent with one another\nIt does it by assessing the mixing of chains by comparing variances within and between chains (technically sequences, as chains are split up) \\[\n\\begin{eqnarray}\n\\hat{R} = \\sqrt{\\frac{\\frac{n-1}{n}\\V_W + \\frac{1}{n}\\V_B}{\\V_W}} = \\sqrt{\\frac{\\V_{\\text{total}}}{\\V_{W}}}\n\\end{eqnarray}\n\\]\n\\(\\V_W\\) is within chain variance and \\(\\V_B\\) is between chain variance\nWe don’t like to see R-hats greater than 1.02 and really don’t like them greater than 1.05\n\n\n\n\nbayesplot::rhat(f1, pars = \"theta\") |&gt; round(3)\n\ntheta \n1.002 \n\nbayesplot::rhat(bad_post, pars = \"sigma\")  |&gt; round(3)\n\nsigma \n1.043"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#stan-homework",
    "href": "bayes-course/03-lecture/03-lecture.html#stan-homework",
    "title": "Bayesian Inference",
    "section": "Stan Homework",
    "text": "Stan Homework\n\nModify the program to incorporate Beta(2, 2) without using theta ~ beta(2, 2) (i.e. using target +=)\nModify the program to account for an arbitrary Beta(a, b) distribution\nVerify that you got the right result in #1 by a) comparing Stan’s posterior means and posterior standard deviations to the means and standard deviations under the conjugate model; b) modifying the Stan program using beta(2, 2) prior and Bernoulli likelihood shown here; c) modifying the R code to get a third point of comparison.\n\n\nHint: For #2, pass parameters a and b in the data block\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  array[N] int&lt;lower=0, upper=1&gt; y;\n}\nparameters {\n  real&lt;lower=0, upper=1&gt; theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#session-5-outline",
    "href": "stats-math/05-lecture/05-lecture.html#session-5-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 5 Outline",
    "text": "Session 5 Outline\n\n\nRandom variables\nBernoulli, Binomial, Geometric\nPDF and CDF\nLOTUS\nPoisson\nExpectations, Variance\nSt. Petersburg Paradox\nNormal\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#random-variables-are-not-random",
    "href": "stats-math/05-lecture/05-lecture.html#random-variables-are-not-random",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Random Variables are Not Random",
    "text": "Random Variables are Not Random\n\nStoryPicture\n\n\n\n\nIt would be inconvenient to enumerate all possible events to describe a stochastic system\nA more general approach is to introduce a function that maps sample space \\(S\\) onto the Real line\nFor each possible outcome \\(s\\), random variable \\(X(s)\\) performs this mapping\nThis mapping is deterministic. The randomness comes from the experiment, not from the random variable (RV)\nWhile it makes sense to talk about \\(\\P(A)\\), where \\(A\\) is an event, it does not make sense to talk about \\(\\P(X)\\), but you can say \\(\\P(X(s) = x)\\), which we usually write as \\(\\P(X = x)\\)\nLet \\(X\\) be the number of Heads in two coin flips. You flip the coin twice, and you get \\(HH\\). In this case, \\(s = {HH}\\), \\(X(s) = 2\\), while \\(S = \\{TT, TH, HT, HH\\}\\)\n\n\n\n\n\nRandom variable \\(X\\) for the number of Heads in two flips"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#characterising-random-variables",
    "href": "stats-math/05-lecture/05-lecture.html#characterising-random-variables",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Characterising Random Variables",
    "text": "Characterising Random Variables\n\nIntroductionR Conventions\n\n\n\n\nTwo ways of describing an RV are CDF (Cumulative Distribution Function) and PMF (Probability Mass Function) for discrete RVs and PDF (Probability Density Function) for continuous RVs. There are other ways, but we will stick with CDF and P[D/M]F.\nCDF \\(F_X(x)\\) is a function of \\(x\\) and is bounded between 0 and 1:\n\n\n\n\\[\nF_X(x) = \\P(X \\leq x)\n\\]\n\n\n\nPMF (for discrete RVs only) \\(f_X(x)\\) is a function of \\(x\\)\n\n\\[\nf_X(x) = \\P(X = x)\n\\]\n\n\n\nYou can get from \\(f_X\\) to \\(F_X\\) by summing. Let’s say \\(x = 4\\). In that case:\n\n\\[\nF_X(4) = \\P(X \\leq 4) = \\sum_{i = 4,3,2,...}\\P(X = i)\n\\]\n\n\n\n\n\nIn R, PMFs and PDFs start with the letter d. For example dbinom() and dnormal() refer to binomial PMF and normal PDF\nCDFs start with p, so pbinom() and pnorm()\nInverse CDFs or quantile functions, start with q so qbinom() and so on\nRandom number generators start with r, so rbinom()\nA binomial RV, which we will define later, represents the number of successes in N trials. In R, the PMF is dbinom() and CDF is pbinom()\nHere is the full function signature: dbinom(x, size, prob, log = FALSE)\n\nx is the number of successes, size is the number of trials N, prob is the probability of success in each trial \\(\\theta\\), and log is a flag asking if we want the results on the log scale."
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#binomial-rv",
    "href": "stats-math/05-lecture/05-lecture.html#binomial-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial RV",
    "text": "Binomial RV\n\nBinomial PMFPMF and CDF PlotsCode to Generate the Plots\n\n\n\n\nBernoulli RV is one coin flip with a set probability of success (say Heads)\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), the PMF can be written directly as \\(\\P(X = x) = \\theta^x (1 - \\theta)^{1-x}, \\, x \\in \\{0, 1\\}\\)\nBinomial can be thought of as the sum of \\(N\\) independent Bernoulli trials. We can also write:\n\n\n\n\\[\n\\text{Bernoulli}(x~|~\\theta) = \\left\\{ \\begin{array}{ll} \\theta &\n\\text{if } x = 1, \\text{ and} \\\\ 1 - \\theta & \\text{if } x = 0\n\\end{array} \\right.\n\\]\n\n\n\nWe can write the Binomial PMF, \\(X \\sim \\text{Binomial}(N, \\theta)\\) this way:\n\n\\[\n\\text{Binomial}(x~|~N,\\theta) = \\binom{N}{x}\n\\theta^x (1 - \\theta)^{N - x}\n\\]\n\n\n\n\\(\\text{Binomial}(x~|~N=4,\\theta = 1/2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\nlibrary(MASS)\n\nN &lt;- 4 # Number of successes out of x trials\n\n# compute and plot the PMF\npmf &lt;- dbinom(x = 0:N, size = N, prob = 1/2)\nd &lt;- data.frame(x =  0:N, y = pmf)\np1 &lt;- ggplot(d, aes(x, pmf))\np1 &lt;- p1 + geom_col(width = .2) + \n  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +\n  ylab(\"P(X = x)\") + xlab(\"x = Number of Heads\") +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n# compute and plot the CDF\nx &lt;- seq(-0.5, 4.5, length = 500)\ncdf &lt;- pbinom(q = x, size = N, prob = 1/2)\nd &lt;- data.frame(q = x, y = cdf)\ndd &lt;- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)\np2 &lt;- ggplot(d, aes(x, cdf)) \np2 &lt;- p2 + geom_point(size = 0.2) + \n  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(CDF: F[X](x) == P(X &lt;= x))) +\n  ylab(expression(P(X &lt;= x))) + xlab(\"x = Number of Heads\")\n\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#binomial-in-r",
    "href": "stats-math/05-lecture/05-lecture.html#binomial-in-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial in R",
    "text": "Binomial in R\n\n# What is the probability of getting 2 Heads out of 5 fair trials?\nN &lt;- 5; x &lt;- 2\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 5/16\n\n# What is the binomial PMF: P(X = x), for N = 5, p = 0.5?\nN &lt;- 5; x &lt;- -2:7 # notice we range x over any integers\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]    0    0 1/32 5/32 5/16 5/16 5/32 1/32    0    0\n\n# Verify that the PMF sums to 1\nsum(dbinom(x = x, size = N, prob = 0.5))\n\n[1] 1\n\n# What is the probability of 3 heads or fewer\npbinom(3, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 13/16\n\n# compute the CDF: P(X &lt;= x), for N = 5, p = 0.5\npbinom(x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n# get from the PMF to CDF; cumsum() is the cumulative sum function\ndbinom(x = x, size = N, prob = 0.5) |&gt; cumsum() |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n\n\n\nYour Turn: Suppose the probability of success is 1/3, N = 10. What is the probability of 6 or more successes? Compute it with a PMF first and verify with the CDF."
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#geometric-rv",
    "href": "stats-math/05-lecture/05-lecture.html#geometric-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Geometric RV",
    "text": "Geometric RV\n\nPMFCheck ConvergenceExamplesPMF\n\n\n\n\nGeometric is a discrete waiting time distribution, and Exponential is its continuous analog\nIf \\(X\\) is the number of failures before first success \\(X \\sim \\text{Geometric}(\\theta)\\), where \\(\\theta\\) is probability of success\nExample: We keep flipping a coin until we get success, say Heads\n\nSay we flip five times, which means we get the following sequence: T T T T H\nThe probability of this sequence is: \\((\\frac{1}{2})^4 (\\frac{1}{2})^1\\)\nNotice this is the only way to get this sequence\n\nIf \\(x\\) is the number of failures, the PMF is \\(P(X = x) = (1 - \\theta)^x \\theta\\), where \\(x = 0, 1, 2, ...\\)\n\n\n\n\n\nTo check if this is a valid PMF, we need to sum over all \\(x\\):\n\n\n\\[\n\\begin{align}\n\\sum_{x = 0}^{\\infty} \\theta (1 - \\theta)^x  =\n\\theta \\sum_{x = 0}^{\\infty} (1 - \\theta)^x \\\\\n\\text{Let } u = 1 - \\theta \\\\\n\\theta \\sum_{x = 0}^{\\infty} u^x = \\theta \\frac{1}{1-u} = \\theta \\frac{1}{1-1 + \\theta} = \\frac{\\theta}{\\theta} = 1\n\\end{align}\n\\]\n\n\n\nThe last bit comes from geometric series for \\(|u| &lt; 1\\)\n\n\n\n\n\nThe probability of T T T H (x = 3 failures) when \\(\\theta = 1/2\\), has to be \\((1/2)^4\\) or \\(1/16\\)\n\n\n\nx &lt;- 3; theta &lt;- 1/2\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 1/16\n\n\n\n\n\nIf \\(\\theta = 1/3\\), the probability of the same sequence has to be \\((2/3)^3 \\cdot 1/3 = 8/81\\)\n\n\nx &lt;- 3; theta &lt;- 1/3\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 8/81\n\n\n\n\n\nThe PMF is unbounded, but it converges to 1 as demonstrated before\n\n\n\n\n\n\nx &lt;-  0:15\ntheta &lt;- 1/5\ny &lt;- dgeom(x = x, prob = theta)\nd &lt;- data.frame(x, y)\np &lt;- ggplot(d, aes(x, y))\np + geom_col(width = 0.2) +\nxlab(\"x = Number of failures before first success\") +\n ylab(expression(P(X == x))) +\nggtitle(\"X ~ Geom(1/5)\", \n subtitle = expression(PDF: p[X](x) == P(X == x)))"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#expectations",
    "href": "stats-math/05-lecture/05-lecture.html#expectations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Expectations",
    "text": "Expectations\n\nIntroductionExamplesR CodeE(X) of Bernoulli and Binomial\n\n\n\n\nAn expectation is a kind of average, typically a weighted average\nExpectation is a single number summary of the distribution\nWhen computing a weighted average, the weights must add up to one\nThat’s fortunate for us since the PMF satisfies this property\nSo, the expectation of a discrete RV is the sum of its values weighted by their respective probabilities\nFor a discrete RV, we have: \\(\\E(X) = \\sum_{x}^{} x f_X(x)\\)\nFor continuous RVs we have: \\(\\E(X) = \\int x f_X(x)\\, dx\\)\n\n\n\n\n\n\nLet’s compute the expectation of the Geometric RV, \\(X \\sim \\text{Geom}(\\theta)\\)\nTo solve it analytically, you would have to sum a slightly more complicated series or condition on the outcome of the first experiment\nHere, we will just write the answer:\n\n\n\n\\[\n\\E(X) = \\sum_{x=0}^{\\infty} x \\theta (1 - \\theta)^x = \\frac{1-\\theta}{\\theta}\n\\]\n\n\n\nIn particular, for \\(X \\sim \\text{Geom}(1/5)\\), what is the expected number of failures before the first success?\nThe answer is \\(4 = (1 - 1/5) / 1/5\\)\n\n\n\n\n\n\nLet’s check computationally:\n\n\n\n\ntheta &lt;- 1/5\nx &lt;- 0:100\nsum(x * dgeom(x = x, prob = theta))\n\n[1] 4\n\ntheta &lt;- 1/4\nsum(x * dgeom(x = x, prob = theta))\n\n[1] 3\n\n\n\n\n\nQuestion: Geometric is unbounded, yet summed a finite number (100) and got the right answer. What’s going on?\nGeometric decays quickly, and higher terms are negligible for these choices of \\(\\theta\\)\nWithout doing any calculations at all, what is \\(\\E(X)\\), where \\(X \\sim \\text{Geom}(1/8)\\)\n\n\n\n\n\n\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), \\(\\E(X) = \\sum_{x=0}^{1} x \\theta^x (1 - \\theta)^{1-x} = 0 + 1 \\cdot \\theta \\cdot 1 = \\theta\\)\nFor the Binomial, is it easier to think about it as a sum of iid Bernoulli RVs. Each Bernoulli RV has an expectation of \\(\\theta\\), and there are \\(n\\) of them, so the expectation is \\(n \\theta\\).\nThis argument relies on the linearity of expectations: \\(\\E(X_1 + X_2 ... + X_n) = \\E(X_1) + \\E(X_2) + ... + \\E(X_n) = \\theta + ... + \\theta = n\\theta\\)"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#st.-petersburg-paradox",
    "href": "stats-math/05-lecture/05-lecture.html#st.-petersburg-paradox",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "St. Petersburg paradox",
    "text": "St. Petersburg paradox\n\nFirst Success DistributionThe ParadoxExpected Utility\n\n\n\n\nThis is a slight modification to Geometric as we are counting the number of failures, including the first success vs. Geometric, where we counted the failures\n\\(X \\sim \\text{FS}(\\theta)\\), if \\(\\P(X = x) = (1 - \\theta)^{x - 1} \\theta\\), for \\(x = 1, 2, 3, ...\\)\n\\(X - 1 \\sim \\text{Geom}(\\theta)\\)\nSay, \\(Y \\sim \\text{Geom}(\\theta)\\), then \\(\\E(X) = \\E(Y + 1) = \\E(Y) + 1 = \\frac{1 - \\theta}{\\theta} + \\frac{\\theta}{\\theta} = \\frac{1}{\\theta}\\)\n\n\n\n\n\n\nSuppose you are offered the following game: you flip a coin until heads appear. You get $2 if the game ends after round 1, $4 after two rounds, $8 after three, and so on.\n\n\n\n\\[\n\\E(X) = \\sum_{n=1}^{\\infty} \\frac{1}{2^n} \\cdot 2^n =  \\sum_{n=1}^{\\infty} 1 = \\infty\n\\]\n\n\n\nVote how much are you willing to pay to play this game? (you can’t lose)\nHow many rounds do we expect to play? If \\(N\\) is the number of rounds, then \\(N \\sim \\text{FS}(1/2)\\), and \\(\\E(N) = \\frac{1}{1/2} = 2\\)\nIn addition, the utility of money has been shown to be non-linear\n\n\n\n\n\n\nDaniel Bernoulli (1700 - 1782) proposed that \\(\\frac{dU}{dw} \\propto \\frac{1}{w}\\), where \\(U\\) is the utility function and \\(w\\) is the wealth\nThis gives rise to the following differential equation \\(\\frac{dU}{dw} = k \\frac{1}{w}\\), where \\(k\\) is the constant of proportionality\nIntegrating both sides: \\(\\int dU = k \\int \\frac{1}{w} dw\\) we get:\n\n\\(U(w) = k \\log(w) + C\\), where \\(C\\) is some initial wealth\n\nFor simplicity let’s take \\(k=1\\), and consider \\(\\log = \\log_2\\)\n\n\n\n\\[\n\\E(U) = \\sum_{n=1}^{\\infty} \\frac{1}{2^n} \\log_2(2^n) =  \\sum_{n=1}^{\\infty} \\frac{n}{2^n} = 2 \\neq \\infty\n\\]\n\n\n\nThe last equality comes from summing the geometric series \\(\\sum_{n=1}^{\\infty} \\frac{n}{2^n}\\) using the same methods we used for Geometric distribution"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#indicator-random-variable",
    "href": "stats-math/05-lecture/05-lecture.html#indicator-random-variable",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Indicator Random Variable",
    "text": "Indicator Random Variable\n\nRecall our \\(\\pi\\) SimulationFundamental Bridge\n\n\n\n\\[\n\\I_A(s) = \\I(s \\in A) = \\left\\{\\begin{matrix}\n1 & \\text{if } s \\in A \\\\\n0 & \\text{if } s \\notin A\n\\end{matrix}\\right.\n\\]\n\n\nRecall from Lecture 1:\n\\[\n\\begin{align}\nX& \\sim \\text{Uniform}(-1, 1) \\\\\nY& \\sim \\text{Uniform}(-1, 1) \\\\\n\\pi& \\approx \\frac{4 \\sum_{i=1}^{N} \\text{I}(x_i^2 + y_i^2 &lt; 1)}{N}\n\\end{align}\n\\]\n\n\n\n\n\nThere is a link between probability and (expectations of) indicator RVs\nJoe Blitzstein calls it the fundamental bridge: \\(\\P(A) = \\E(\\I_A)\\)\n\n\n\n\n# Pr of two heads in five trials\n(P2 &lt;- dbinom(2, 5, prob = 1/2))\n\n[1] 0.3125\n\n# Let's create a realization Binom(5, 1/2)\nx &lt;- rbinom(1e6, size = 5, prob = 1/2)\nx[1:30]\n\n [1] 1 3 3 3 5 0 3 3 2 4 4 4 2 2 4 1 3 2 5 3 4 3 2 3 3 2 2 3 2 4\n\n# indicator RV: if x == 2 Ix = TRUE (1), else Ix = (0)\nI2 &lt;- (x == 2)\nI2[1:30] |&gt; as.integer()\n\n [1] 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0\n\n# compute E(Ix) == P2\nmean(I2)\n\n[1] 0.311906"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#lotus",
    "href": "stats-math/05-lecture/05-lecture.html#lotus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "LOTUS",
    "text": "LOTUS\n\n\nLOTUS stands for the Law Of The Unconscious Statistician\nIt allows us to work with PDFs of the original RV \\(X\\), even though we want to compute an expectation of the function of that RV, say \\(g(X)\\)\n\n\n\n\\[\n\\begin{eqnarray}\nE(g(X)) &=& \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx \\\\\nE(g(X)) &=& \\sum_{x} g(x) p_X(x)\n\\end{eqnarray}\n\\]\n\n\n\nNotice, that we don’t need to compute \\(f_{g(X)}(x)\\) or \\(p_{g(X)}(x)\\)\nThe idea is that even though the \\(x\\)s change from, say \\(x\\) to \\(x^2\\), reflecting the fact the \\(g(X) = X^2\\), that did not change their probability assignments, we can still work with \\(\\P(X = x)\\) (in case of X discrete)"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#variance",
    "href": "stats-math/05-lecture/05-lecture.html#variance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Variance",
    "text": "Variance\n\nDefinitionsComputation\n\n\n\n\nVariance is a measure of the spread of the distribution \\(\\V(X) = \\E(X - E(X))^2 = \\E(X^2) - \\E(X)^2\\)\nIf we let \\(\\mu = \\E(X)\\), this becomes: \\(\\E(X - \\mu)^2 = \\E(X^2) - \\mu^2\\)\nUnlike Expectation, Variance is not a linear operator. In particular, \\(\\V(cX) = c^2 \\V(X)\\)\n\\(\\V(c + X) = \\V(X)\\): constants don’t vary\nIf \\(X\\) and \\(Y\\) are independent \\(\\V(X + Y) = \\V(X) + \\V(Y)\\), but unlike for Expectations, this is not true in general\nSquare root of Variance is called a standard deviation \\(\\text{sd} := \\sqrt{\\V}\\), which is easier to interpret as it is expressed in the same units as data \\(x\\)\n\n\n\n\n\nIn R, estimated Variance can be computed with var() and standard deviation with sd()\nFor reference, the variance of the Geometric distribution is: \\((1 - \\theta)/\\theta^2\\)\n\n\nn &lt;- 1e5; theta1 &lt;- 1/6; theta2 &lt;- 1/3\nx &lt;- rgeom(n, prob = theta1)\nvar(x) |&gt; round(2)\n\n[1] 29.96\n\ny &lt;- rgeom(n, prob = theta2)\nvar(y) |&gt; round(2)\n\n[1] 6\n\n(var(x) + var(y)) |&gt; round(2)\n\n[1] 35.96\n\nvar(x + y) |&gt; round(2)\n\n[1] 35.89\n\ncov(x, y) |&gt; round(2)\n\n[1] -0.03\n\n(var(x) + var(y) + 2*cov(x, y)) |&gt; round(2)\n\n[1] 35.89\n\n# compare to the analytic result\n(1 - theta1) / (theta1)^2 + (1 - theta2) / (theta2)^2 \n\n[1] 36"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#poisson-random-variable",
    "href": "stats-math/05-lecture/05-lecture.html#poisson-random-variable",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Poisson Random Variable",
    "text": "Poisson Random Variable\n\n\nPoisson distributions arise when we are modeling counts\nBut not every type of counting can be modeled with Poisson, just like not every kind of waiting can be modeled by Geometric\nWe write \\(X \\sim \\text{Poisson}(\\lambda)\\)\nThe PDF of Poisson is \\(\\P(X=x) = \\frac{{\\lambda^x e^{-\\lambda}}}{{x!}}\\), where \\(x = 0, 1, 2, ...\\) and \\(\\lambda &gt; 0\\)\n\n\n\n\\[\n\\begin{eqnarray}\n\\sum_{x=0}^{\\infty} \\frac{{\\lambda^x e^{-\\lambda}}}{{x!}} = e^{-\\lambda} \\sum_{x=0}^{\\infty} \\frac{\\lambda^x}{x!} = 1\n\\end{eqnarray}\n\\]\n\n\n\nThe last equality follows, because: \\(e^{\\lambda} = \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}\\), which is its Taylor Series expansion\nFor Poisson, \\(\\E(X) = \\lambda\\) and \\(\\V(X) = \\lambda\\), which is why most real count data do not follow this distribution"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#poisson-pmf",
    "href": "stats-math/05-lecture/05-lecture.html#poisson-pmf",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Poisson PMF",
    "text": "Poisson PMF\n\nNotice the location of \\(\\E(X) = \\lambda\\) in each plot\n\n\np1 &lt;- dot_plot(0:10, dpois(0:10, lambda = 3)) + xlab(\"x\") + \n  ylab(expression(P(X == x))) + ggtitle(\"X ~ Poisson(3)\")\np2 &lt;- dot_plot(0:22, dpois(0:22, lambda = 10)) + xlab(\"x\") + \n  ylab(expression(P(X == x))) + ggtitle(\"X ~ Poisson(10)\")\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#continuous-rvs-and-the-uniform",
    "href": "stats-math/05-lecture/05-lecture.html#continuous-rvs-and-the-uniform",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Continuous RVs and the Uniform",
    "text": "Continuous RVs and the Uniform\n\nDefinitionsUniformExample: Stick breaking\n\n\n\n\nWe leave the discrete world and enter continuous RVs\nWe can no longer say, \\(\\P(X = x)\\), since for a continuous RV \\(\\P(X = x) = 0\\) for all \\(x\\)\nInstead of PMFs, we will be working with PDFs, and we get probability out of them by integrating over the region that we care about\nFor a continuous RV: \\(\\int_{-\\infty}^{\\infty} f_X(x)\\, dx = 1\\)\n\n\n\n\\[\n\\begin{eqnarray}\nP(a &lt; X &lt; b) & = & \\int_{a}^{b} f_X(x)\\, dx \\\\\nF_X(x) & = & \\int_{-\\infty}^{x} f_X(u)\\, du\n\\end{eqnarray}\n\\]\n\n\n\n\nUniform \\(X \\sim \\text{Uniform}(\\alpha, \\beta)\\) has the following PDF:\n\\[\n\\text{Uniform}(x|\\alpha,\\beta) =\n\\frac{1}{\\beta - \\alpha}, \\,  \\text{where } \\alpha \\in \\mathbb{R} \\text{ and } \\beta \\in (\\alpha,\\infty)\n\\]\n\n\n\nYour Turn: Guess the \\(\\E(X)\\)\nNow derive \\(\\E(X)\\) using the definition of the Expected Value: \\(\\E(X) = \\int x f_X(x)\\, dx\\)\n\n\n\n\nx &lt;- seq(-0.5, 1.5, length = 100)\npdf_x &lt;- dunif(x, min = 0, max = 1)\np &lt;- ggplot(data.frame(x, pdf_x), aes(x, pdf_x))\np + geom_line() + ylab(expression(p[X](x))) +\n  ggtitle(\"X ~ Unif(0, 1)\", subtitle = expression(PDF: p[X](x) == 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou break a unit-length stick at a random point\nLet \\(X\\) be the breaking point so \\(X \\sim \\text{Uniform}(0, 1)\\)\nLet \\(Y\\) be the larger piece. What is \\(\\E(Y)\\)?\nLOTUS, says that we do not need \\(f_Y\\), we can work with \\(f_X\\)\n\n\n\n\\[\n\\E(Y)= \\int y(x) f_X(x) dx\n\\]\n\n\n\nThis works only if \\(Y\\) is a function of \\(X\\). Here, \\(Y = \\max\\{X, 1 - X\\}\\).\nWe consider two cases: when \\(x\\) is larger than \\(1/2\\), \\(y(x)\\) is between \\(1/2\\) and \\(1\\) and when \\(1 - x\\) is larger, it is between \\(0\\) and \\(1/2\\):\n\n\n\n\\[\ny(x) = \\left\\{\n\\begin{array}{ll} x &\n\\text{if } 1/2 &lt; x &lt; 1, \\text{ and} \\\\ 1 - x & 0 &lt; x &lt; 1/2\n\\end{array}  \\right.\n\\]\n\n\n\nIn other words, \\(\\E(Y)\\) can be computed as the sum of two integrals:\n\n\\[\n\\E(Y) = \\int_{0}^{1} y(x) f_X(x)\\, dx =    \\\\\n\\int_{1/2}^{1} x \\cdot 1 \\, dx + \\int_{0}^{1/2} (1-x) \\cdot 1 \\, dx = \\\\\n\\frac{x^2}{2} \\Biggr|_{1/2}^{1} + \\frac{1}{2} - \\frac{x^2}{2} \\Biggr|_{0}^{1/2} = \\frac{3}{4}\n\\]\n\n\n\nLet’s do a quick simulation in R\n\n\nx &lt;- runif(1e4, min = 0, max = 1) # pick a random breaking point on (0, 1)\ny &lt;- ifelse(x &gt; 0.5, x, 1 - x)    # generate y = max(x, 1-x)\nmean(y) |&gt; round(3)               # estimate the expectation E(Y)\n\n[1] 0.748"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#normal-rv",
    "href": "stats-math/05-lecture/05-lecture.html#normal-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Normal RV",
    "text": "Normal RV\n\nNormalProperties of Standard NormalExample: Heights of US adults\n\n\n\n\n\\(X \\sim \\text{Normal}(\\mu, \\sigma)\\) has the following PDF:\n\n\n\n\\[\n\\text{Normal}(x \\mid \\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{x - \\mu}{\\sigma} \\right)^2    \\right) \\!\n\\]\n\n\n\nThe bell shape comes from the \\(\\exp(-x^2)\\) part\nThe expected value is \\(\\E(X) = \\mu\\), the mode (highest peak) and median are also \\(\\mu\\).\nVariance is \\(\\V(X) = \\sigma^2\\) and standard deviation \\(\\text{sd} = \\sigma\\)\nA Normal RV can be converted to standard normal by subtracting \\(\\mu\\) and dividing by \\(\\sigma\\)\n\n\n\n\\[\n\\text{Normal}(x \\mid 0, 1) \\ = \\\n\\frac{1}{\\sqrt{2 \\pi}} \\, \\exp \\left( \\frac{-x^2}{2} \\right)\\\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSums of small contributions tend to have a normal distribution\nHeights of people stratified by gender is a good example\nThe following data come from “Teaching Statistics, A Bag of Tricks” by Gelman and Nolan\n\n\n\n\nmu_m &lt;- 69.1   # mean heights of US males in inches\nsigma_m &lt;- 2.9 # standard deviation for same\nmu_w &lt;- 63.7   # mean heights of US women in inches\nsigma_w &lt;- 2.7 # standard deviation of the same\n\nx &lt;- seq(50, 80, len = 1e3)\npdf_m &lt;- dnorm(x, mean = mu_m, sd = sigma_m)\npdf_w &lt;- dnorm(x, mean = mu_w, sd = sigma_w)\np &lt;- ggplot(data.frame(x, pdf_m, pdf_w), aes(x, pdf_m))\np &lt;- p + geom_line(size = 0.2, color = 'red') + \n  geom_line(aes(y = pdf_w), size = 0.2, color = 'blue') +\n  xlab(\"Height (in)\") + ylab(\"\") +\n  ggtitle(\"Distribution of heights of US adults\") +\n  annotate(\"text\", x = 73.5, y = 0.10, label = \"Men\", color = 'red') +\n  annotate(\"text\", x = 58, y = 0.10, label = \"Women\", color = 'blue') +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nThe combined distribution is not Normal:\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou randomly sample a man from the population. What is \\(\\P(\\text{Height}_m &lt; 65)\\)\n\n\nintegrate(dnorm, lower = -Inf, upper = 65, \n          mean = mu_m, sd = sigma_m)$value |&gt;\n  round(2)\n\n[1] 0.08\n\n\n\n\n\nYou randomly sample a woman from the population. What is \\(\\P(60 &lt; \\text{Height}_w &lt; 70)\\)\n\n\nintegrate(dnorm, lower = 60, upper = 70, \n          mean = mu_w, sd = sigma_w)$value |&gt;\n  round(2)\n\n[1] 0.9\n\n(integrate(dnorm, lower = -Inf, upper = 70, \n          mean = mu_w, sd = sigma_w)$value - \nintegrate(dnorm, lower = -Inf, upper = 60, \n          mean = mu_w, sd = sigma_w)$value) |&gt;\n  round(2)\n\n[1] 0.9\n\n\n\n\n\nWhat is the probability that a randomly chosen man is taller than a randomly chosen woman?\nWe have two distributions \\(M \\sim \\text{Normal}(\\mu_m, \\sigma_m)\\) and \\(W \\sim \\text{Normal}(\\mu_w, \\sigma_w)\\). We want \\(\\P(M &gt; W) = P(Z &gt; 0),\\, \\text{where }Z = M - W\\)\nSum of two normals is normal where both means and variances sum:\n\n\n\n\\[\n\\begin{eqnarray}\nZ & \\sim & \\text{Normal}(\\mu_m + \\mu_w,\\, \\sigma_m^2 + \\sigma_m^2) \\\\\nE(Z) & = & E(M - W) = E(M) - E(W) = 69.1 - 63.7 = 5.4 \\\\\n\\text{Var}(Z) & = & \\text{Var}(M - W) = \\text{Var}(M) + \\text{Var}(W) =  2.9^2 + 2.7^2 = 15.7 \\\\\n\\text{sd} & = & \\sqrt{Var} = \\sqrt{15.7} =3.96 \\\\\nZ & \\sim & \\text{Normal}(5.4, 3.96)\n\\end{eqnarray}\n\\]\n\n\n\nTo figure out when \\(Z &gt; 0\\), we can integrate the PDF from 0 to Infinity:\n\n\nintegrate(dnorm, lower = 0, upper = Inf, \n          mean = 5.4, sd = sqrt(15.7))$value |&gt;\n  round(2)\n\n[1] 0.91\n\n\n\n\n\nWe don’t have to integrate. We can evaluate the CDF instead:\n\n\n1 - pnorm(0, mean = 5.4, sd = sqrt(15.7)) |&gt;\n  round(2)\n\n[1] 0.91\n\n\n\n\n\nBy symmetry, the probability that a randomly chosen woman is taller than a randomly chosen man is 0.09\nHow can we check the analytic solution? We can do a simulation!\n\n\n\n\nn &lt;- 1e5\ntaller_m &lt;- numeric(n)\nfor (i in 1:n) {\n  height_m &lt;- rnorm(1, mu_m, sigma_m)\n  height_w &lt;- rnorm(1, mu_w, sigma_w)\n  taller_m[i] &lt;- height_m &gt; height_w\n}\nmean(taller_m) |&gt; round(2)\n\n[1] 0.91\n\n\n\n\n\nSuppose you wanted to compute the variance this way. How would you do it?"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#what-we-did-not-cover",
    "href": "stats-math/05-lecture/05-lecture.html#what-we-did-not-cover",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What We Did Not Cover",
    "text": "What We Did Not Cover\n\nJoint, Marginal, and Conditional P[M/D]Fs\nCovariance and correlation\nConditional Expectations\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#session-4-outline",
    "href": "stats-math/04-lecture/04-lecture.html#session-4-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 4 Outline",
    "text": "Session 4 Outline\n\n\nWhat is probability?\nHow does it relate to statistics?\nSample spaces and arithmetic of sets\nHow to count without counting\nConditional probability\nIndependence and Bayes rule\nExamples\n\nCOVID testing\nBirthday problem\nLeibniz’s error\nMonte Hall"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#what-is-probability",
    "href": "stats-math/04-lecture/04-lecture.html#what-is-probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What is Probability",
    "text": "What is Probability\n\n\n\n\nStatistics is the art of quantifying uncertainty, and probability is the language of statistics\nProbability is a mathematical object\nPeople argue over the interpretation of probability\nPeople don’t argue about the mathematical definition of probability\n\n\n\n\n\n\n\n\nAndrei Kolmogorov (1903 — 1987)\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#laplaces-demon",
    "href": "stats-math/04-lecture/04-lecture.html#laplaces-demon",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world”"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#sample-spaces",
    "href": "stats-math/04-lecture/04-lecture.html#sample-spaces",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Sample Spaces",
    "text": "Sample Spaces\n\n\nA sample space which we will call \\(S\\) is a set of all possible outcomes \\(s\\) of an experiment.\nIf we flip a coin twice, our sample space has \\(4\\) elements: \\(\\{TT, TH, HT, HH\\}\\).\nAn event \\(A\\) is a subset of a sample space. We say that \\(A \\subseteq S\\)\nPossible events: 1) At least one \\(T\\); 2) The first one is \\(H\\); 3) Both are \\(T\\); 4) Both are neither \\(H\\) nor \\(T\\); etc"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#sample-spaces-and-de-morgan",
    "href": "stats-math/04-lecture/04-lecture.html#sample-spaces-and-de-morgan",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Sample Spaces and De Morgan",
    "text": "Sample Spaces and De Morgan\n\n\nA compliment of an event \\(A\\) is \\(A^c\\). Formally, this is \\(A^c = \\{s \\in S: s \\notin A\\}\\)\nSuppose, \\(S = \\{1, 2, 3, 4\\}\\), \\(A = \\{1, 2\\}\\), \\(B = \\{2, 3\\}\\), and \\(C = \\{3, 4\\}\\)\nThen, the union of A and B is \\(A \\cup B = \\{1, 2, 3\\}\\), \\(A \\cap B = \\{2\\}\\), \\(A^c = \\{3, 4\\}\\), and \\(B^c = \\{1, 4\\}\\). And \\(A \\cap C = \\{\\emptyset \\}\\)\nDe Morgan (1806 — 1871) said that:\n\n\\((A \\cup B)^c = A^c \\cap B^c\\) and\n\\((A \\cap B)^c = A^c \\cup B^c\\)\n\nIn our case, \\((A \\cup B)^c = (\\{1, 2, 3\\})^c = \\{4\\} = (\\{3, 4\\} \\cap \\{1, 4\\})\\)\nYour Turn: Compute \\((A \\cap B)^c = A^c \\cup B^c\\)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#naive-definition",
    "href": "stats-math/04-lecture/04-lecture.html#naive-definition",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Naive Definition",
    "text": "Naive Definition\n\n\nNaive definition of probability is an assumption that every outcome is equally likely\nIn that case, \\(\\P(A) = \\frac{\\text{number of times A occurs}}{\\text{number of total outcomes}}\\)\nWhat is the probability of rolling an even number on a six-sided die?\n\n\\(\\P(\\text{Even Number}) = \\frac{3}{6} = \\frac{1}{2}\\)\n\nWhat is the probability of rolling a prime number?\n\n\\(\\P(\\text{Prime Numner}) =\\)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#counting",
    "href": "stats-math/04-lecture/04-lecture.html#counting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Counting",
    "text": "Counting\n\n\nYour mom feels probabilistic, so she asks you to flip a coin. If it lands heads, you get pizza for dinner, and if lands tails, you get broccoli. You flip again for dessert. If it lands heads, you get ice cream, and if it lands heads, you get a grasshopper cookie. Assume you are not a fan of veggies and insects. What is the probability of having a completely disappointing meal? What is the probability of being partly disappointed?\nFrom Blitzstein and Chen (2015): Suppose that 10 people are running a race. Assume that ties are not possible and that all 10 will complete the race, so there will be well-defined first place, second place, and third place winners. How many possibilities are there for the first, second, and third place winners?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#basic-counting",
    "href": "stats-math/04-lecture/04-lecture.html#basic-counting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Basic Counting",
    "text": "Basic Counting\n\n\nHow many permutations of \\(n\\) object are there?\nSampling with replacement (order matters): how many ways are there to select \\(k\\) elements from \\(n\\) objects?\nSampling without replacement (order matters): how many ways are there to select \\(k\\) elements from \\(n\\) objects?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#binomial-coefficient",
    "href": "stats-math/04-lecture/04-lecture.html#binomial-coefficient",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial coefficient",
    "text": "Binomial coefficient\n\n\nHow many ways are there to choose a number of subsets from a set; e.g., how many two-person teams can be formed from five people? (here, John and Jane is the same set as Jane and John)\n\n\n\n\\[\n{n \\choose k} = \\frac{n!}{(n - k)! k!} = {n \\choose n-k}\n\\]\n\n\n\nWhat is the probability of the full house in poker? (3 cards of the same rank and 2 cards of the other rank)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#birthday-problem",
    "href": "stats-math/04-lecture/04-lecture.html#birthday-problem",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\nProblem DescriptionSimulationPlotAnalysis\n\n\nThere are \\(j\\) people in a room. Assume each person’s birthday is equally likely to be any of the 365 days of the year (excluding February 29) and that people’s birthdays are independent. What is the probability that at least one pair of group members has the same birthday?\n\n\n\nn &lt;- 1e5\nn_people &lt;- 80 \nj &lt;- 2:n_people\ndays &lt;- 1:365 \n\nfind_match &lt;- function(x, d) {\n  y &lt;- sample(d, x, replace = TRUE)\n  match_found &lt;- length(unique(y)) &lt; length(y)\n}\n\nprop &lt;- numeric(n_people - 1)\nfor (i in seq_along(j)) {\n  matches &lt;- replicate(n, find_match(i, days))\n  prop[i] &lt;- mean(matches)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s say we have \\(j\\) people. There are 365 ways in which the first person can get a birthday, 365 for the second, and so on, assuming no twins. So the denominator is \\(365^j\\)\nFor the numerator, it is easier to compute a probability of no match. Let’s say we have three people in the room. The numerator would be \\(365 \\cdot 364 \\cdot 363\\), the last term being \\(365 - j + 1\\)\n\n\n\n\\[\n\\P(\\text{At least one match}) = 1 - P(\\text{No match}) = \\\\\n1 - \\frac{\\prod_{i = 0}^{j-1} (365-i)}{365^j} = 1 -\n\\frac{365 \\cdot 364 \\cdot 363 \\cdot \\,... \\, \\cdot (365 - j + 1)}{365^j}\n\\]\n\n\n\nmax_people &lt;- 23\nPr_match &lt;- numeric(max_people - 1)\nfor (j in 2:max_people) {\n  numerator &lt;- prod(seq(365, 365 - j + 1))\n  Pr_match[j - 1] &lt;- round(1 - (numerator / 365^j), 2)\n}\ncat(Pr_match, sep = \", \")\n\n0, 0.01, 0.02, 0.03, 0.04, 0.06, 0.07, 0.09, 0.12, 0.14, 0.17, 0.19, 0.22, 0.25, 0.28, 0.32, 0.35, 0.38, 0.41, 0.44, 0.48, 0.51"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#probability",
    "href": "stats-math/04-lecture/04-lecture.html#probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Probability",
    "text": "Probability\n\nProbability \\(\\P\\) assigns a real number to each event \\(A\\) and satisfies the following axioms:\n\n\n\\[\n\\begin{eqnarray}\n\\P(A) & \\geq & 0 \\text{ for all } A \\\\\n\\P(S) & = & 1 \\\\\n\\text{If } \\bigcap_{i=1}^{\\infty}A_i = \\{\\emptyset \\} & \\implies &\n\\P\\left( \\bigcup_{i=1}^{\\infty}A_i  \\right) = \\sum_{i=1}^{\\infty}\\P(A_i)\n\\end{eqnarray}\n\\]\n\n\n\nFor \\(S = \\{1, 2, 3, 4\\}\\), where \\(s_i = 1/4\\), and \\(A = \\{A_1, A_2\\}\\), where \\(A_1 = \\{1\\}\\) and \\(A_2 = \\{2\\}\\). Verify that all 3 axioms hold."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#some-consequences",
    "href": "stats-math/04-lecture/04-lecture.html#some-consequences",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Consequences",
    "text": "Some Consequences\n\n\\[\n\\begin{eqnarray}\n\\P\\{\\emptyset\\} & = & 0 \\\\\nA \\subseteq B  & \\implies & \\P(A) \\leq \\P(B) \\\\\n\\P(A^c) & = & 1 - P(A) \\\\\nA \\cap B = {\\emptyset} & \\implies & \\P(A \\cup B) = \\P(A) + \\P(B) \\\\\n\\P(A \\cup B) & = & \\P(A) + \\P(B) - \\P(A \\cap B)\n\\end{eqnarray}\n\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n\\]\n\n\n\nThe last one is called inclusion-exclusion\nTwo events are independent \\(A \\indep B\\) if:\n\n\n\n\\[\n\\P(A \\cap B) = \\P(A) \\P(B)\n\\]"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#some-examples",
    "href": "stats-math/04-lecture/04-lecture.html#some-examples",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Examples",
    "text": "Some Examples\n\nSimulationAnalysisYour Turn\n\n\nYou flip a fair coin four times. What is the probability that you get one or more heads?\n\n\none_or_more &lt;- function() {\n  flips &lt;- sample(c(1, 0), size = 4, replace = TRUE)\n  if (mean(flips) &gt;= 1/4) {\n    return(TRUE)\n  } else {\n    return(FALSE)\n  }\n}\n# flip the coin 10,000 times\nx &lt;- replicate(1e4, one_or_more())\nmean(x)\n\n[1] 0.9332\n\n\n\n\n\nLet \\(A\\) be the probability of at least one Head. Then \\(A^c\\) is the probability of all Tails. Let \\(B_i\\) be the event of Tails on the \\(i\\)th trial.\n\n\\[\n\\begin{eqnarray}\n\\P(A)  = 1 - \\P(A^c) & = & \\\\\n1 - \\P(B_1 \\cap B_2 \\cap B_3 \\cap B_4) & = & \\\\\n1 - \\P(B_1)\\P(B_2)\\P(B_3)\\P(B_4) & = & \\\\\n1 - \\left( \\frac{1}{2} \\right)^4 =  1 - \\frac{1}{16} = 0.9375\n\\end{eqnarray}\n\\]\n\n\n\n\nModify the function so that instead of 1 or more from 4 trials, it computes 1 or more from \\(n\\) trials.\nModify it so it computes \\(x\\) or more from \\(n\\) trials and simulate at least 2 Heads out of 5 trials.\nCan you solve it analytically to validate your simulation results?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#conditional-probability",
    "href": "stats-math/04-lecture/04-lecture.html#conditional-probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditioning is the soul of statistics — Joe Blitzstein\n\n\n\nThink of conditioning as the probability in reduced sample spaces since when we condition, we are looking at the subset of \\(S\\) where some events already occurred.\nNote: In some texts, \\(AB\\) is used as a shortcut for \\(A \\cap B\\). You can’t multiply events, but you can multiply their probabilities.\n\n\n\n\\[\n\\P(A | B) = \\frac{\\P(A \\cap B)}{\\P(B)}\n\\]\n\n\n\nNote that \\(\\P(A|B)\\) and \\(\\P(B|A)\\) are different things. Doctors and lawyers confuse those all the time.\nTwo events are independent iff \\(\\P(A | B) = \\P(A)\\). In other words, learning \\(B\\) does not improve our estimate of \\(\\P(A)\\)."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#law-of-total-probability-and-bayes",
    "href": "stats-math/04-lecture/04-lecture.html#law-of-total-probability-and-bayes",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Law of Total Probability and Bayes",
    "text": "Law of Total Probability and Bayes\n\nLOTPBayes\n\n\n\\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B | A_i) \\P(A_i)\n\\]\n\n\n\n\n\n\n\n\n\nWe take the definition of conditional probability and expand the numerator and denominator:\n\n\n\n\\[\n\\P(A|B) = \\frac{\\P(B \\cap A)}{\\P(B)} = \\frac{\\P(B|A) \\P(A)}{\\sum_{i=1}^{n} \\P(B | A_i) \\P(A_i)}\n\\]\n\n\n\nWe call \\(\\P(A)\\), prior probability of \\(A\\) and \\(\\P(A|B)\\) a posterior probability of \\(A\\) after we learned \\(B\\).\n\n\n\n\n\n\nImage Source: Introduction to Probability, Blitzstein et al."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-medical-testing",
    "href": "stats-math/04-lecture/04-lecture.html#example-medical-testing",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nDerivationDiscussion\n\n\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(P(D^+ | T^+)\\)\n\\(\\text{Specificity } := P(T^- | D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - P(T^- | D^-) = P(T^+ | D^-) = 0.002\\)\n\\(\\text{Sensitivity } := P(T^+ | D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - P(T^+ | D^+) = P(T^- | D^+) = 0.546\\)\nPrevalence: \\(P(D^+) = 0.001\\)\n\n\\[\n\\begin{eqnarray}\nP(D^+ | T^+) = \\frac{P(T^+ | D^+) P(D^+)}{P(T^+)} & = & \\\\\n\\frac{P(T^+ | D^+) P(D^+)}{\\sum_{i=1}^{n}P(T^+ | D^i) P(D^i) } & = & \\\\\n\\frac{P(T^+ | D^+) P(D^+)}{P(T^+ | D^+) P(D^+) + P(T^+ | D^-) P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]\n\n\n\nThe answer, 18%, is very sensitive to the prevalence of disease, or in our language, to the prior probability of an infection\nAt the time of the test, the actual prevalence was estimated at 4.8%, not 0.1%, which would change our answer by a lot: \\(\\P(D^+ | T^+) \\approx 0.92\\)\nLesson: don’t rely on intuition and check the prior"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-leibnizs-error",
    "href": "stats-math/04-lecture/04-lecture.html#example-leibnizs-error",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Leibniz’s Error",
    "text": "Example: Leibniz’s Error\n\nYou have two 6-sided fair dice. You roll the dice and compute the sum. Which one is more likely 11 or 12?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-monte-hall",
    "href": "stats-math/04-lecture/04-lecture.html#example-monte-hall",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Monte Hall",
    "text": "Example: Monte Hall\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#session-6-outline",
    "href": "stats-math/06-lecture/06-lecture.html#session-6-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 6 Outline",
    "text": "Session 6 Outline\n\n\nVectors and vector arithmetic\nMatrices and matrix arithmetic\nDeterminants\nMatrix inverses\nSolving linear systems\n\n\n\nInspiration for a lot of the examples came from the Essense of Linear Algebra by 3blue1brown\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nLinear Algebra offers a language for manipulating n-dimensional objects\nVectors are the basic building block of Linear Algebra\nA straightforward way to think about them is as a column of numbers\nHow we interpret the entries is up to us\nVectors are typically written in a column form:\n\n\n\n\\[\\begin{align} V = \\left[\\begin{matrix}a\\\\b\\\\c\\end{matrix}\\right] \\end{align}\\]\n\n\n\nIf we want the row version, we transpose it:\n\n\\[\\begin{align} V^T = \\left[\\begin{matrix}a & b & c\\end{matrix}\\right] \\end{align}\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-1",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nIn R, vectors, unfortunately, are neither row nor column vectors\nR makes some assumptions when performing vector-matrix arithmetic\nWe already saw lots of vectors whenever we used c() function or generated random numbers with, say runif() function\n\n\n\n\nset.seed(123)\n(v &lt;- c(sample(3)))\n\n[1] 3 1 2\n\nclass(v)\n\n[1] \"integer\"\n\n\n\n\n\nR reports the \\(v\\) is an integer vector\nYou can add two vectors in a usual way, elementwise:\n\n\\[\\begin{align} \\left[\\begin{matrix}3\\\\1\\\\2\\end{matrix}\\right]+\\left[\\begin{matrix}2\\\\1\\\\3\\end{matrix}\\right]=\\left[\\begin{matrix}5\\\\2\\\\5\\end{matrix}\\right] \\end{align}\\]\n\n\n\nIn R:\n\n\nv\n\n[1] 3 1 2\n\nw\n\n[1] 2 1 3\n\nv + w\n\n[1] 5 2 5"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-2",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nYou can take linear combination \\(av + bw\\), where a and b are scalars\n\n\n2*v + 3*w # linear combination\n\n[1] 12  5 13\n\n\n\n\n\nYou can also take dot product: \\(v \\cdot w\\), which results in a scalar\n\n\\[\\begin{align} v \\cdot w = \\left[\\begin{matrix}3 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}2\\\\1\\\\3\\end{matrix}\\right]=\\left[\\begin{matrix}13\\end{matrix}\\right] \\end{align}\\]\n\n\n\nIn R, we multiply vectors and Matrices with %*%, not *, which will produce a component-wise multiplication, not a dot product\n\n\nv %*% w # dot product\n\n     [,1]\n[1,]   13\n\nv * w   # component wise multiplication\n\n[1] 6 1 6\n\n\n\n\n\nWe write dot product this way:\n\n\\[\nv^T w = v_1w_1 + v_2w_2 + \\cdots + v_nw_n\n\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-3",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-3",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nWhen is dot product zero?\n\n\nv &lt;- c(1, 1)\nw &lt;- c(1, -1)\nv %*% w\n\n     [,1]\n[1,]    0\n\n(3 * v) %*% (4 * w)\n\n     [,1]\n[1,]    0\n\n\n\n\n\nThese vectors are perpendicular (orthogonal)"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-4",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-4",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\nA length of a vector is the square root of the dot product with itself\n\n\\[\n||v|| = \\sqrt{v \\cdot v} = \\sqrt{v_1^2 + v_2^2 + \\cdots v_n^2}\n\\]\n\nv &lt;- c(1, 1)  \nsqrt(v %*% v) # this is sqrt of 2 \n\n         [,1]\n[1,] 1.414214"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Matrices",
    "text": "Introduction to Matrices\n\n\nYou can think of a matrix as a rectangular (or square) set of numbers\n\\(m{\\times}n\\) matrix has m rows and n columns\nIn statistics, it is convenient to think of a matrix as n columns where each column is a variable like the price of the house, and the rows are the observations for each house\nIn Linear Algebra books, you will often see linear equations written as \\(Ax = b\\), where we are trying to find \\(x\\)\nIn statistics, we usually write the same thing as \\(X\\beta = y\\), and we are trying to find \\(\\beta\\)\nAnother source of confusion: matrix \\(A\\) is sometimes called a coefficient matrix in Linear Algebra books. In statistics, the entries in \\(A\\) (the \\(X\\) matrix) have observations, and the \\(\\beta\\) vector contains coefficients estimated from A and b (X and y)."
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices-1",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Matrices",
    "text": "Introduction to Matrices\n\nBasisPlot of v and wTransformationR Example\n\n\n\n\nA good way to think about the Matrices is that they act on vectors and transform them in some way (stretch or turn them)\nYou can think of this transformation as encoding the eventual locations of the basis vectors\nStandard basis vectors in \\(R^2\\) (two-dimensional space), are commonly called \\(\\hat{i}\\) with location \\((1, 0)\\), and \\(\\hat{j}\\) with location \\((0, 1)\\).\nNotice their dot product is zero: \\(\\hat{i} \\cdot \\hat{j} = 0\\) and so they are orthogonal\nLet’s look at one simple transformation – rotation by 90 degrees counter-clockwise\n\n\n\n\n\n\n\n\nRecall, vectors \\(v\\) and \\(w\\):\n\n\\[\nv = \\left[ \\begin{matrix}1\\\\2\\end{matrix} \\right]\nw = \\left[ \\begin{matrix}3\\\\1\\end{matrix} \\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following matrix encodes a 90-degree, counterclockwise rotation. Why?\n\n\\[\nR =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\]\n\n\n\nLet’s see how this matrix will act on vectors \\(v\\) and \\(w\\). That’s where multiplication comes in.\n\n\\[\nRv =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n=\n1\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n+\n2\n\\begin{bmatrix}\n-1 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n-2 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n\n\n\nWe scale the first vector, then scale the second, and then add.\n\n\n\n\n\n\n\n\nThe following shows the same operation you can do in one step in R.\n\n\nR &lt;- matrix(c(0, 1, -1, 0), ncol = 2)\nR\n\n     [,1] [,2]\n[1,]    0   -1\n[2,]    1    0\n\nv &lt;- c(1, 2)\n(v_prime &lt;- R %*% v)\n\n     [,1]\n[1,]   -2\n[2,]    1\n\n\n\n\n\n\nNow let’s plot \\(v\\) and \\(v'\\). Notice, the 90 degree rotation"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#matrix-multiplication",
    "href": "stats-math/06-lecture/06-lecture.html#matrix-multiplication",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\n\nMultiplicationPlotYour Turn\n\n\n\n\nOur matrix encodes a rotation of every single vector in \\(R^2\\).\nIn particular, it rotates every point \\((x, y)\\) in \\(R^2\\) by 90 degrees\nWe can do this transformation in one go by applying the Rotation matrix to another matrix comprising our vectors \\(v\\) and \\(w\\)\nThe number of columns in the Rotation matrix has to match the number of rows in the \\(K = [v, w]\\) matrix\n\n\n\n\n(K &lt;- matrix(c(v, w), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    1\n\n(K_prime &lt;- R %*% K)\n\n     [,1] [,2]\n[1,]   -2   -1\n[2,]    1    3\n\n\n\n\n\nThe resulting matrix \\(K\\), has the correctly rotated \\(v\\) in the first column and rotated \\(w\\) in the second column.\nAnother way to think about this operation is to encode two transformations in \\(K'\\) — the \\(K\\) transformation followed by the \\(R\\) transformation. We can now use the resulting \\(K'\\) matrix and apply these two transformations in one swoop to any vector in \\(R^2\\).\nThink about why matrix multiplication, in general, does not commute — \\(RK \\neq KR\\)\n\n\n\n\n\nRotating red (\\(v = [1 \\ 2]\\)) and blue (\\(w = [3 \\ 1]\\)) vectors by 90 degrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn: Come up with a 90-degree clockwise rotation matrix and show that it sends \\((2, 2)\\) to \\((2, -2)\\)\nNow pick three vectors in \\(R^2\\) and rotate all three at the same time"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#linear-independence-and-determinants",
    "href": "stats-math/06-lecture/06-lecture.html#linear-independence-and-determinants",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Independence and Determinants",
    "text": "Linear Independence and Determinants\n\n\n\n\nThink of a determinant as a (signed) area scaling factor from the basis \\((\\hat{i}, \\hat{j})\\) to the transformed vectors\nThe area represented by two \\(R^2\\) basis vectors is 1\n\n\n\n\n# this matrix scales the area by 12\n(X &lt;- matrix(c(4, 0, 0, 3), ncol = 2))\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    0    3\n\n# compute the determinant\ndet(X)\n\n[1] 12\n\n\n\n\n\nWhat happens if the columns of \\(X\\) are linear combinations of each other?\nGeometrically, that means that in \\(R^2\\), they both sit on the same line\n\n\n\n\n\n# the second column is the first column * 1.5\n(X &lt;- matrix(c(2, 1, 1.5 * 2, 1.5 * 1), ncol = 2))\n\n     [,1] [,2]\n[1,]    2  3.0\n[2,]    1  1.5\n\n# compute the determinant\ndet(X)\n\n[1] 0"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#example-random-matrix",
    "href": "stats-math/06-lecture/06-lecture.html#example-random-matrix",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Random Matrix",
    "text": "Example: Random Matrix\n\n\nGenerate a random, say 5x5 matrix\nIs it likely that we will get linearly dependent columns?\n\n\n\n\nset.seed(123)\n(X &lt;- matrix(sample(25), ncol = 5))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   15   18    9   25    2\n[2,]   19   11   21   17   16\n[3,]   14    5   24    1    7\n[4,]    3   23   20   12    8\n[5,]   10    6   22   13    4\n\ndet(X)\n\n[1] 1155943\n\n\n\n\n\nThis is a full rank matrix that encodes a transformation in \\(R^5\\) — 5-dimensional space"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#computing-determinants",
    "href": "stats-math/06-lecture/06-lecture.html#computing-determinants",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Computing Determinants",
    "text": "Computing Determinants\n\nDon’t bother doing it by hand\n\n\\[\\begin{align} A = \\left[\\begin{matrix}a & c\\\\b & d\\end{matrix}\\right] \\end{align}\\begin{align} \\text{det}(A) = a d - b c \\end{align}\\begin{align} B = \\left[\\begin{matrix}a & d & g\\\\b & e & h\\\\c & f & i\\end{matrix}\\right] \\end{align}\\begin{align} \\text{det}(B) = a e i - a f h - b d i + b f g + c d h - c e g \\end{align}\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#linear-systems-and-inverses",
    "href": "stats-math/06-lecture/06-lecture.html#linear-systems-and-inverses",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Systems and Inverses",
    "text": "Linear Systems and Inverses\n\nLinear SystemsNo solutionIdea of an inverseInverse in RAx = b\n\n\n\n\nOne of the key ideas from Linear Algebra that is relevant to statistics is solving linear systems\nThis is where estimating coefficients in \\(X\\beta = y\\) comes from\nIf \\(X\\) is a square matrix, the problem is not statistical but algebraic\nThe reason is that if we have the same number of equations as we have unknowns, we can solve the system exactly unless X is not full rank\nFor example, try to solve this system:\n\n\n\n\\[\n2x + y = 1 \\\\\n4x + 2y = 1\n\\]\n\n\n\nQuestion: Geometrically, what does it mean to solve a system of equations?\n\n\n\n\n\n\nYou should now have a deeper understanding of why there is no solution\n\n\n(A &lt;- matrix(c(2, 4, 1, 2), ncol = 2))\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    4    2\n\ndet(A)\n\n[1] 0\n\n\n\n\n\nBefore we talk about overdetermined systems (more equations than unknowns) that we see in statistics, let’s see how it works in the square matrix case\n\n\n\n\n\n\nTo see what a matrix inverse does, let’s bring back the 90-degree counterclockwise rotation matrix \\(A\\)\n\n\\[\\begin{align} A = \\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right] \\end{align}\\]\n\n\n\nThe idea of an inverse is to reverse the action of this transformation\nIn this case, we need a clockwise 90-degree rotation matrix\nWe can find that matrix by directly tracing the basis vectors\n\n\n\n\\[\\begin{align} A^{-1} = \\left[\\begin{matrix}0 & 1\\\\-1 & 0\\end{matrix}\\right] \\end{align}\\]\n\n\n\n\n\nLet’s check our results in R. When you multiply a matrix by its inverse, you get back the identity matrix (which is like when you divide a scalar by its reciprocal, you get 1)\n\n\n\n\n(A &lt;- matrix(c(0, 1, -1, 0), 2, 2))\n\n     [,1] [,2]\n[1,]    0   -1\n[2,]    1    0\n\n(A_inv &lt;- matrix(c(0, -1, 1, 0), 2, 2))\n\n     [,1] [,2]\n[1,]    0    1\n[2,]   -1    0\n\nA_inv %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# find an inverse using R's solve function\nsolve(A)\n\n     [,1] [,2]\n[1,]    0    1\n[2,]   -1    0\n\n\n\n\n\n\n\nWe are now ready to solve the square linear system \\(Ax = b\\), in the case when A is full rank\n\n\\[\n\\begin{eqnarray}\nAx & = & b \\\\\nA^{-1}Ax & = & A^{-1}b \\\\\nx & = & A^{-1}b\n\\end{eqnarray}\n\\]\n\n\n\nThe general solution in two-dimensional case:\n\n\\[\\begin{align} \\left[\\begin{matrix}a & c\\\\b & d\\end{matrix}\\right]x = \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right] \\end{align}\\begin{align} \\text{det}(A) = a d - b c \\end{align}\\begin{align} A^{-1} = \\left[\\begin{matrix}\\frac{1 + \\frac{b c}{a \\left(d - \\frac{b c}{a}\\right)}}{a} & - \\frac{c}{a \\left(d - \\frac{b c}{a}\\right)}\\\\- \\frac{b}{a \\left(d - \\frac{b c}{a}\\right)} & \\frac{1}{d - \\frac{b c}{a}}\\end{matrix}\\right] \\end{align}\\begin{align} x = \\left[\\begin{matrix}\\frac{b_{1} d - b_{2} c}{a d - b c}\\\\\\frac{a b_{2} - b b_{1}}{a d - b c}\\end{matrix}\\right] \\end{align}\\]\n\n\n\nIn R, solve(A) inverts the matrix, and solve(A, b) solves \\(Ax = b\\).\n\n\n(A &lt;- matrix(sample(9), ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    3    1    7\n[2,]    4    9    2\n[3,]    6    5    8\n\n(b &lt;- c(1, 2, 3))\n\n[1] 1 2 3\n\nsolve(A, b) %&gt;% round(2)\n\n[1]  0.93 -0.14 -0.24\n\n# same as above\nsolve(A) %*% b %&gt;% round(2)\n\n      [,1]\n[1,]  0.93\n[2,] -0.14\n[3,] -0.24"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#your-turn-1",
    "href": "stats-math/06-lecture/06-lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nConsider the following system of equations:\n\n\\[\n\\begin{eqnarray}\n3x + 2y + 1.5z & = & 4 \\\\\n7x + y & = & 2 \\\\\n3y + 2z & = & 1\n\\end{eqnarray}\n\\]\n\nExpress it in matrix form, solve it in R using the solve() function, and validate that the results are correct"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#some-useful-rules",
    "href": "stats-math/06-lecture/06-lecture.html#some-useful-rules",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Useful Rules",
    "text": "Some Useful Rules\n\nIf you put on socks and then shoes, the first to be taken off are the ____\n\n— Gilbert Strang (discussing the order of undoing the inverse)\n\\[\n(ABC \\dots)^{-1} = \\dots C^{-1}B^{-1}A^{-1} \\\\\n(A^T)^{-1} = (A^{-1})^T \\\\\n(A + B)^T = A^T + B^T \\\\\n(ABC \\dots)^T = \\dots C^T B^T A^T\n\\]\n\nFor a complete list, see the Matrix Cookbook"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-n-p-systems",
    "href": "stats-math/06-lecture/06-lecture.html#solving-n-p-systems",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving n > p Systems",
    "text": "Solving n &gt; p Systems\n\n\nStatistical inference is concerned with “solving” overdetermined systems\nWe have lots of observations with relatively few variables. (Sometimes, we have more variables than observations, but we will not discuss it here)\nClearly, there is no exact solution\nIn fact, there are typically infinitely many ways in which you can draw a line through a cloud of points or a plane through n-dimensional space\nOptimization based (or frequentist) inference is concerned with finding the most likely values of the unknowns giving rise to that line (or plane) and approximating their variance\nIntegration based (or Bayesian) inference is concerned with finding a joint PDF of the unknowns – all plausible values weighted by their probability. Therefore, the “estimated variance” is a consequence of this PDF. (Recall our example of estimating variance from the distribution of the difference in heights.) In this sense, Bayesian inference is an uncertainty-preserving system, a very desirable quality.\nFor simple linear models, both of these methods produce similar results, so we will first show the optimized solution."
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y",
    "href": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)\n\n\nWe are switching to a more familiar (to statisticians) notation of \\(X \\beta = y\\) (instead of \\(Ax = b\\))\nWe typically augment the \\(X\\) matrix with the column of 1s on the left to model the intercept term, sometimes called \\(\\beta_0\\)\nThis should make sense when you think about \\(Xb\\) as the linear combination of columns of \\(X\\)\nIn this form, \\(X\\) is usually called the design matrix or the model matrix\nThe problem is that \\(X\\) is not invertible, even if columns of \\(X\\) are linearly independent\nLet’s say that \\(X\\) is a \\(3 \\times 2\\) matrix (3 rows and 2 columns), and columns are linearly independent. Note that \\(\\beta\\) is \\(2 \\times 1\\) and \\(y\\) is \\(3 \\times 1\\).\n\\(X\\) and its linear combinations \\(X\\beta\\) span a plane in three-dimensional space – there is no way for it to reach the target vector \\(y\\) in \\(R^3\\)"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#approximating-a-solution-to-overdetermined-xhatbeta-y",
    "href": "stats-math/06-lecture/06-lecture.html#approximating-a-solution-to-overdetermined-xhatbeta-y",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Approximating a Solution to Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Approximating a Solution to Overdetermined \\(X\\hat{\\beta} = y\\)\n\n\nIf we can not reach \\(y\\), we need to find a vector in the column space of \\(X\\) that is closest (in a certain sense) to \\(y\\)\nThe projection of \\(y\\) onto this plane gives us the answer\n\n\n\n\n\n\n\n\n\n\nImage from Introduction to Linear Algebra, Boyd and Vandenberghe"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y-1",
    "href": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)\n\nSolutionExample of an \\(X^TX\\)\n\n\n\n\nThis residual \\(r\\) is also called the error term\nThis error, \\(y - X\\hat{\\beta}\\), is the smallest when it’s perpendicular to the plane\nAnother way of saying that is that:\n\n\n\n\\[\nX^T(y - X\\hat{\\beta}) = 0\n\\]\n\n\n\nWe can now solve this normal equation:\n\n\\[\n\\begin{eqnarray}\nX^T(y - X\\hat{\\beta}) & = & 0 \\\\\nX^TX\\hat{\\beta} & = & X^Ty \\\\\n(X^TX)^{-1}(X^TX)\\hat{\\beta} & = & (X^TX)^{-1}X^Ty \\\\\n\\hat{\\beta} & = & (X^TX)^{-1}X^Ty\n\\end{eqnarray}\n\\]\n\n\n\n\n\nMatrix \\(X^TX\\) is square and symmetric. It is also invertible if the columns of \\(X\\) are linearly independent:\n\n\n\n\n(X &lt;- matrix(sample(6), ncol = 2))\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    5    4\n[3,]    3    6\n\n(XtX &lt;- t(X) %*% X)\n\n     [,1] [,2]\n[1,]   38   40\n[2,]   40   53\n\nsolve(XtX) %&gt;% round(2)\n\n      [,1]  [,2]\n[1,]  0.13 -0.10\n[2,] -0.10  0.09\n\n(X &lt;- matrix(c(1, 2, 3, 2, 4, 6), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n\n(XtX &lt;- t(X) %*% X)  # will not invert\n\n     [,1] [,2]\n[1,]   14   28\n[2,]   28   56"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line",
    "href": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Motion in the Straight Line",
    "text": "Motion in the Straight Line\n\n\nRecall, from lecture 1, our example of a car moving in a straight line where we have noisy measurements of its position over time\nAt the time, we assumed we were able somehow to find the intercept and slope of the equation\nWe are now in a position to solve the problem"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line-1",
    "href": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Motion in the Straight Line",
    "text": "Motion in the Straight Line\n\nSetupSolutionPlot the Fit\n\n\n\n\nThis is a quadratic function. How can we solve it using linear regression (the least squares method)?\nWe assumed the equation of motion was \\(x(t) = a + bt^2\\)\nThe unknowns are \\(a\\) and \\(b\\)\nLet’s express it in the matrix notation and relabel the variables to match our system: \\(y = \\beta_0 + \\beta x^2\\). Nothing changes, just the labels.\nWhat is our design matrix \\(X\\)? We had 51 observations, so \\(X\\) will have two columns with 51 elements each: a column of 1s and a column representing the squares measurements: \\(x^2\\).\nOur unknown vector \\(\\hat{\\beta}\\) has two elements \\((\\hat{\\beta_0},\\, \\hat{\\beta_1)}\\)\n\\(y\\) is our outcome vector of length 51, capturing the car’s position at each time point \\(x\\).\nThis way, \\(y = X\\hat{\\beta}\\) captures our system in matrix form\n\n\n\n\n\n\n\n\n# length of x (our time vector)\nlength(x)\n\n[1] 51\n\n# first few elements of x\nhead(x)\n\n[1] 0.0 0.1 0.2 0.3 0.4 0.5\n\n# construct the design matrix\nX &lt;- matrix(c(rep(1, length(x)), x^2), ncol = 2)\nhead(X)\n\n     [,1] [,2]\n[1,]    1 0.00\n[2,]    1 0.01\n[3,]    1 0.04\n[4,]    1 0.09\n[5,]    1 0.16\n[6,]    1 0.25\n\n# inspect the vector y\nlength(y)\n\n[1] 51\n\nhead(y)\n\n[1] -2.4417028  6.7615084 -0.7502334 -0.4900157 -3.5129263  1.9331119\n\n\n\n\n\n\nRecall, that the least squares estimate of \\(\\beta\\) is given by \\(\\hat{\\beta} =  (X^TX)^{-1}X^Ty\\)\n\n\n\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat %&gt;% round(2)\n\n     [,1]\n[1,] 1.46\n[2,] 3.01\n\n# compare to R's lm()\nlm(y ~ I(x^2))\n\n\nCall:\nlm(formula = y ~ I(x^2))\n\nCoefficients:\n(Intercept)       I(x^2)  \n      1.460        3.014  \n\n\n\n\n\nWarning: do not use \\((X^TX)^{-1}X^Ty\\) directly as above; it’s computationally inefficient\n\n\n\n\n\n\nd &lt;- data.frame(yhat = beta_hat[1] + beta_hat[2] * x^2)\np + geom_line(aes(y = yhat), data = d, size = 0.2, color = 'red')"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#your-turn-2",
    "href": "stats-math/06-lecture/06-lecture.html#your-turn-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nLook at the dataset mtcars\nPlot mpg against wp, treating mpg as the output variable \\(y\\). (We generally don’t like to use the words dependent vs. independent in this context)\nUsing the normal equations, find the intercept and slope of the best-fit line\nValidate that it matches the output from lm\nNow plot the line over the data and see if it “fits”\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#session-2-outline",
    "href": "stats-math/02-lecture/02-Lecture.html#session-2-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 2 Outline",
    "text": "Session 2 Outline\n\n\n\n\nLinear, exponential, and logarithmic functions\nLimits\nDefinition of the derivative\nRules of differentiation\nThe chain rule and product rules\n\n\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#lines",
    "href": "stats-math/02-lecture/02-Lecture.html#lines",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Lines",
    "text": "Lines\n\nLinesExamplesCode\n\n\n\n\nWe typically use the slope-intercept form of the line: \\(y = a + bx\\), where \\(a\\) is the intercept, and \\(b\\) is the slope (rise over run).\nFor example: \\(y = 1.5 + 0.5x\\)\n\\(1.5\\) is the value of the function when either \\(x = 0\\) or \\(b = 0\\).\nIn practice, \\(b\\) is almost never zero.\n\n\n\n\n\n\nA graph of a line with the equation \\(y = 1 + 2x\\).\n\n\n\n\n\n\n\n\n\n\nA graph of a line with the equation \\(y = 7.5 - 2x\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n# y = 1 + 2x\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = 2, intercept = 1, size = 0.2)\n\n# y = 7.5 + -2x\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = -2, intercept = 7.5, size = 0.2)\n\n# to see some other ways of plotting lines\n?geom_abline"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nTurn the following code into a function called plot_line(slope, intercept)\nTest it for different values of slope and intercept\n\n\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = 2, intercept = 1, size = 0.2)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#exponential-functions",
    "href": "stats-math/02-lecture/02-Lecture.html#exponential-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Exponential Functions",
    "text": "Exponential Functions\n\n\nThe idea of an exponential function is that the rate of change of the function at time \\(t\\) is proportional to the value of the function at time \\(t\\). In other words:\n\n\n\n\\[\n\\frac{d[y(t)]}{dt} = ky(t)\n\\]\n\n\n\nThe solution to this differential equation is the exponential function.\nThink of population growth or growth of an interest-bearing asset."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-compound-interest",
    "href": "stats-math/02-lecture/02-Lecture.html#example-compound-interest",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Compound Interest",
    "text": "Example: Compound Interest\n\nAn asset that has a value \\(A\\) is invested with an annual interest rate \\(r\\). What is the balance in the account, \\(P_1\\), at the end of the first year?\n\n\n\\[\nP_1 = A + rA = A(1 + r)\n\\]\n\n\nAfter year two, the value is:\n\\[\nP_2 = P_1 + rP_1 = P_1(1 + r) = \\\\\nA(1 + r)(1 + r) = A(1 + r)^2\n\\]\n\n\nAnd after year \\(t\\), the value is:\n\\[\nP_n = A(1 + r)^t\n\\]\n\n\nWhat if we compound the interest twice per year? In that case:\n\\[\nP_1 = A \\left (1 + \\frac{r}{2} \\right)^2\n\\]\n\n\nIf we compound \\(n\\) times per year, the principal would be:\n\\[\nP_1 = A \\left (1 + \\frac{r}{n} \\right)^n\n\\]\n\n\nCombining these ideas, if we compound \\(n\\) times per year, for \\(t\\) years, we get:\n\\[\nP = A \\left (1 + \\frac{r}{n} \\right)^{nt}\n\\]\nIf we let \\(n \\to \\infty\\), we call this process exponential growth."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-compound-interest-1",
    "href": "stats-math/02-lecture/02-Lecture.html#example-compound-interest-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Compound Interest",
    "text": "Example: Compound Interest\n\nCodeCompoundingGrowth over time\n\n\n\nrate &lt;- 0.05    # interest rate r\nP &lt;- 100        # pricical P\nn_comp &lt;- 1:100 # number of compoundings n\n\nPn &lt;- function(n, A, r, t) A * (1 + r/n)^(t*n)\nPe &lt;- function(A, r, t) A * exp(r * t)\n\npn &lt;- Pn(n = n_comp, A = P, r = rate, t = 50)\npe &lt;- Pe(A = P, r = rate, t = 50)\n\nd &lt;- data.frame(n_comp, pn)\np &lt;- ggplot(d, aes(n_comp, pn))\np + geom_line(size = 0.2) +\n  geom_hline(yintercept = pe, color = 'red', size = 0.2) +\n  xlab(\"Number of times the interest is compounded\") +\n  ylab(\"Value of an asset\") +\n  ggtitle(\"Value of $100 at r = 0.05 after 50 years\")"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-1",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\nSuppose a particular population of bacteria is known to double in size every 4 hours. If a culture starts with 1000 bacteria, what is the population after 10 hours and 24 hours?"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-number-e",
    "href": "stats-math/02-lecture/02-Lecture.html#the-number-e",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Number \\(e\\)",
    "text": "The Number \\(e\\)\n\n\nRecall the equation for asset value after \\(t\\) years compounded \\(n\\) times:\n\n\\[\nP = A \\left (1 + \\frac{r}{n} \\right)^n = A \\left (1 + \\frac{r}{n} \\right)^{nt}\n\\]\n\n\n\nLet’s make a substituion \\(m = n/r\\) and take limit and \\(m \\to \\infty\\).\n\n\\[\nP = \\lim_{\\Delta m \\to \\infty}A \\left ( 1 + \\frac{1}{m} \\right)^{m(rt)}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-number-e-1",
    "href": "stats-math/02-lecture/02-Lecture.html#the-number-e-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Number \\(e\\)",
    "text": "The Number \\(e\\)\n\nPlotCode\n\n\n\n\n\n\nWe can compute the value of \\(\\left ( 1 + \\frac{1}{m} \\right)^m\\) as m increases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number that this limit approaches is called \\(e\\).\n\n\\[\nP = \\lim_{\\Delta m \\to \\infty}A \\left ( 1 + \\frac{1}{m} \\right)^{m(rt)} =\nA e^{rt}\n\\]\n\n\n\n\n\nf &lt;- function(m) (1 + 1/m)^m\nx &lt;- 0:100\ny &lt;- f(x)\np &lt;- ggplot(data.frame(x, y), aes(x, y))\np + geom_line(size = 0.2) +\n  geom_hline(yintercept = exp(1), color = 'red', size = 0.2)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#properties-of-exponents",
    "href": "stats-math/02-lecture/02-Lecture.html#properties-of-exponents",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Properties of Exponents",
    "text": "Properties of Exponents\n\n\n\\(a^x a^y = a^{x + y}\\)\n\n\n2^7 * 2^8 == 2^(7 + 8) \n\n[1] TRUE\n\n\n\n\n\n\\(\\frac{a^x}{a^y} = a^{x-y}\\)\n\n\n2^7 / 2^8 == 2^(7 - 8) \n\n[1] TRUE\n\n\n\n\n\n\\((a^x)^y = a^{xy}\\)\n\n\n(2^7)^8 == 2^(7*8)\n\n[1] TRUE\n\n\n\n\n\n\\((ab)^x = a^x b^x\\)\n\n\n(2*3)^7 == 2^7 * 3^7\n\n[1] TRUE\n\n\n\n\n\n\\(\\frac{a^x}{b^x} = \\left( \\frac{a}{b} \\right)^x\\)\n\n\n2^4 / 3^4 == (2/3)^4\n\n[1] FALSE\n\nall.equal(2^4 / 3^4, (2/3)^4)\n\n[1] TRUE"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-2",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\nSuppose $750 is invested in an account at an annual interest rate of 5.5%, compounded continuously. Let \\(t\\) denote the number of years after the initial investment and \\(A(t)\\) denote the amount of money in the account at time \\(t\\). Find a formula for \\(A(t)\\). Find the amount of money in the account after 5 years, 10 years, and 50 years."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#logarithmic-functions",
    "href": "stats-math/02-lecture/02-Lecture.html#logarithmic-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Logarithmic functions",
    "text": "Logarithmic functions\n\nIntroductionExample: Log-Sum-Exp\n\n\n\n\nExponential function of the form \\(f(x) = a^x\\) is one-to-one, so it has an inverse called a logarithmic function.\nIn Statistics, when we write \\(\\log\\) we mean natural log, base \\(e\\). This makes it easier to interpret the log regression coefficients as percentage changes. For example, \\(\\log(1.04) \\approx 4\\%\\).\nThe following relationship always holds: \\(a^t = e^{\\log(a)t}\\) since \\(e^{\\log(a)} = a\\)\n\\(\\log_a(uv) = \\log_a(u) + \\log_a(v)\\)\n\\(\\log_a(u/v) = \\log_a(u) - \\log_a(v)\\)\n\\(\\log_a u^n = n \\log_a u\\)\n\n\n\n\nIn statistics and Machine Learning, we often want to normalize a vector so that it adds to one. One such function is called a softmax:\n\n\\[\n\\text{softmax}(x)  = \\frac{\\exp(x)} {\\sum_{n=1}^N \\exp(x_n)}\n\\]\n\nIf \\(x\\) is a vector of size 3, the numerator is component-wise \\(\\exp\\) of size 3, the denominator is a scalar, and the function value is a vector of size 3 that adds to 1\n\n\n\n\ny &lt;- 1:3\n(numer &lt;- exp(y))\n\n[1]  2.718282  7.389056 20.085537\n\n(denom &lt;- sum(exp(y)))\n\n[1] 30.19287\n\n(softmax &lt;- numer / denom)\n\n[1] 0.09003057 0.24472847 0.66524096\n\ny &lt;- c(1e3, 1e3 + 1, 1e3 + 2)\nexp(y)\n\n[1] Inf Inf Inf\n\n\n\nThis produces overflow – the numbers are too big for the computer. Let’s compute the \\(\\log \\text{softmax}(x)\\).\n\n\n\n\\[\n    \\log\\text{softmax}(x) = x - \\log \\sum_{n=1}^N \\exp(x_n)\n\\]\n\nThe second term is the log-sum-exp or LSE for short. It has the same problem, as it will overflow or underflow.\nThe idea is that we need to reduce the magnitude of \\(x_n\\) while preserving the integrity of the LSE function.\n\n\n\n\\[\n    \\begin{eqnarray}\n    \\text{Let } c & = & \\max(x_1, x_2, x_3, ..., x_N) \\\\\n    y & = & \\log \\sum_{n=1}^N \\exp(x_n) \\\\\n    \\exp(y) & = & \\sum_{n=1}^N \\exp(x_n) \\\\\n    \\exp(y) & = & \\exp(c) \\sum_{n=1}^N \\exp(x_n - c) \\\\\n    y & = & c + \\log \\sum_{n=1}^N \\exp(x_n - c)\n    \\end{eqnarray}\n\\]\n\nBecause \\(c = \\max(x)\\), the largest exponent is zero.\n\n\n\n\nYour Turn: write a function called LSE that takes in vector \\(x\\) and implements the last equation and tests it on small and large values of \\(x\\). Now implement the softmax_log function using the LSE function. Now compute the same version of the softmax function and check that it sums to 1.\n\n\n\n\nLSE &lt;- function(x) {\n  c &lt;- max(x)\n  y &lt;- c + log(sum(exp(x - c)))\n  return(y)\n}\nsoftmax_log &lt;- function(x) {\n  x - LSE(x)\n}\nsoftmax_unsafe &lt;- function(x) exp(x) / sum(exp(x))\n\nsoftmax_unsafe(1:3)\n\n[1] 0.09003057 0.24472847 0.66524096\n\nsoftmax_log(1:3)\n\n[1] -2.407606 -1.407606 -0.407606\n\nexp(softmax_log(1:3))\n\n[1] 0.09003057 0.24472847 0.66524096\n\ny\n\n[1] 1000 1001 1002\n\nsoftmax_unsafe(y)\n\n[1] NaN NaN NaN\n\nexp(softmax_log(y))\n\n[1] 0.09003057 0.24472847 0.66524096\n\nsum(exp(softmax_log(y)))\n\n[1] 1"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#limits",
    "href": "stats-math/02-lecture/02-Lecture.html#limits",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Limits",
    "text": "Limits\nThe idea of the limit is to evaluate what the function approaches as we increase or decrease the inputs.\n\nThis demo shows what happens to the secant line as it approaches the tangent.\n\n\nSource: Calculus Volume 1"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-motion-in-a-straight-line",
    "href": "stats-math/02-lecture/02-Lecture.html#example-motion-in-a-straight-line",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Motion in a Straight Line",
    "text": "Example: Motion in a Straight Line\nSuppose you have an experiment where you can measure the position of a car moving in a straight line every \\(1/10\\) of a second for 5 seconds. When you look at the graph of time versus position, it looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\nYou guess that the function is quadratic in \\(t\\). If \\(x\\) is measured in meters, and \\(t\\) is in seconds, then \\(a\\) must have units of \\(m\\) and \\(b\\) must have units \\(m/s^2\\).\n\\[\nx(t) = a + bt^2\n\\]\n\n\nThe statistical inference problem is to find plausible values of \\(a\\) and \\(b\\), given our noisy measurements and the assumption that the position function is quadratic. We will come back to how to do it later, but for now, assume that the most likely values were found to be \\(a = 2\\) and \\(b = 3\\).\n\n\n\n\n\n\n\n\n\n\n\nWe can now ask, what was the average velocity between \\(1\\) and \\(4\\) seconds? This is the same as the slope of the secant line:\n\\[\n\\bar{v} =\n\\frac{\\text{displacment} (m)}{\\text{elapsed time} (s)} =\n\\frac{\\Delta x}{\\Delta t} =  \n\\frac{x(4) - x(1)}{4-1} = \\frac{50-5}{3} = 15 \\text{ m/s} = 54 \\text{ km/h}  \n\\]\n\n\nSince we know that this is a line of the form \\(x(t) = a + 15t\\), that goes through the point \\((t_1, x_1) = (1, 5)\\), \\(5 = a + 15\\cdot1\\) or \\(a = -10\\). The equation of the secant line is, therefore:\n\\[\nx(t) = -10 + 15t\n\\]\n\n\n\np &lt;- ggplot(data.frame(t, x), aes(t, x))\np + geom_line(size = 0.2) +\n  geom_abline(slope = 15, intercept = -10, size = 0.2, color = 'red') +\n  geom_point(x = 1, y = 5, color = 'blue') +\n  geom_point(x = 4, y = 50, color = 'blue') +\n  xlab(\"time t (s)\") + ylab(\"position x (m)\") +\n  annotate(\"text\", 3.7, 55, label = \"(4, 50)\") +\n  annotate(\"text\", 1, 12, label = \"(1, 5)\") +\n  ggtitle(\"Car's position function and the secant line\")\n\n\n\n\n\n\n\n\n\n\nWhat if we wanted to know the speedometer reading at \\(4\\) seconds? In other words, we want to know the speed at that instant. This is where the limit comes in.\n\\[\nv = \\lim_{\\Delta t \\to 0} \\frac{\\Delta x}{\\Delta t} = \\frac{dx}{dt}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\n\nHow do we compute this limit, which we wrote as \\(dx/dt\\)?\n\n\n\n\n\n\n\n\n\nWe re-write the secant or average velocity equation in the following way:\n\n\\[\n\\begin{eqnarray}\n\\bar v & = & \\frac{x(a + h) - x(a)}{a + h - a} = \\frac{x(a + h) - x(a)}{h} \\\\\nv & = & \\lim_{\\Delta h \\to 0} \\frac{x(a + h) - x(a)}{h} \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative-1",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\n\nOur original problem was to find the velocity at \\(t = 4\\) given our position function \\(x(t) = 2 + 3t^2\\).\n\n\\[\n\\begin{eqnarray}\nv & = & \\lim_{\\Delta h \\to 0} \\frac{x(a + h) - x(a)}{h} =\n\\lim_{\\Delta h \\to 0} \\frac{2 + 3(t + h)^2 - 2 - 3t^2}{h} = \\\\\n& \\lim_{\\Delta h \\to 0} & \\frac{6ht + 3h^2}{h} = \\lim_{\\Delta h \\to 0} (6t + 3h) = 6t\n\\end{eqnarray}\n\\]\n\n\n\nAt \\(t = 4\\), the speedometer was reading \\(6 \\cdot 4 = 24 \\text{ m/s } = 86.4 \\text{ km/h}\\).\nThe velocity is the derivative of position with respect to time, and we can write:\n\n\\[\nv(t) = \\frac{dx}{dt} = 6t\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative-2",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\nBut what about acceleration?\n\n\n\nAcceleration is a change in velocity with respect to time or the second derivative of position:\n\n\\[\na(t) = \\frac{dv}{dt} = \\frac{d}{dt} \\left( \\frac{d x}{d t} \\right) = \\frac{d^2 x}{d t^2}\n\\]\n\n\n\nTo compute acceleration at \\(t = 4\\), we take the limit again, only this time with respect to the velocity function:\n\n\\[\na = \\frac{dv}{dt} = \\lim_{\\Delta h \\to 0} \\frac{6(t + h) - 6t}{h} = \\\\\n\\lim_{\\Delta h \\to 0} \\frac{6t + 6h - 6t}{h} = \\lim_{\\Delta h \\to 0} 6 = 6\n\\]\n\n\n\nThis function \\(a(t) = 6\\) does not depend on \\(t\\), and so acceleration is a constant \\(6 \\text{ m}/\\text{s}^2\\) at all values of \\(t\\) including \\(t = 4\\)\n\n\n\n\nIt would be cumbersome to compute derivatives this way. Fortunately, we have shortcuts."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#rules-of-differentiation",
    "href": "stats-math/02-lecture/02-Lecture.html#rules-of-differentiation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Rules of Differentiation",
    "text": "Rules of Differentiation\n\nWe will state a few rules of differentiation without deriving them\n\n\\[\n\\begin{eqnarray}\n\\frac{d}{dx}(c) & = & 0 \\\\\n\\frac{d}{dx}(x) & = & 1 \\\\\n\\frac{d}{dx}(x^n) & = & n x^{n-1} \\\\\n\\frac{d}{dx}[c f(x)] & = & c \\frac{d}{dx}[f(x)] \\\\\n\\frac{d}{dx}[f(x) + g(x)] & = & \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x) \\\\\n\\frac{d}{dx}(e^x) & = & e^x \\\\                                                            \\end{eqnarray}                                                               \n\\]\n\n\nIf you are interested in why the last statement is true, see this.\nThe derivatives of trigonometric functions can be found here."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-3",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-3",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nSuppose we have the following equations of motion:\n\n\\[\nx(t) = 4 + 3t + 5t^2\n\\]\n\nWhat is the car’s position at time zero and time 1 second?\nWhat is the velocity at time zero?\nWhat is the velocity and acceleration at 5 seconds?"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#product-rule-and-chain-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#product-rule-and-chain-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Product Rule and Chain Rule",
    "text": "Product Rule and Chain Rule\n\nProduct and chain rules help us differentiate products and compositions of functions, respectively.\nIntuition for those can be found in the Essense of Calculus video.\nWe will state them here without derivation. Product rule: Right d left, Left d right.\n\n\\[\n\\frac{d}{dx}[f(x) g(x)] = f(x) \\frac{d}{dx}[g(x)] + g(x)\\frac{d}{dx}[f(x)]\n\\]\n\nIf \\(F = f \\circ g\\), in that \\(F(x)= f(g(x))\\):\n\n\\[\nF'(x) = f'(g(x))g'(x)\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-of-a-product-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#example-of-a-product-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example of a Product Rule",
    "text": "Example of a Product Rule\n\nSuppose we wanted to compute the derivative of \\(x(t) = \\sin(t) \\cos(t)\\)\n\n\\[\n\\begin{eqnarray}\n\\frac{d}{dt}(\\sin(t) \\cos(t)) & = & \\\\\n\\sin(t)\\frac{d}{dt}[\\cos(t)] + \\cos(t)\\frac{d}{dt}[\\sin(t)] & = & \\\\\n\\sin(t) (-\\sin(t)) + \\cos(t)\\cos(t) & = & \\\\\n\\cos^2(t) - \\sin^2(t)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#approximating-derivatives",
    "href": "stats-math/02-lecture/02-Lecture.html#approximating-derivatives",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Approximating Derivatives",
    "text": "Approximating Derivatives\n\nlibrary(ggplot2)\nlibrary(dplyr)\nn &lt;- 100\nt &lt;- seq(0, pi, length = n)\nx &lt;- sin(t) * cos(t)\ndxdt &lt;- cos(t)^2 - sin(t)^2\n\nd &lt;- tibble(t, x, dxdt)\np &lt;- ggplot(d, aes(t, x))\np + geom_line() + geom_line(aes(y = dxdt), col = 'red')\n\n\n\n\n\n\n\n# approximate the derivative\ns &lt;- pi / (n - 1)        # choose the same step s as the increment in the t sequence\nall.equal(s, diff(t)[1]) # check that the above statement is true\n\n[1] TRUE\n\nappr_dxdt &lt;- diff(x)/s   # derivative ≈ rise / run\nd &lt;- d %&gt;%\n  mutate(appr_dxdt = c(NA, appr_dxdt))\np &lt;- ggplot(d, aes(t, x))\np + geom_line() + geom_line(aes(y = dxdt), col = 'red') +\n  geom_line(aes(y = appr_dxdt), col = 'blue', size = 5, alpha = 1/5)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#examples-of-the-chain-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#examples-of-the-chain-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Examples of the Chain Rule",
    "text": "Examples of the Chain Rule\n\nWhat is the derivative of \\(\\sin(x^2)\\)? It is \\(2x \\cos(x^2)\\)\nYour turn: \\[\n\\frac{d}{dx} \\left( e^{\\cos(x) x^2} \\right ) =\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#homework",
    "href": "stats-math/02-lecture/02-Lecture.html#homework",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Homework",
    "text": "Homework\n\nA particle is moving in a counter-clockwise uniform circular motion according to the following equations:\n\n\\[\nx(t) = R \\cos(\\omega t) \\\\\ny(t) = R \\sin(\\omega t)\n\\] - Where \\(\\omega\\) is called angular frequency (radians per unit of time) and \\(R\\) is the radius of rotation. The full cycle is achieved when \\(T = 2 \\pi/ \\omega\\)\n\nCreate two functions that take \\(R\\), \\(t\\), and \\(\\omega\\) as parameters\nCreate a vector \\(t\\) of length 100 that achieves one full rotation\nPlot the \\(x\\) position against \\(t\\) and the \\(y\\) position against \\(t\\) on the same graph\nNow plot \\(x\\) against \\(y\\). Do the plots make sense?\nCreate 4 more functions: 2 for velocity (in x and y direction) and 2 for acceleration\nPlot the velocity against the position (say in the x direction) and the velocity against acceleration\nWhat have you learned?\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-objectives",
    "href": "stats-math/01-lecture/01-Lecture.html#course-objectives",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Objectives",
    "text": "Course Objectives\n\n\nThis course will help you to prepare for the A3SR MS Program by covering the minimal necessary foundation in computing, math, and probability.\nAfter completing the course, you will be able to write simple R programs and perform simulations, plot and manipulate data, solve basic probability problems, and understand the concept of regression\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-format",
    "href": "stats-math/01-lecture/01-Lecture.html#course-format",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Format",
    "text": "Course Format\n\n\nThe course will run for two weeks, five days per week, 3 hours per day\nEach day will consist of:\n\n~30-minute going over the homework questions\n~1.5-hour lecture\n~1-hour hands of exercises\n\nThe course will be computationally intensive – we will write many small programs."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-outline",
    "href": "stats-math/01-lecture/01-Lecture.html#course-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Outline",
    "text": "Course Outline\n\n\n\n\nIntroduction to the R language, RStudio, and R Markdown.\nBasic differentiation. Meaning of the derivative. Numeric and symbolic differentiation and optimization.\nBasic integration. Riemann integral and basic rules of integration.\nReview of one-dimensional probability. Conditional probability, random variables, and expectations. Solving probability problems by simulation.\n\n\n\n\n\nDiscrete distributions like Bernoulli and Binomial and continuous distributions like Normal and Exponential\nIntroduction to Matrix Algebra. Vectors and vector arithmetic. Matrixes and matrix operations.\nManipulating and graphing data and Exploratory Data Analysis\nProgramming basics: variables, flow control, loops, functions, and writing simulations\nIntroduction to basic statistics and linear regression."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#references",
    "href": "stats-math/01-lecture/01-Lecture.html#references",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\nHands-On Programming with R, Grolemund (2014)\nR for Data Science 2e, Wickham, Çetinkaya-Rundel, and Grolemund (2023)\nCalculus Made Easy, Thompson (1980)\nCalculus, Herman, Strang, and OpenStax (2016)\nYouTube: Essence of Calculus, Sanderson (2018a)\nOptional: YouTube: Essense of Linear Algebra, Sanderson (2018b)\nOptional: Introduction to Linear Algebra, Boyd and Vandenberghe (2018)\nOptional: Matrix Cookbook, Petersen and Pedersen (2012)\nIntoduction to Probability, Blitzstein and Hwang (2019)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#where-are-you-from",
    "href": "stats-math/01-lecture/01-Lecture.html#where-are-you-from",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Where are you from?",
    "text": "Where are you from?"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#session-1-outline",
    "href": "stats-math/01-lecture/01-Lecture.html#session-1-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 1 Outline",
    "text": "Session 1 Outline\n\n\nThe big picture – costs and benefits\nSetting up an analysis environment\nRStudio projects\nWorking with interpreted languages like R\nSome basic R syntax and elements of R style\nGenerating data with R\nSome basic R and ggplot graphics\nWriting your first Monte Carlo simulation"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-silly",
    "href": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-silly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Silly",
    "text": "Some Mistakes Are Silly"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-deadly",
    "href": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-deadly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Deadly",
    "text": "Some Mistakes Are Deadly\nOn January 28, 1986, shortly after launch, Shuttle Challenger exploded, killing all seven crew members.\n\nO-Ring DataBinomial ModelReferences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability of 1 or more rings being damaged at launch is about 0.99\nProbability of all 6 rings being damaged at launch is about 0.46\n\n\n\n\n\nFowlkes and Hoadley (1989): Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure\nMartz and Zimmer (1992): The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.\n\n\n\n\n\nData source: UCI Machine Learning Repository"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#allan-mcdonald-dies-at-83",
    "href": "stats-math/01-lecture/01-Lecture.html#allan-mcdonald-dies-at-83",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Allan McDonald Dies at 83",
    "text": "Allan McDonald Dies at 83\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication is part of the job — it’s worth learning how to do it well.\n\n\nSource: The New York Times, March 9, 2021"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#smac-why-bother",
    "href": "stats-math/01-lecture/01-Lecture.html#smac-why-bother",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "SMaC: Why Bother?",
    "text": "SMaC: Why Bother?\n\n\n\n\nProgramming: a cheap way to do experiments and a lazy way to do math\nDifferential calculus: optimize functions, compute MLEs\nIntegral calculus: compute probabilities, expectations\nLinear algebra: solve many equations at once\nProbability: the language of statistics\nStatistics: quantify uncertainty"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-differential-calculus",
    "href": "stats-math/01-lecture/01-Lecture.html#example-differential-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Differential Calculus",
    "text": "Example: Differential Calculus\n\nRegression Line ViewLikelihood View\n\n\n\n\n\n\nDifferentiation comes up when you want to find the most likely values of parameters (unknowns) in optimization-based (sometimes called frequentist) inference\nImagine that we need to find the values of the slope and intercept such that the yellow line fits “nicely” through the cloud of points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have linear regression model of the form \\(y_n \\sim \\operatorname{normal}(\\alpha + \\beta x_n, \\, \\sigma)\\) and you want to learn the most likely values of \\(\\alpha\\) and \\(\\beta\\)\nThe most likely values for slope (beta) and intercept (alpha) are at the peak of this function, which can be found by using (partial) derivatives"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-integral-calculus",
    "href": "stats-math/01-lecture/01-Lecture.html#example-integral-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Integral Calculus",
    "text": "Example: Integral Calculus\n\n\n\n\nSuppose we have a probability distribution of some parameter \\(\\theta\\), which represents the differences between the treated and control units\nFurther, suppose that \\(\\theta &gt; 0\\) favors the treatment group\nWe want to know the probability that treatment is better than control\nThis probability can be written as:\n\n\n\n\\[\n    \\P(\\theta &gt; 0) = \\int_{0}^{\\infty} p_{\\theta}\\, \\text{d}\\theta\n\\]\n\n\n\n\n\n\nAssuming \\(\\theta\\) is normally distributed with \\(\\mu = 1\\) and \\(\\sigma = 1\\) we can evaluate the integral as an area under the normal curve from \\(0\\) to \\(\\infty\\)."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-linear-regression",
    "href": "stats-math/01-lecture/01-Lecture.html#example-linear-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Linear Regression",
    "text": "Example: Linear Regression\nNotice a Linear Algebra notation \\(X \\beta\\), which is matrix-vector multiplication. \n\ndata {\n  int&lt;lower=0&gt; N;   // number of data items\n  int&lt;lower=0&gt; K;   // number of predictors\n  matrix[N, K] X;   // predictor matrix\n  vector[N] y;      // outcome vector\n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real&lt;lower=0&gt; sigma;  // error scale\n}\nmodel {\n  y ~ normal(X * beta + alpha, sigma);  // likelihood\n}\n\n\nLinear Regression in Stan. Source: Stan Manual\n\n\n\nStan is a probabilistic programming language. You write your data-generating process (model), and Stan performs inference for all the unknowns."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-bionomial-regression",
    "href": "stats-math/01-lecture/01-Lecture.html#example-bionomial-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Bionomial Regression",
    "text": "Example: Bionomial Regression\nThis is the type of model we fit to the O-Rings data.\n\n\ndata {\n  int&lt;lower=0&gt; N_rows; // number of rows in data\n  int&lt;lower=0&gt; N;      // number of possible \"successes\" in Binom(N, p)\n  vector[N_rows] x;    // temperature for the O-Rings example\n  array[N_rows] int&lt;lower=0, upper=N&gt; y; // number of \"successes\" in y ~ Binom(N, p)\n}\nparameters {\n  real alpha; \n  real beta;  \n}\nmodel {\n  alpha ~ normal(0, 2.5); // we can encode what we know about plausible values\n  beta ~ normal(0, 1);    // of alpha and beta prior to conditioning on the date\n  y ~ binomial_logit(N, alpha + beta * x); // likehood (conditioned on x)\n}\n\n\\[\n\\begin{eqnarray*}\n\\text{BinomialLogit}(y~|~N,\\theta) & = &\n\\text{Binomial}(y~|~N,\\text{logit}^{-1}(\\theta)) \\\\[6pt] & = &\n\\binom{N}{y} \\left( \\text{logit}^{-1}(\\theta) \\right)^{y}  \\left( 1 -\n\\text{logit}^{-1}(\\theta) \\right)^{N - y}  \n\\end{eqnarray*}\n\\]\n\nSource: Stan Manual"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-irt-model",
    "href": "stats-math/01-lecture/01-Lecture.html#example-irt-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: IRT Model",
    "text": "Example: IRT Model\nItem-Response Theory models are popular in education research but generalize to other applications. \n\ndata {\n  int&lt;lower=1&gt; J;                     // number of students\n  int&lt;lower=1&gt; K;                     // number of questions\n  int&lt;lower=1&gt; N;                     // number of observations\n  array[N] int&lt;lower=1, upper=J&gt; jj;  // student for observation n\n  array[N] int&lt;lower=1, upper=K&gt; kk;  // question for observation n\n  array[N] int&lt;lower=0, upper=1&gt; y;   // correctness for observation n\n}\nparameters {\n  real delta;            // mean student ability\n  array[J] real alpha;   // ability of student j - mean ability\n  array[K] real beta;    // difficulty of question k\n}\nmodel {\n  alpha ~ std_normal();         // informative true prior\n  beta ~ std_normal();          // informative true prior\n  delta ~ normal(0.75, 1);      // informative true prior\n  for (n in 1:N) {\n    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);\n  }\n}\n\n\n1PL item-response model. Source: Stan Manual"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#analysis-environment",
    "href": "stats-math/01-lecture/01-Lecture.html#analysis-environment",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Environment",
    "text": "Analysis Environment\n\n\n\n\nInstructions for installing R and RStudio\nInstall the latest version of R\nInstall the latest version of the RStudio Desktop\nCreate a directory on your hard drive and give it a simple name. Mine is called statsmath\nIn RStudio, go to File -&gt; New Project and select: “Existing Directory”\nHow many of you have not used RStudio?\n\n\n\n\n\n\nDownload R, Download RStudio"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#what-is-r",
    "href": "stats-math/01-lecture/01-Lecture.html#what-is-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What is R",
    "text": "What is R\n\n\n\n\nR is an open-source, interpreted, (somewhat) functional programming language\nR is an implementation of the S language developed at Bell Labs around 1976\nRoss Ihaka and Robert Gentleman started working on R in the early 1990s\nVersion 1.0 was released in 2000\nThere are approximately 20,000 R packages available in CRAN\nR has a large and generally friendly user community"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-very-basics",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-very-basics",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Very Basics",
    "text": "Hands-On: Very Basics\n\n\n\n2 + 3\n\n[1] 5\n\n10^3\n\n[1] 1000\n\n10 - 5 / 2\n\n[1] 7.5\n\n(10 - 5) / 2\n\n[1] 2.5\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n1:10 + 1\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\nsin(pi/2) + cos(pi/2)\n\n[1] 1\n\n1i\n\n[1] 0+1i\n\nround(exp(1i * pi) + 1)\n\n[1] 0+0i\n\n(exp(1i * pi) + 1) |&gt; round()\n\n[1] 0+0i\n\n\n\n\n\n\ndie &lt;- 1:6\ndie &lt;- seq(1, 6, by = 1)\ndie\n\n[1] 1 2 3 4 5 6\n\nlength(die) \n\n[1] 6\n\nstr(die)     \n\n num [1:6] 1 2 3 4 5 6\n\nsum(die)\n\n[1] 21\n\nprod(die)\n\n[1] 720\n\nmean(die)\n\n[1] 3.5\n\nsum(die) / length(die)\n\n[1] 3.5\n\nmedian(die)\n\n[1] 3.5\n\nsd(die)\n\n[1] 1.870829"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#r-documentation",
    "href": "stats-math/01-lecture/01-Lecture.html#r-documentation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "R Documentation",
    "text": "R Documentation\n\n?mean # same as help(mean)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#your-turn",
    "href": "stats-math/01-lecture/01-Lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nCreate a variable called fib that contains the first 10 Fibonacci numbers\nThey are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34\nCompute the length of this vector in R\nCompute the sum, the product, and the difference (diff()) between successive numbers\nWhat do you notice about the pattern in the differences?\nNow, create a vector of 100 integers from 1 to 100\nYoung Gauss was asked to sum them by hand\nHe figured out that the sum has to be \\(N (N + 1) / 2\\)\nVerify that Gauss was right (just for 100)\nNow compute the sum of the first hundred squares"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-lists",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-lists",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Lists",
    "text": "Hands-On: Lists\n\n\nLists are collections of objects of different types and shapes\nContrast with a data frame, which we will discuss later, that contains objects of different types but is rectangular\n\n\n\n\nlist_x &lt;- list(A = pi, B = c(0, 1), C = 1:10, D = c(\"one\", \"two\"))\nlist_x\n\n$A\n[1] 3.141593\n\n$B\n[1] 0 1\n\n$C\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$D\n[1] \"one\" \"two\"\n\nstr(list_x)\n\nList of 4\n $ A: num 3.14\n $ B: num [1:2] 0 1\n $ C: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ D: chr [1:2] \"one\" \"two\"\n\nlist_x$A\n\n[1] 3.141593\n\nlist_x[1]\n\n$A\n[1] 3.141593\n\nstr(list_x[1])\n\nList of 1\n $ A: num 3.14\n\nlist_x[[1]] # same as x$A\n\n[1] 3.141593"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#data-frames",
    "href": "stats-math/01-lecture/01-Lecture.html#data-frames",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\nData frames are rectangular structures that are often used in data analysis\nThere is a built-in function called data.frame, but we recommend tibble, which is part of the dplyr package\nYou can look up the documentation of any R function this way: ?dplyr::tibble. If the package is loaded by using library(dplyr) you can omit dplyr:: prefix\nJohn F. W. Herschel’s data on the orbit of the Twin Stars \\(\\gamma\\) Virginis"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#data-frames-1",
    "href": "stats-math/01-lecture/01-Lecture.html#data-frames-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\nlibrary(HistData)\ndata(\"Virginis.interp\")\nclass(Virginis.interp)\n\n[1] \"data.frame\"\n\nVirginis.interp |&gt; dplyr::as_tibble()\n\n# A tibble: 14 × 4\n    year posangle distance velocity\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  1720    160      17.2    -0.32 \n 2  1730    157.     16.8    -0.354\n 3  1740    153      16.3    -0.376\n 4  1750    149.     15.5    -0.416\n 5  1760    144.     14.5    -0.478\n 6  1770    140.     13.7    -0.533\n 7  1780    134.     13.5    -0.547\n 8  1790    129.     12.9    -0.597\n 9  1800    122.     12.6    -0.632\n10  1810    116.     11.2    -0.8  \n11  1815    111.     10.4    -0.929\n12  1820    106.      9.57   -1.09 \n13  1825     98.3     7.09   -1.99 \n14  1830     84.3     4.9    -4.16 \n\nVirginis.interp |&gt; dplyr::as_tibble() |&gt; class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n Code for generated the above plot can be found here.\n\n\n\nThe Origin and Development of the Scatterplot by M Friendly and H Wainer"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#your-turn-1",
    "href": "stats-math/01-lecture/01-Lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nSave Virginis.interp in a new tibble called virginis using as_tibble() function\nCompute average velocity in virginis\nHint: you can access columns of tibbles and data frames with an $ like this: dataframe_name$variable_name\nWe will do a lot more work with tibbles and data frames in later sessions"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#basic-plotting",
    "href": "stats-math/01-lecture/01-Lecture.html#basic-plotting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Basic Plotting",
    "text": "Basic Plotting\n\nCodePlot Deaths Over TimePlot the Map\n\n\n\n\n\n\n\n\n\n\n\n\nR base plotting system is great for making quick basic graphs with relatively little typing\nWe will demonstrate with John Snow’s data from the 1854 cholera outbreak\n\n\n\n\nlibrary(HistData)\nlibrary(lubridate)\npar(mar = c(3, 3, 2, 1), mgp = c(2, .7, 0), tck = -.01)\nclr &lt;- ifelse(Snow.dates$date &lt; mdy(\"09/08/1854\"), \n              \"red\", \"darkgreen\")\nplot(deaths ~ date, data = Snow.dates, \n     type = \"h\", lwd = 2, col = clr, xlab = \"\")\npoints(deaths ~ date, data = Snow.dates, \n       cex = 0.5, pch = 16, col = clr)\ntext(mdy(\"09/08/1854\"), 40, \n     \"Pump handle\\nremoved Sept. 8\", pos = 4)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-simulating-coin-flips",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-simulating-coin-flips",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Simulating Coin Flips",
    "text": "Hands-On: Simulating Coin Flips\nWe can learn a lot through simulation. We will start with the sample() function.\nsample(x, size, replace = FALSE, prob = NULL)\n\nSampleFor LoopsSolution\n\n\n\n# simulating coin flips\ncoin &lt;- c(\"H\", \"T\")\n\n# flip a fair coin 10 times; try it with replace = FALSE\nsample(coin, 10, replace = TRUE)\n\n [1] \"H\" \"T\" \"T\" \"H\" \"T\" \"T\" \"H\" \"H\" \"H\" \"T\"\n\n# flip a coin that has P(H = 0.6) 10 times\nsample(coin, 10, replace = TRUE, prob = c(0.6, 0.4))\n\n [1] \"T\" \"H\" \"H\" \"T\" \"T\" \"H\" \"H\" \"T\" \"H\" \"H\"\n\n# it's more convenient to make H = 1 and T = 0\ncoin &lt;- c(1, 0)\nsample(coin, 10, replace = TRUE)\n\n [1] 1 0 1 1 1 0 0 1 1 1\n\n\n\n\nYour turn: flip a coin 1000 times and compute the proportion of heads\n\n\n\n\n\n\nSuppose you want to add the first 100 integers as before but without using the sum() function or the formula\nIn math notation: \\(\\sum_{i = 1}^{100}x_i, \\ x \\in \\{1, ..., 100\\}\\)\n\n\n\n\nn &lt;- 100\nx &lt;- 1:n\ns &lt;- 0\nfor (i in 1:n) {\n  s &lt;- s + x[i] \n}\n# test\ns == sum(x)\n\n[1] TRUE\n\n\n\n\n\nYour turn: modify the loop to add only even numbers in 1:100. Look up help(if) statement and modulo operator help(%%); write a test to check your work\n\n\n\n\n\n\ns &lt;- 0\nfor (i in 1:n) {\n  if (x[i] %% 2 == 0) {\n    s &lt;- s + x[i]\n  }\n}\ncat(s)\n\n2550\n\n# test \ns == sum(seq(2, 100, by = 2))\n\n[1] TRUE\n\n\n\n\n\nCongratulations, you are now Turing Complete!"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#simulate-coin-flips",
    "href": "stats-math/01-lecture/01-Lecture.html#simulate-coin-flips",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Simulate Coin Flips",
    "text": "Simulate Coin Flips\n\n\nIf we flip a coin more and more times, would the estimate of the proportion become better?\nIf so, what is the rate of convergence?\n\n\n\n\nset.seed(1) # why do we do this?\nn &lt;- 1e4\nest_prop &lt;- numeric(n) # allocate a vector of size n\nfor (i in 1:n) {\n  x &lt;- sample(coin, i, replace = TRUE)\n  est_prop[i] &lt;- mean(x)\n}\n\n\n\n\nYour turn: write down in plain English what the above code is doing\nAt the end of the loop, what does the variable est_prop contain?"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#introduction-to-ggplot",
    "href": "stats-math/01-lecture/01-Lecture.html#introduction-to-ggplot",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to ggplot",
    "text": "Introduction to ggplot\n\n\nOur task is to visualize the estimated proportion as a function of the number of coin flips\nThis can be done in base plot(), but we will do it with ggplot\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nx &lt;- seq(0, 100, by = 5)\ny &lt;- x^2\n\nquadratic &lt;- tibble(x = x, y = y)\np1 &lt;- ggplot(data = quadratic, \n             mapping = aes(x = x, y = y))\np2 &lt;- p1 + geom_point(size = 0.5)\np3 &lt;- p1 + geom_line(linewidth = 0.2, \n                     color = 'red')\np4 &lt;- p1 + geom_point(size = 0.5) + \n  geom_line(linewidth = 0.2, color = 'red')\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#law-of-large-numbers",
    "href": "stats-math/01-lecture/01-Lecture.html#law-of-large-numbers",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\nCodePlot\n\n\n\nset.seed(1) \nn &lt;- 1e4\nest_prop &lt;- numeric(n) \nfor (i in 1:n) {\n  x &lt;- sample(coin, i, replace = TRUE)\n  est_prop[i] &lt;- mean(x)\n}\n\nlibrary(scales)\ndata &lt;- tibble(num_flips = 1:n, est_prop = est_prop)\np &lt;- ggplot(data = data, mapping = aes(x = num_flips, y = est_prop))\np + geom_line(size = 0.1) + \n  geom_hline(yintercept = 0.5, size = 0.2, color = 'red') +\n  scale_x_continuous(trans = 'log10', label = comma) +\n  xlab(\"Number of flips on Log10 scale\") +\n  ylab(\"Estimated proportion of Heads\") +\n  ggtitle(\"Error decreases with the size of the sample\")\n\n\n\nWe can see some evidence for the Law of Large Numbers."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#functions",
    "href": "stats-math/01-lecture/01-Lecture.html#functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Functions",
    "text": "Functions\n\nStructureWriting Your Own FunctionCoin Flip Example\n\n\n\n\nFunctions help you break up the code into self-contained, understandable pieces.\nFunctions take in arguments and return results. You saw functions like sum() and mean() before. Here, you will learn how to write your own.\n\n\n\n\n\n\n\n\n\n\nSource: Hands-On Programming with R\n\n\n\n\n\nWe will write a function that produces one estimate of the proportion given a fixed sample size n.\n\n\nestimate_proportion &lt;- function(n) {\n  coin &lt;- c(1, 0)\n  x &lt;- sample(coin, n, replace = TRUE)\n  est &lt;- mean(x)\n  return(est)\n}\nestimate_proportion(10)\n\n[1] 0.4\n\nx &lt;- replicate(1e3, estimate_proportion(10))\nhead(x)\n\n[1] 0.3 0.7 0.5 0.8 0.5 0.6\n\n\n\n\n\nhist(x, \n     xlab = \"Number of simulations\", \n     main = \"Distribution of Proportions of Heads\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo reproduce our earlier example, generating estimates for increasing sample sizes, use map_dbl() function from purrr package. More on that here.\n\n\n\nlibrary(purrr)\npar(mar = c(3, 3, 2, 1), mgp = c(2, .7, 0), tck = -.01, bg = \"#f0f1eb\")\ny &lt;- map_dbl(2:500, estimate_proportion)\n# above is the same as sapply(2:500, estimate_proportion)\nplot(2:500, y, xlab = \"\", ylab = \"\", type = 'l')"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#generating-continuous-uniform-draws",
    "href": "stats-math/01-lecture/01-Lecture.html#generating-continuous-uniform-draws",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Generating Continuous Uniform Draws",
    "text": "Generating Continuous Uniform Draws\n\n\nHere, we will examine a continuous version of the sample() function: runif(n, min = 0, max = 1)\nrunif generates realizations of a random variable uniformly distributed between min and max.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn: what is the approximate value of this line of code: mean(runif(1e3, min = -1, max = 0))? Guess before running it."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\nThe idea is that we can approximate the ratio of the area of an inscribed circle, \\(A_c\\), to the area of the square, \\(A_s\\), by uniformly “throwing darts” at the square with the side \\(2r\\) and counting how many darts land inside the circle versus inside the square.\n\n\\[\n\\begin{align}\nA_{c}& = \\pi r^2 \\\\\nA_{s}& = (2r)^2 = 4r^2 \\\\\n\\frac{A_{c}}{A_{s}}& = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\implies \\pi = \\frac{4A_{c}}{A_{s}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-1",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\nTo estimate \\(\\pi\\), we perform the following simulation:\n\n\\[\n\\begin{align}\nX& \\sim \\text{Uniform}(-1, 1) \\\\\nY& \\sim \\text{Uniform}(-1, 1) \\\\\n\\pi& \\approx \\frac{4 \\sum_{i=1}^{N} \\I(x_i^2 + y_i^2 &lt; 1)}{N}\n\\end{align}\n\\]\n\n\nThe numerator is a sum over an indicator function \\(\\I\\), which evaluates to \\(1\\) if the inequality holds and \\(0\\) otherwise."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-2",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\n\nCode and PlotEstimate\n\n\n\n\nn &lt;- 1e3\nx &lt;- runif(n, -1, 1); y &lt;- runif(n, -1, 1)\ninside &lt;- x^2 + y^2 &lt; 1\ndata &lt;- tibble(x, y, inside)\np &lt;- ggplot(aes(x = x, y = y), data = data)\np + geom_point(aes(color = inside)) + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\ncat(\"Estimated value of pi =\", 4*sum(inside) / n)\n\nEstimated value of pi = 3.112\n\n\n\n\n\n\n\n\nx\ny\ninside\n\n\n\n\n-0.8287282\n-0.2198830\nTRUE\n\n\n-0.9964402\n0.4200821\nFALSE\n\n\n-0.1473951\n-0.1864385\nTRUE\n\n\n0.6455366\n0.6547743\nTRUE\n\n\n0.7709646\n-0.9771841\nFALSE"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#references-2",
    "href": "stats-math/01-lecture/01-Lecture.html#references-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\n\n\nhttps://ericnovik.github.io/smac.html\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. Second edition. Boca Raton: crc Press/Taylor & Francis Group.\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge, UK ; New York, NY: Cambridge University Press.\n\n\nFowlkes, Edward B., and Bruce Hoadley. 1989. “Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure AU - Dalal, Siddhartha r.” Journal of the American Statistical Association 84 (408): 945–57. https://doi.org/10.1080/01621459.1989.10478858.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. First edition. Sebastopol, CA: O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHerman, Edwin, Gilbert Strang, and OpenStax. 2016. Calculus Volume 1. https://d3bxy9euw4e147.cloudfront.net/oscms-prodcms/media/documents/CalculusVolume1-OP.pdf.\n\n\nMartz, H. F., and W. J. Zimmer. 1992. “The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.” The American Statistician 46 (1): 42–47. https://doi.org/10.1080/00031305.1992.10475846.\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The Matrix Cookbook.” https://www.freetechbooks.com/the-matrix-cookbook-t435.html.\n\n\nSanderson, Grant. 2018a. “Essence of Calculus,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr.\n\n\n———. 2018b. “Essence of Linear Algebra,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab.\n\n\nThompson, Silvanus P. 1980. Calculus Made Easy: Being a Very-Simplest Introduction to Those Beautiful Methods of Reckoning Which Are Generally Called by the Terrifying Names of the Differential Calculus and the Integral Calculus. 3d ed. New York: St. Martin’s Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. O’Reilly Media."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#course-objectives",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#course-objectives",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Objectives",
    "text": "Course Objectives\n\n\nThis course will help you to prepare for the A3SR MS Program by covering the minimal necessary foundation in computing, math, and probability.\nAfter completing the course, you will be able to write simple R programs and perform simulations, plot and manipulate data, solve basic probability problems, and understand the concept of regression\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#course-format",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#course-format",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Format",
    "text": "Course Format\n\n\nThe course will run for two weeks, five days per week, 3 hours per day\nEach day will consist of:\n\n~30-minute going over the homework questions\n~1.5-hour lecture\n~1-hour hands of exercises\n\nThe course will be computationally intensive – we will write many small programs."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#course-outline",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#course-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Outline",
    "text": "Course Outline\n\n\n\n\nIntroduction to the R language, RStudio, and R Markdown.\nBasic differentiation. Meaning of the derivative. Numeric and symbolic differentiation and optimization.\nBasic integration. Riemann integral and basic rules of integration.\nReview of one-dimensional probability. Conditional probability, random variables, and expectations. Solving probability problems by simulation.\n\n\n\n\n\nDiscrete distributions like Bernoulli and Binomial and continuous distributions like Normal and Exponential\nIntroduction to Matrix Algebra. Vectors and vector arithmetic. Matrixes and matrix operations.\nManipulating and graphing data and Exploratory Data Analysis\nProgramming basics: variables, flow control, loops, functions, and writing simulations\nIntroduction to basic statistics and linear regression."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#references",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#references",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\nHands-On Programming with R, Grolemund (2014)\nR for Data Science 2e, Wickham, Çetinkaya-Rundel, and Grolemund (2023)\nCalculus Made Easy, Thompson (1980)\nCalculus, Herman, Strang, and OpenStax (2016)\nYouTube: Essence of Calculus, Sanderson (2018a)\nOptional: YouTube: Essense of Linear Algebra, Sanderson (2018b)\nOptional: Introduction to Linear Algebra, Boyd and Vandenberghe (2018)\nOptional: Matrix Cookbook, Petersen and Pedersen (2012)\nIntoduction to Probability, Blitzstein and Hwang (2019)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#where-are-you-from",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#where-are-you-from",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Where are you from?",
    "text": "Where are you from?"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#session-1-outline",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#session-1-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 1 Outline",
    "text": "Session 1 Outline\n\n\nThe big picture – costs and benefits\nSetting up an analysis environment\nRStudio projects\nWorking with interpreted languages like R\nSome basic R syntax and elements of R style\nGenerating data with R\nSome basic R and ggplot graphics\nWriting your first Monte Carlo simulation"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#some-mistakes-are-silly",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#some-mistakes-are-silly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Silly",
    "text": "Some Mistakes Are Silly"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#some-mistakes-are-deadly",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#some-mistakes-are-deadly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Deadly",
    "text": "Some Mistakes Are Deadly\nOn January 28, 1986, shortly after launch, Shuttle Challenger exploded, killing all seven crew members.\n\nO-Ring DataBinomial ModelReferences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability of 1 or more rings being damaged at launch is about 0.99\nProbability of all 6 rings being damaged at launch is about 0.46\n\n\n\n\n\nFowlkes and Hoadley (1989): Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure\nMartz and Zimmer (1992): The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.\n\n\n\n\n\nData source: UCI Machine Learning Repository"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#allan-mcdonald-dies-at-83",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#allan-mcdonald-dies-at-83",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Allan McDonald Dies at 83",
    "text": "Allan McDonald Dies at 83\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication is part of the job — it’s worth learning how to do it well.\n\n\nSource: The New York Times, March 9, 2021"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#smac-why-bother",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#smac-why-bother",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "SMaC: Why Bother?",
    "text": "SMaC: Why Bother?\n\n\n\n\nProgramming: a cheap way to do experiments and a lazy way to do math\nDifferential calculus: optimize functions, compute MLEs\nIntegral calculus: compute probabilities, expectations\nLinear algebra: solve many equations at once\nProbability: the language of statistics\nStatistics: quantify uncertainty"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#example-differential-calculus",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#example-differential-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Differential Calculus",
    "text": "Example: Differential Calculus\n\nRegression Line ViewLikelihood View\n\n\n\n\n\n\nDifferentiation comes up when you want to find the most likely values of parameters (unknowns) in optimization-based (sometimes called frequentist) inference\nImagine that we need to find the values of the slope and intercept such that the yellow line fits “nicely” through the cloud of points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have linear regression model of the form \\(y_n \\sim \\operatorname{normal}(\\alpha + \\beta x_n, \\, \\sigma)\\) and you want to learn the most likely values of \\(\\alpha\\) and \\(\\beta\\)\nThe most likely values for slope (beta) and intercept (alpha) are at the peak of this function, which can be found by using (partial) derivatives"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#example-integral-calculus",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#example-integral-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Integral Calculus",
    "text": "Example: Integral Calculus\n\n\n\n\nSuppose we have a probability distribution of some parameter \\(\\theta\\), which represents the differences between the treated and control units\nFurther, suppose that \\(\\theta &gt; 0\\) favors the treatment group\nWe want to know the probability that treatment is better than control\nThis probability can be written as:\n\n\n\n\\[\n    \\P(\\theta &gt; 0) = \\int_{0}^{\\infty} p_{\\theta}\\, \\text{d}\\theta\n\\]\n\n\n\n\n\n\nAssuming \\(\\theta\\) is normally distributed with \\(\\mu = 1\\) and \\(\\sigma = 1\\) we can evaluate the integral as an area under the normal curve from \\(0\\) to \\(\\infty\\)."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#example-linear-regression",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#example-linear-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Linear Regression",
    "text": "Example: Linear Regression\nNotice a Linear Algebra notation \\(X \\beta\\), which is matrix-vector multiplication. \n\ndata {\n  int&lt;lower=0&gt; N;   // number of data items\n  int&lt;lower=0&gt; K;   // number of predictors\n  matrix[N, K] X;   // predictor matrix\n  vector[N] y;      // outcome vector\n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real&lt;lower=0&gt; sigma;  // error scale\n}\nmodel {\n  y ~ normal(X * beta + alpha, sigma);  // likelihood\n}\n\n\nLinear Regression in Stan. Source: Stan Manual\n\n\n\nStan is a probabilistic programming language. You write your data-generating process (model), and Stan performs inference for all the unknowns."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#example-bionomial-regression",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#example-bionomial-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Bionomial Regression",
    "text": "Example: Bionomial Regression\nThis is the type of model we fit to the O-Rings data.\n\n\ndata {\n  int&lt;lower=0&gt; N_rows; // number of rows in data\n  int&lt;lower=0&gt; N;      // number of possible \"successes\" in Binom(N, p)\n  vector[N_rows] x;    // temperature for the O-Rings example\n  array[N_rows] int&lt;lower=0, upper=N&gt; y; // number of \"successes\" in y ~ Binom(N, p)\n}\nparameters {\n  real alpha; \n  real beta;  \n}\nmodel {\n  alpha ~ normal(0, 2.5); // we can encode what we know about plausible values\n  beta ~ normal(0, 1);    // of alpha and beta prior to conditioning on the date\n  y ~ binomial_logit(N, alpha + beta * x); // likehood (conditioned on x)\n}\n\n\\[\n\\begin{eqnarray*}\n\\text{BinomialLogit}(y~|~N,\\theta) & = &\n\\text{Binomial}(y~|~N,\\text{logit}^{-1}(\\theta)) \\\\[6pt] & = &\n\\binom{N}{y} \\left( \\text{logit}^{-1}(\\theta) \\right)^{y}  \\left( 1 -\n\\text{logit}^{-1}(\\theta) \\right)^{N - y}  \n\\end{eqnarray*}\n\\]\n\nSource: Stan Manual"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#example-irt-model",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#example-irt-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: IRT Model",
    "text": "Example: IRT Model\nItem-Response Theory models are popular in education research but generalize to other applications. \n\ndata {\n  int&lt;lower=1&gt; J;                     // number of students\n  int&lt;lower=1&gt; K;                     // number of questions\n  int&lt;lower=1&gt; N;                     // number of observations\n  array[N] int&lt;lower=1, upper=J&gt; jj;  // student for observation n\n  array[N] int&lt;lower=1, upper=K&gt; kk;  // question for observation n\n  array[N] int&lt;lower=0, upper=1&gt; y;   // correctness for observation n\n}\nparameters {\n  real delta;            // mean student ability\n  array[J] real alpha;   // ability of student j - mean ability\n  array[K] real beta;    // difficulty of question k\n}\nmodel {\n  alpha ~ std_normal();         // informative true prior\n  beta ~ std_normal();          // informative true prior\n  delta ~ normal(0.75, 1);      // informative true prior\n  for (n in 1:N) {\n    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);\n  }\n}\n\n\n1PL item-response model. Source: Stan Manual"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#analysis-environment",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#analysis-environment",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Environment",
    "text": "Analysis Environment\n\n\n\n\nInstructions for installing R and RStudio\nInstall the latest version of R\nInstall the latest version of the RStudio Desktop\nCreate a directory on your hard drive and give it a simple name. Mine is called statsmath\nIn RStudio, go to File -&gt; New Project and select: “Existing Directory”\nHow many of you have not used RStudio?\n\n\n\n\n\n\nDownload R, Download RStudio"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#what-is-r",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#what-is-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What is R",
    "text": "What is R\n\n\n\n\nR is an open-source, interpreted, (somewhat) functional programming language\nR is an implementation of the S language developed at Bell Labs around 1976\nRoss Ihaka and Robert Gentleman started working on R in the early 1990s\nVersion 1.0 was released in 2000\nThere are approximately 20,000 R packages available in CRAN\nR has a large and generally friendly user community"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#hands-on-very-basics",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#hands-on-very-basics",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Very Basics",
    "text": "Hands-On: Very Basics\n\n\n\n2 + 3\n\n[1] 5\n\n10^3\n\n[1] 1000\n\n10 - 5 / 2\n\n[1] 7.5\n\n(10 - 5) / 2\n\n[1] 2.5\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n1:10 + 1\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\nsin(pi/2) + cos(pi/2)\n\n[1] 1\n\n1i\n\n[1] 0+1i\n\nround(exp(1i * pi) + 1)\n\n[1] 0+0i\n\n(exp(1i * pi) + 1) |&gt; round()\n\n[1] 0+0i\n\n\n\n\n\n\ndie &lt;- 1:6\ndie &lt;- seq(1, 6, by = 1)\ndie\n\n[1] 1 2 3 4 5 6\n\nlength(die) \n\n[1] 6\n\nstr(die)     \n\n num [1:6] 1 2 3 4 5 6\n\nsum(die)\n\n[1] 21\n\nprod(die)\n\n[1] 720\n\nmean(die)\n\n[1] 3.5\n\nsum(die) / length(die)\n\n[1] 3.5\n\nmedian(die)\n\n[1] 3.5\n\nsd(die)\n\n[1] 1.870829"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#r-documentation",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#r-documentation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "R Documentation",
    "text": "R Documentation\n\n?mean # same as help(mean)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#your-turn",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nCreate a variable called fib that contains the first 10 Fibonacci numbers\nThey are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34\nCompute the length of this vector in R\nCompute the sum, the product, and the difference (diff()) between successive numbers\nWhat do you notice about the pattern in the differences?\nNow, create a vector of 100 integers from 1 to 100\nYoung Gauss was asked to sum them by hand\nHe figured out that the sum has to be \\(N (N + 1) / 2\\)\nVerify that Gauss was right (just for 100)\nNow compute the sum of the first hundred squares"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#hands-on-lists",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#hands-on-lists",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Lists",
    "text": "Hands-On: Lists\n\n\nLists are collections of objects of different types and shapes\nContrast with a data frame, which we will discuss later, that contains objects of different types but is rectangular\n\n\n\n\nlist_x &lt;- list(A = pi, B = c(0, 1), C = 1:10, D = c(\"one\", \"two\"))\nlist_x\n\n$A\n[1] 3.141593\n\n$B\n[1] 0 1\n\n$C\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$D\n[1] \"one\" \"two\"\n\nstr(list_x)\n\nList of 4\n $ A: num 3.14\n $ B: num [1:2] 0 1\n $ C: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ D: chr [1:2] \"one\" \"two\"\n\nlist_x$A\n\n[1] 3.141593\n\nlist_x[1]\n\n$A\n[1] 3.141593\n\nstr(list_x[1])\n\nList of 1\n $ A: num 3.14\n\nlist_x[[1]] # same as x$A\n\n[1] 3.141593"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#data-frames",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#data-frames",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\nData frames are rectangular structures that are often used in data analysis\nThere is a built-in function called data.frame, but we recommend tibble, which is part of the dplyr package\nYou can look up the documentation of any R function this way: ?dplyr::tibble. If the package is loaded by using library(dplyr) you can omit dplyr:: prefix\nJohn F. W. Herschel’s data on the orbit of the Twin Stars \\(\\gamma\\) Virginis"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#data-frames-1",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#data-frames-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\nlibrary(HistData)\ndata(\"Virginis.interp\")\nclass(Virginis.interp)\n\n[1] \"data.frame\"\n\nVirginis.interp |&gt; dplyr::as_tibble()\n\n# A tibble: 14 × 4\n    year posangle distance velocity\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  1720    160      17.2    -0.32 \n 2  1730    157.     16.8    -0.354\n 3  1740    153      16.3    -0.376\n 4  1750    149.     15.5    -0.416\n 5  1760    144.     14.5    -0.478\n 6  1770    140.     13.7    -0.533\n 7  1780    134.     13.5    -0.547\n 8  1790    129.     12.9    -0.597\n 9  1800    122.     12.6    -0.632\n10  1810    116.     11.2    -0.8  \n11  1815    111.     10.4    -0.929\n12  1820    106.      9.57   -1.09 \n13  1825     98.3     7.09   -1.99 \n14  1830     84.3     4.9    -4.16 \n\nVirginis.interp |&gt; dplyr::as_tibble() |&gt; class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n Code for generated the above plot can be found here.\n\n\n\nThe Origin and Development of the Scatterplot by M Friendly and H Wainer"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#your-turn-1",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nSave Virginis.interp in a new tibble called virginis using as_tibble() function\nCompute average velocity in virginis\nHint: you can access columns of tibbles and data frames with an $ like this: dataframe_name$variable_name\nWe will do a lot more work with tibbles and data frames in later sessions"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture-Session-1.html#references-2",
    "href": "stats-math/01-lecture/01-Lecture-Session-1.html#references-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\n\n\nhttps://ericnovik.github.io/smac.html\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. Second edition. Boca Raton: crc Press/Taylor & Francis Group.\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge, UK ; New York, NY: Cambridge University Press.\n\n\nFowlkes, Edward B., and Bruce Hoadley. 1989. “Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure AU - Dalal, Siddhartha r.” Journal of the American Statistical Association 84 (408): 945–57. https://doi.org/10.1080/01621459.1989.10478858.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. First edition. Sebastopol, CA: O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHerman, Edwin, Gilbert Strang, and OpenStax. 2016. Calculus Volume 1. https://d3bxy9euw4e147.cloudfront.net/oscms-prodcms/media/documents/CalculusVolume1-OP.pdf.\n\n\nMartz, H. F., and W. J. Zimmer. 1992. “The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.” The American Statistician 46 (1): 42–47. https://doi.org/10.1080/00031305.1992.10475846.\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The Matrix Cookbook.” https://www.freetechbooks.com/the-matrix-cookbook-t435.html.\n\n\nSanderson, Grant. 2018a. “Essence of Calculus,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr.\n\n\n———. 2018b. “Essence of Linear Algebra,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab.\n\n\nThompson, Silvanus P. 1980. Calculus Made Easy: Being a Very-Simplest Introduction to Those Beautiful Methods of Reckoning Which Are Generally Called by the Terrifying Names of the Differential Calculus and the Integral Calculus. 3d ed. New York: St. Martin’s Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. O’Reilly Media."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#session-3-outline",
    "href": "stats-math/03-lecture/03-lecture.html#session-3-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 3 Outline",
    "text": "Session 3 Outline\n\n\nTransforming data for plotting\nAntiderivative\nSome rules of integration\nEvaluating integrals numerically\nThe waiting time distribution\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#data-transformations",
    "href": "stats-math/03-lecture/03-lecture.html#data-transformations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data transformations",
    "text": "Data transformations\n\n\n\n\nWe saw that continuous compounding does not make such a big difference over time\nHow would the value vary by rate?\nWe investigate with a common simulate-pivot-plot pattern"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#map-function",
    "href": "stats-math/03-lecture/03-lecture.html#map-function",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Map Function",
    "text": "Map Function\n“The purrr::map* functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input.”\n\n\nGenerate 10 vectors of 100 uniform random realizations, where the first vector has min = 1, second min = 2 … last vector has min = 10, and max = 15 for all\nNow compute the average value of each of the 10 vectors\n\n\n\n\nlibrary(purrr)\n\nx &lt;- 1:10\ny &lt;- x |&gt;\n  map(\\(x) runif(n = 100, min = x, max = 15))\n\ny &lt;- x |&gt;\n  map(\\(x) runif(n = 100, min = x, max = 15)) |&gt;\n  map_dbl(mean)\n\n\n\n\nRun the code and look inside y after then first function call and after the second. What do you expect to see?"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#pivot-functions",
    "href": "stats-math/03-lecture/03-lecture.html#pivot-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Pivot Functions",
    "text": "Pivot Functions\n“pivot_longer()”lengthens” data, increasing the number of rows and decreasing the number of columns. The inverse transformation is pivot_wider()”\n\n\nlibrary(tidyr)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\nlong &lt;- iris |&gt; pivot_longer(!Species, names_to = \"species\", values_to = \"measure\")\nhead(long)\n\n# A tibble: 6 × 3\n  Species species      measure\n  &lt;fct&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 setosa  Sepal.Length     5.1\n2 setosa  Sepal.Width      3.5\n3 setosa  Petal.Length     1.4\n4 setosa  Petal.Width      0.2\n5 setosa  Sepal.Length     4.9\n6 setosa  Sepal.Width      3"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#data-transformations-1",
    "href": "stats-math/03-lecture/03-lecture.html#data-transformations-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Transformations",
    "text": "Data Transformations\n\nSimulateLoops and MapsPivotPlot\n\n\n\n\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nrates &lt;- seq(0.05, 0.20, length = 10)\nP &lt;- 100 \ntime &lt;- seq(1, 50, length = 50)\n\nPe &lt;- function(A, r, t) A * exp(r * t)\nd &lt;- time |&gt;\n  map(\\(x) Pe(A = P, r = rates, t = x)) \nclass(d)\n\n[1] \"list\"\n\nd[[1]][1:4]\n\n[1] 105.1271 106.8939 108.6904 110.5171\n\nnames(d) &lt;- as.character(time)\nd &lt;- as_tibble(d)\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n105.1271\n110.5171\n116.1834\n\n\n106.8939\n114.2631\n122.1403\n\n\n108.6904\n118.1360\n128.4025\n\n\n110.5171\n122.1403\n134.9859\n\n\n112.3745\n126.2802\n141.9068\n\n\n114.2631\n130.5605\n149.1825\n\n\n116.1834\n134.9859\n156.8312\n\n\n118.1360\n139.5612\n164.8721\n\n\n120.1215\n144.2917\n173.3253\n\n\n122.1403\n149.1825\n182.2119\n\n\n\n\n\n\n\n\n\n\n\nModern R usage offers a lot of shortcuts but those may be confusing to beginners.\nIn particular, R loops have mostly been replaced with map() functions.\nWe recommend purrr::map() functions instead of R’s *apply().\n\n\n\n\nrates &lt;- seq(0.05, 0.20, length = 10)\nP &lt;- 100 \ntime &lt;- seq(1, 5, length = 50)\n\nPe &lt;- function(A, r, t) A * exp(r * t)\ntime |&gt; map(\\(x) Pe(A = P, r = rates, t = x)) \n\n# above is a shortcut for\nmap(time, function(x) Pe(A = P, r = rates, t = x))\n\n# and the above is a shortcut for the following loop\nl &lt;- list()\nfor (i in seq_along(time)) {\n  l[[i]] &lt;- Pe(A = P, r = rates, t = time[i])\n}\n\n\n\n\n\n\n\nlibrary(tidyr)\n# add rates as a column\nd &lt;- d %&gt;% mutate(rate = \n            round(rates, 2) |&gt;\n              as.character())\n\n# convert from wide format to long\nd &lt;- d %&gt;% \n  pivot_longer(!rate, \n               names_to = \"year\", \n               values_to = \"value\") \n\nd$year &lt;- as.numeric(d$year)\n\nHere is a cheat sheet explaining tidyr functions.\n\n\n\n\n\n\n\nrate\nyear\nvalue\n\n\n\n\n0.05\n1\n105.1271\n\n\n0.05\n2\n110.5171\n\n\n0.05\n3\n116.1834\n\n\n0.05\n4\n122.1403\n\n\n0.05\n5\n128.4025\n\n\n0.05\n6\n134.9859\n\n\n0.05\n7\n141.9068\n\n\n0.05\n8\n149.1825\n\n\n0.05\n9\n156.8312\n\n\n0.05\n10\n164.8721\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(d, aes(year, value))\np + geom_line(aes(color = rate), linewidth = 0.2) +\n  scale_y_continuous(labels = \n          scales::dollar_format()) +\n  xlab(\"Time (years)\") + \n  ylab(\"Asset value\") +\n  ggtitle(\"Growth of $100 at different interest rates\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd |&gt; group_by(rate) |&gt; \n  summarise(y50 = last(value)) |&gt;\n  knitr::kable()\n\n\n\n\nrate\ny50\n\n\n\n\n0.05\n1218.249\n\n\n0.07\n2803.162\n\n\n0.08\n6450.009\n\n\n0.1\n14841.316\n\n\n0.12\n34149.510\n\n\n0.13\n78577.199\n\n\n0.15\n180804.241\n\n\n0.17\n416026.201\n\n\n0.18\n957266.257\n\n\n0.2\n2202646.579\n\n\n\n\n\n\n\n\n\n\n\nRStudio cheatsheats"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#your-turn",
    "href": "stats-math/03-lecture/03-lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nDataInstructionsOutput\n\n\n\n\n\n\n\n\n\n\nRun the following command: install.packages('HistData')\nFollowed by library(HistData)\nTale a look at the Arbuthnot dataset: ?Arbuthnot\n\n\n\n\n\n\nYear\nMales\nFemales\nPlague\nMortality\nRatio\nTotal\n\n\n\n\n1629\n5218\n4683\n0\n8771\n1.114243\n9.901\n\n\n1630\n4858\n4457\n1317\n10554\n1.089971\n9.315\n\n\n1631\n4422\n4102\n274\n8562\n1.078011\n8.524\n\n\n1632\n4994\n4590\n8\n9535\n1.088017\n9.584\n\n\n1633\n5158\n4839\n0\n8393\n1.065923\n9.997\n\n\n1634\n5035\n4820\n1\n10400\n1.044606\n9.855\n\n\n\n\n\n\n\n\nUse the tools to produce the plot the looks something like this.\nBonus: compute the ratio of female births and plot it."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#integral-calculus",
    "href": "stats-math/03-lecture/03-lecture.html#integral-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Integral Calculus",
    "text": "Integral Calculus\n\n\n\n\nIntegration plays a central role in Statistics\nIt is a way to compute Expectations and Event Probabilities\nIn Bayesian Statistics, we use integration to compute posterior distributions of the unknowns\nIn Frequentist Statistics, we use derivatives to find the most likely values of the unknowns\n\n\n\n\n\n\n\n\n\n\nUnknowns are sometimes called parameters, like our \\(a\\) and \\(b\\), in the \\(x(t) = a + bt^2\\) model"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#intuition-behind-integration",
    "href": "stats-math/03-lecture/03-lecture.html#intuition-behind-integration",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Intuition Behind Integration",
    "text": "Intuition Behind Integration\n\n\n\n\nIntegration is a continous analog of summation\nYou can also think of an integral as undoing a derivative\nYou can also think of it as a signed area under a function \\(f\\)\nIn modern applications, integration is almost always done numerically on the computer\nBut it helps to understand what what the computer is doing\n\n\n\n\n\n\n\n\n\n\nImage Source: Calculus Volume 1"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#from-velocity-to-postion-functions",
    "href": "stats-math/03-lecture/03-lecture.html#from-velocity-to-postion-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "From Velocity to Postion Functions",
    "text": "From Velocity to Postion Functions\n\nAntiderivativePlot\n\n\n\n\nRecal our position function \\(x(t) = 2 + 3t^2\\)\nWe found the velocity function by differentiating and we can almost get back the position function by integrating.\n\n\n\n\\[\n\\begin{eqnarray}\nv(t) & = & \\frac{d}{dt} \\left( 2 + 3t^2 \\right) = 6t \\\\\nx(t) & = & \\int{6t\\, dt} = 3t^2 + C\n\\end{eqnarray}\n\\]\n\n\n\nWhy almost? Look at the constant \\(2\\) in \\(\\frac{d}{dt}(2 + 3t^2)\\). You can replace it with any other constant and the result will still be \\(6t\\).\nTo put it another way, to characterize the position fucntion you need to know the intial position and you can’t get that from the velocity function alone.\n\n\n\n\nThe position function for different values of initial position \\(C\\). Notice that the only thing that changes is the intercept."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#some-common-integrals",
    "href": "stats-math/03-lecture/03-lecture.html#some-common-integrals",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Common Integrals",
    "text": "Some Common Integrals\n\n\nOpenStax: Here is a more complete list"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#techniques-of-intergration",
    "href": "stats-math/03-lecture/03-lecture.html#techniques-of-intergration",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Techniques of Intergration",
    "text": "Techniques of Intergration\n\n\nIntegral, like a derivative, is a linear operator:\n\n\n\n\\[\n\\begin{eqnarray}\n\\int [f(x) + g(x)] \\, dx &=& \\int f(x) \\, dx + \\int g(x) \\, dx \\\\\n\\int [c \\cdot f(x)] \\, dx &=& c \\int f(x) \\, dx\n\\end{eqnarray}\n\\]\n\n\n\nUnlike derivatives, there are generablly no rules for finding integrals\nMost integrals do not have a closed-form, analytical solutions. This is true for almost all integrals in statistics.\nIn one or two dimentions, it is easy to evaluate most integrals numerically.\nIn higher dimentions, you need very sophisticated methods that rely on Markov Chain Monte Carlo (MCMC). We will not cover MCMC in this course.\nFor simple integrals we can somtimes find a closed-form solution by relying on u-substitution and integration by parts.\n\n\n:::"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#playing-with-integrals",
    "href": "stats-math/03-lecture/03-lecture.html#playing-with-integrals",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Playing with Integrals",
    "text": "Playing with Integrals\n\n\nGiven our intuition for integrals being singed areas, let’s see how to compute them analytically and numerically.\nWarning: these techniques only work in low dimentions. For high dimentional integrals you need to use MCMC.\nSuppose we want evaluate the integral \\(\\int_{1}^{3} x \\sin(x^2)\\, dx\\)"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#constructing-a-riemann-sum",
    "href": "stats-math/03-lecture/03-lecture.html#constructing-a-riemann-sum",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Constructing a Riemann Sum",
    "text": "Constructing a Riemann Sum\n\n\n\n\n\n\n\nriemann &lt;- function(f, lower, upper, step_size) {\n  step &lt;- seq(lower, upper, by = step_size)\n  s &lt;- 0 # initialize the sum\n  for (i in 2:length(step)) {\n    # multiply base by the height of the rectangle\n    area &lt;- step_size * f(step[i]) \n    s &lt;- s + area\n  }\n  return(s)\n}\n\n\n\nNotice that the function takes a function as an argument. These are called higher order functions.\n\n\nSource: OpenStax Calculus Volume 1"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#evaluating-the-integral",
    "href": "stats-math/03-lecture/03-lecture.html#evaluating-the-integral",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Integral",
    "text": "Evaluating the Integral\n\nf &lt;- function(x) x * sin(x^2)\nx &lt;- seq(0, pi, len = 100)\n\nriemann(f, 1, 3, 0.01)\n\n[1] 0.7275414\n\n# compute using R's integrate function\nintegrate(f, 1, 3)\n\n0.7257163 with absolute error &lt; 1.3e-09"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#integrating-analytically",
    "href": "stats-math/03-lecture/03-lecture.html#integrating-analytically",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Integrating Analytically",
    "text": "Integrating Analytically\n\n\nMost integrals can’t be evaluated analytically but we can do \\(\\int x \\sin(x^2)\\, dx\\).\nWe make a substitution. Let \\(u = x^2\\), then \\(du/dx = 2x\\) and \\(dx = \\frac{1}{2 \\sqrt{u}}du\\)\n\n\n\n\\[\n\\begin{eqnarray}\n\\int \\sqrt{u} \\cdot \\sin(u) \\frac{1}{2\\sqrt{u}}du  & = & \\\\\n\\frac{1}{2}\\int \\sin(u)\\, du & = & \\\\\n-\\frac{1}{2} \\cos(u) & = & -\\frac{1}{2} \\cos(x^2)  \n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#comparing-the-results",
    "href": "stats-math/03-lecture/03-lecture.html#comparing-the-results",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Comparing the Results",
    "text": "Comparing the Results\n\n\nUsing R’s integrate function:\n\n\nintegrate(f, 1, 3)\n\n0.7257163 with absolute error &lt; 1.3e-09\n\n\n\n\n\nUsing the analytical solution\n\n\nf1 &lt;- function(x) -1/2 * cos(x^2)\n\nf1(3) - f1(1)\n\n[1] 0.7257163\n\n\n\n\n\nThe universe is in balance!"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#analytical-integration-on-the-computer",
    "href": "stats-math/03-lecture/03-lecture.html#analytical-integration-on-the-computer",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analytical Integration on the Computer",
    "text": "Analytical Integration on the Computer\n\n\nWhen in doubt, you can always try WolframAlpha\nPython library SymPy through R pacakge caracas\n\n\n\nlibrary(caracas); library(stringr)\nadd_align &lt;- function(latex) {\n  str_c(\"\\\\begin{align} \", latex, \" \\\\end{align}\")\n}\nadd_int &lt;- function(latex) {\n  str_c(\"\\\\int \", latex, \"\\\\, dx\")\n}\nx &lt;- symbol('x'); f &lt;- x^2 / sqrt(x^2 + 4)\ntex(f) %&gt;% add_int() %&gt;% str_c(\" =\") %&gt;% add_align() %&gt;% cat()\n\\[\\begin{align} \\int \\frac{x^{2}}{\\sqrt{x^{2} + 4}}\\, dx = \\end{align}\\]\nint(f, x) %&gt;% tex() %&gt;% add_align() %&gt;% cat()\n\\[\\begin{align} \\frac{x \\sqrt{x^{2} + 4}}{2} - 2 \\operatorname{asinh}{\\left(\\frac{x}{2} \\right)} \\end{align}\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#your-turn-waiting-time",
    "href": "stats-math/03-lecture/03-lecture.html#your-turn-waiting-time",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn: Waiting Time",
    "text": "Your Turn: Waiting Time\n\nThere is s famous distribution in statistics called Exponential distribution\nIts probability density function (PDF) is given by:\n\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\, x &gt; 0, \\text{and } \\lambda &gt; 0\n\\]\n\nThis distribution is sometimes called the waiting time (to some event) distribution, where \\(\\lambda\\) is the rate of events we expect\nOne property of this distribution is that no matter how long you wait, the probability of seeing an event remains the same.\nOne of the properties of the PDF is that it must integrate to 1\nLet’s check that it’s true\n\n\\[\n\\int_{0}^{\\infty} \\lambda e^{-\\lambda x} dx =\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#ingegration-by-parts",
    "href": "stats-math/03-lecture/03-lecture.html#ingegration-by-parts",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Ingegration by Parts",
    "text": "Ingegration by Parts\n\\[\n\\begin{eqnarray}\n(f g)' & = & f'g + g'f \\\\\n\\int (f g)' \\, dx & = & \\int f'g dx + \\int g'f \\, dx \\\\\nfg & = & \\int f'g \\, dx + \\int g'f \\, dx \\\\\n\\int f g' \\, dx & = & fg - \\int f' g \\, dx \\\\\nu & = & f(x) \\\\\nv & = & g(x) \\\\\ndu & = & f'(x) \\, dx \\\\\ndv & = & g'(x) \\, dx \\\\\n\\int u \\, dv & = & uv - \\int v \\, du\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#ex-with-lambda-1",
    "href": "stats-math/03-lecture/03-lecture.html#ex-with-lambda-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "E(X) with \\(\\lambda = 1\\)",
    "text": "E(X) with \\(\\lambda = 1\\)\n\nIf the rate of arrival is \\(\\lambda = 1\\), we have\n\n\\[\nE(X) = \\int_{0}^{\\infty} x e^{-x} dx =\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#exponential-growth-again",
    "href": "stats-math/03-lecture/03-lecture.html#exponential-growth-again",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Exponential Growth (again)",
    "text": "Exponential Growth (again)\nRecall, at the beginning we defined an exponential growth with the following differential equation:\n\n\\[\n\\frac{dy(t)}{dt} = k \\cdot y(t)\n\\]\n\n\nWe can now solve it:\n\\[\n\\begin{align*}\n\\frac{1}{y} \\, dy &= k \\, dt \\\\\n\\int \\frac{1}{y} \\, dy &= \\int k \\, dt \\\\\n\\log(y) &= k \\cdot t + C \\\\\ny(t) &= y_0 \\cdot e^{kt}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#homework",
    "href": "stats-math/03-lecture/03-lecture.html#homework",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Homework",
    "text": "Homework\n\nTake a look at dataset iris (?iris)\nCompute the overall average Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\nCompute the average by each Species of flower (hint: use group_by and summarise functions from dplyr)\nProduce the plot that looks like this:\n\n\n\n\n\n\n\nProduce the plot that looks like this: (check out geom_density and facet_wrap functions)\n\n\n\n\n\n\n\nCompute the following integral and show the steps:\n\n\\[\n\\int 2x \\cos(x^2)\\, dx\n\\]\n\nEvaluate this integral (on paper) from \\(0\\) to \\(2\\pi\\) and use R’s integrate function to validate your answer.\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#session-8-outline",
    "href": "stats-math/07-lecture/07-lecture.html#session-8-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 8 Outline",
    "text": "Session 8 Outline\n\n\nStatistical analysis workflow\nIntroduction to the rstanarm package\nSetting up a linear regression\nModel evaluation\nModel expansion\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#analysis-workflow",
    "href": "stats-math/07-lecture/07-lecture.html#analysis-workflow",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Workflow",
    "text": "Analysis Workflow\n\nFollowing is a high-level, end-to-end view from “R for Data Science” by Wickham and Grolemund"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#analysis-workflow-1",
    "href": "stats-math/07-lecture/07-lecture.html#analysis-workflow-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Workflow",
    "text": "Analysis Workflow\n\n\n\nA more comprehensive “Bayesian Workflow” by Gelman at al."
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#some-notation",
    "href": "stats-math/07-lecture/07-lecture.html#some-notation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Notation",
    "text": "Some Notation\n\n\nObserved data \\(y\\)\nUnobserved but observable data \\(\\widetilde{y}\\)\nUnobservable parameters \\(\\theta\\)\nCovariates \\(X\\)\nPrior distribution \\(f(\\theta)\\)\nLikelihood (as a function of \\(\\theta\\)), \\(f(y | \\theta, X)\\)\nPosterior distribution \\(f(\\theta | y, X)\\) (for Bayesian inference only)"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#bayes-vs-frequentist-inference",
    "href": "stats-math/07-lecture/07-lecture.html#bayes-vs-frequentist-inference",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Bayes vs Frequentist Inference",
    "text": "Bayes vs Frequentist Inference\n\n\n\n\nEstimation is the process of figuring out the unknowns, i.e., unobserved quantities\nIn frequentist inference, the problem is framed in terms of the most likely value(s) of \\(\\theta\\)\nBayesians want to characterize the whole distribution, a much more ambitious goal\n\n\n\n\n\nSuppose we want to characterize the following function, which represents some distribution of the unknown parameter:"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#wine-dataset",
    "href": "stats-math/07-lecture/07-lecture.html#wine-dataset",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Wine Dataset",
    "text": "Wine Dataset"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#wine-dataset-1",
    "href": "stats-math/07-lecture/07-lecture.html#wine-dataset-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Wine Dataset",
    "text": "Wine Dataset\n\nd &lt;- read.delim(\"data/winequality-red.csv\", sep = \";\")\ndim(d)\n\n[1] 1599   12\n\nd &lt;- d[!duplicated(d), ] # remove the duplicates\ndim(d)\n\n[1] 1359   12\n\nknitr::kable(head(d))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n1\n7.4\n0.70\n0.00\n1.9\n0.076\n11\n34\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n2\n7.8\n0.88\n0.00\n2.6\n0.098\n25\n67\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n3\n7.8\n0.76\n0.04\n2.3\n0.092\n15\n54\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n4\n11.2\n0.28\n0.56\n1.9\n0.075\n17\n60\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n6\n7.4\n0.66\n0.00\n1.8\n0.075\n13\n40\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n7\n7.9\n0.60\n0.06\n1.6\n0.069\n15\n59\n0.9964\n3.30\n0.46\n9.4\n5"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#scaling-the-data",
    "href": "stats-math/07-lecture/07-lecture.html#scaling-the-data",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Scaling the Data",
    "text": "Scaling the Data\n\nds &lt;- scale(d) # subtract the mean and divide by sd\nknitr::kable(head(ds) %&gt;% round(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n1\n-0.52\n0.93\n-1.39\n-0.46\n-0.25\n-0.47\n-0.38\n0.58\n1.29\n-0.58\n-0.95\n-0.76\n\n\n2\n-0.29\n1.92\n-1.39\n0.06\n0.20\n0.87\n0.60\n0.05\n-0.71\n0.12\n-0.58\n-0.76\n\n\n3\n-0.29\n1.26\n-1.19\n-0.17\n0.08\n-0.09\n0.21\n0.16\n-0.32\n-0.05\n-0.58\n-0.76\n\n\n4\n1.66\n-1.36\n1.47\n-0.46\n-0.27\n0.11\n0.39\n0.69\n-0.97\n-0.46\n-0.58\n0.46\n\n\n6\n-0.52\n0.71\n-1.39\n-0.53\n-0.27\n-0.28\n-0.20\n0.58\n1.29\n-0.58\n-0.95\n-0.76\n\n\n7\n-0.24\n0.39\n-1.09\n-0.68\n-0.39\n-0.09\n0.36\n-0.17\n-0.06\n-1.16\n-0.95\n-0.76\n\n\n\n\ncheck &lt;- apply(ds, 2, function(x) c(mean = mean(x), sd = sd(x))) %&gt;% round(2)\nknitr::kable(check)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\nmean\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsd\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nclass(ds); ds &lt;- as_tibble(ds)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#quality-rating-of-red-wine",
    "href": "stats-math/07-lecture/07-lecture.html#quality-rating-of-red-wine",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Quality Rating of Red Wine",
    "text": "Quality Rating of Red Wine\n\n\n\nUnscaled Quality\n\n\nqplot(d$quality, geom = \"histogram\") + xlab(\"Quality Rating of Red Wine\")\n\n\n\n\n\n\n\n\n\n\nScaled Quality\n\n\nqplot(ds$quality, geom = \"histogram\") + xlab(\"Quality Rating of Red Wine\")"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#alcohol",
    "href": "stats-math/07-lecture/07-lecture.html#alcohol",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Alcohol",
    "text": "Alcohol\n\np &lt;- ggplot(ds, aes(alcohol, quality))\np + geom_jitter(width = 0.1, height = 0.2, alpha = 1/5) + xlab(\"Alcohol\") + ylab(\"Quality (Jittered)\")"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#volatile-acidity",
    "href": "stats-math/07-lecture/07-lecture.html#volatile-acidity",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Volatile Acidity",
    "text": "Volatile Acidity\n\np &lt;- ggplot(ds, aes(volatile.acidity, quality))\np + geom_jitter(width = 0.1, height = 0.2, alpha = 1/5) + xlab(\"Volatile Acidity\") + ylab(\"Quality (Jittered)\")"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#correlations",
    "href": "stats-math/07-lecture/07-lecture.html#correlations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Correlations",
    "text": "Correlations\n\nlibrary(corrplot)\nM &lt;- cor(ds)\ncorrplot(M, method = 'ellipse', order = 'AOE', type = 'upper')"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#linear-regression",
    "href": "stats-math/07-lecture/07-lecture.html#linear-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nWe will fit a linear regression to the scaled wine dataset\nThe priors come from (sensible) defaults in rstanarm\nLinear regression can be specified in the following way, where \\(X\\) is the design matrix with one or more scaled predictors and \\(y\\) is the scaled quality score\n\n\n\n\\[\n\\begin{aligned}\ny &\\sim \\mathrm{Normal}(\\alpha + X \\beta, \\ \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, \\ 2.5) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, \\ 2.5) \\\\\n\\sigma &\\sim \\mathrm{Exp}(1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#estimation-using-rs-lm",
    "href": "stats-math/07-lecture/07-lecture.html#estimation-using-rs-lm",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimation using R’s lm()",
    "text": "Estimation using R’s lm()\n\n\nWe will start by comparing alcohol content to quality ratings\nIn R’s lm() priors are not specified – they are uniform\n\n\n\n\nfit1_freq &lt;- lm(quality ~ alcohol, data = ds)\narm::display(fit1_freq)\n\nlm(formula = quality ~ alcohol, data = ds)\n            coef.est coef.se\n(Intercept) 0.00     0.02   \nalcohol     0.48     0.02   \n---\nn = 1359, k = 2\nresidual sd = 0.88, R-Squared = 0.23\n\n# avoid R's summary() function\nsummary(fit1_freq)\n\n\nCall:\nlm(formula = quality ~ alcohol, data = ds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4372 -0.4761 -0.2097  0.6494  3.1666 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.310e-15  2.380e-02    0.00        1    \nalcohol      4.803e-01  2.381e-02   20.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8774 on 1357 degrees of freedom\nMultiple R-squared:  0.2307,    Adjusted R-squared:  0.2302 \nF-statistic:   407 on 1 and 1357 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#running-the-regression-in-rstanarm",
    "href": "stats-math/07-lecture/07-lecture.html#running-the-regression-in-rstanarm",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Running the Regression in rstanarm",
    "text": "Running the Regression in rstanarm\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\nfit1 &lt;- stan_glm(quality ~ alcohol, data = ds, refresh = 0)\nsummary(fit1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ alcohol\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.0    0.0  0.0   0.0   0.0  \nalcohol     0.5    0.0  0.4   0.5   0.5  \nsigma       0.9    0.0  0.9   0.9   0.9  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.0    0.0  0.0   0.0   0.0  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3773 \nalcohol       0.0  1.0  3938 \nsigma         0.0  1.0  3610 \nmean_PPD      0.0  1.0  4028 \nlog-posterior 0.0  1.0  1744 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#plotting",
    "href": "stats-math/07-lecture/07-lecture.html#plotting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Plotting",
    "text": "Plotting\n\n\n\nplot(fit1, plotfun = \"areas\", prob = 0.9)\n\n\n\n\n\n\n\n\n\n\nposterior_vs_prior(fit1)"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#evaluating-the-model",
    "href": "stats-math/07-lecture/07-lecture.html#evaluating-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\n\nHow good is this model?\n\n\np1 &lt;- pp_check(fit1, plotfun = \"ppc_dens_overlay\"); p2 &lt;- pp_check(fit1, plotfun = \"ppc_ecdf_overlay\")\np3 &lt;- pp_check(fit1, plotfun = \"ppc_stat\", stat =\"mean\"); p4 &lt;- pp_check(fit1, plotfun = \"ppc_stat\", stat =\"sd\")\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#evaluating-the-model-1",
    "href": "stats-math/07-lecture/07-lecture.html#evaluating-the-model-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\n\nHow good is this model?\n\n\nyrep1 &lt;- posterior_predict(fit1)\ndim(yrep1)\n\n[1] 4000 1359\n\ns &lt;- sample(1:nrow(ds), size = 50) # select 50 random records\np1 &lt;- ppc_intervals(ds$quality[s], yrep1[, s]); p1"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#we-can-add-other-predictors",
    "href": "stats-math/07-lecture/07-lecture.html#we-can-add-other-predictors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "We Can Add Other Predictors",
    "text": "We Can Add Other Predictors\n\nfit2 &lt;- stan_glm(quality ~ ., data = ds, refresh = 0)\nplot(fit2, plotfun = \"areas\", prob = 0.9, params = \"alcohol\")"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#have-we-improved-the-model",
    "href": "stats-math/07-lecture/07-lecture.html#have-we-improved-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Have We Improved the Model",
    "text": "Have We Improved the Model\n\nyrep2 &lt;- posterior_predict(fit2)\np1 &lt;- p1 + ggtitle(\"Model 1 predictions\")\np2 &lt;- ppc_intervals(ds$quality[s], yrep2[, s]) + ggtitle(\"Model 2 predictions\")\np1 + p2 + plot_layout(nrow = 2, byrow = FALSE)"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#have-we-improved-the-model-1",
    "href": "stats-math/07-lecture/07-lecture.html#have-we-improved-the-model-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Have We Improved the Model",
    "text": "Have We Improved the Model\n\n# Mean Square Error for Model 1\nmean((colMeans(yrep1) - ds$quality)^2) %&gt;% round(2)\n\n[1] 0.77\n\n# Mean Square Error for Model 2\nmean((colMeans(yrep2) - ds$quality)^2) %&gt;% round(2)\n\n[1] 0.64\n\nwidth &lt;- function(yrep, q1, q2) {\n  q &lt;- apply(yrep, 2, function (x) quantile(x, probs = c(q1, q2)))\n  width &lt;- apply(q, 2, diff)\n  return(mean(width))\n}\nwidth(yrep1, 0.25, 0.75) %&gt;% round(2)\n\n[1] 1.18\n\nwidth(yrep2, 0.25, 0.75) %&gt;% round(2)\n\n[1] 1.09"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#comparing-out-of-sample-performance",
    "href": "stats-math/07-lecture/07-lecture.html#comparing-out-of-sample-performance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Comparing Out of Sample Performance",
    "text": "Comparing Out of Sample Performance\n\nloo1 &lt;- loo(fit1, cores = 4)\nloo2 &lt;- loo(fit2, cores = 4)\nloo_compare(loo1, loo2)\n\n     elpd_diff se_diff\nfit2    0.0       0.0 \nfit1 -117.4      17.7 \n\n\nVehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413–1432. :10.1007/s11222-016-9696-4. Links: published | arXiv preprint."
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#can-we-still-improve-the-model",
    "href": "stats-math/07-lecture/07-lecture.html#can-we-still-improve-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Can We Still Improve the Model",
    "text": "Can We Still Improve the Model\n\n\nQuality variable is an ordinal scale variable, so a more appropriate likelihood would also be ordinal\nFor more information, see ordered logistic distribution in the Stan manual\nWe are going to use the following parameterization available in Stan:\n\n\n\n\\[\n\\text{OrderedLogistic}(k~|~\\eta,c) = \\left\\{ \\begin{array}{ll} 1 -\n\\text{logit}^{-1}(\\eta - c_1)  &  \\text{if } k = 1, \\\\[4pt]\n\\text{logit}^{-1}(\\eta - c_{k-1}) - \\text{logit}^{-1}(\\eta - c_{k})  &\n\\text{if } 1 &lt; k &lt; K, \\text{and} \\\\[4pt] \\text{logit}^{-1}(\\eta -\nc_{K-1}) - 0  &  \\text{if } k = K \\end{array} \\right.\n\\]\n\n\n\nA similar model can fit with frequentist methods using MASS::polr() function"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#fitting-ordered-logistic",
    "href": "stats-math/07-lecture/07-lecture.html#fitting-ordered-logistic",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Fitting Ordered Logistic",
    "text": "Fitting Ordered Logistic\n\nds$quality &lt;- d$quality - 2          # quality vector is now 1:6\nds$quality &lt;- as.factor(ds$quality)  # it has to be treated as factor (category)\nfit3 &lt;- stan_polr(quality ~ ., prior = R2(0.25), # the prior is on explained variance\n                  prior_counts = dirichlet(1), \n                  data = ds, iter = 1000, refresh = 0)\nyrep3 &lt;- posterior_predict(fit3)\nyrep3[1:5, 1:5]\n\n     1   2   3   4   5  \n[1,] \"3\" \"3\" \"3\" \"3\" \"4\"\n[2,] \"3\" \"3\" \"4\" \"4\" \"3\"\n[3,] \"4\" \"1\" \"4\" \"3\" \"3\"\n[4,] \"3\" \"3\" \"3\" \"3\" \"3\"\n[5,] \"3\" \"3\" \"4\" \"2\" \"4\"\n\nyrep3 &lt;- apply(yrep3, 2, as.numeric)\nyrep3[1:5, 1:5]\n\n     1 2 3 4 5\n[1,] 3 3 3 3 4\n[2,] 3 3 4 4 3\n[3,] 4 1 4 3 3\n[4,] 3 3 3 3 3\n[5,] 3 3 4 2 4"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#evaluating-model-fit",
    "href": "stats-math/07-lecture/07-lecture.html#evaluating-model-fit",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\n\n\n\npp_check(fit2, plotfun = \"ppc_dens_overlay\")\n\n\n\n\n\n\n\n\n\n\npp_check(fit3, plotfun = \"ppc_dens_overlay\")"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#evaluating-model-fit-1",
    "href": "stats-math/07-lecture/07-lecture.html#evaluating-model-fit-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\n\nPlotsMetrics\n\n\n\np3 &lt;- ppc_intervals(as.numeric(ds$quality[s]), yrep3[, s]) + ggtitle(\"Model 3 predictions\")\np2 + p3 + plot_layout(nrow = 2, byrow = FALSE)\n\n\n\n\n\n\n\n\n\n\n\ncat(\"Mean Square Error for Model 2:\", mean((colMeans(yrep2) - scale(d$quality))^2) %&gt;% round(2))\n\nMean Square Error for Model 2: 0.64\n\ncat(\"Mean Square Error for Model 3:\", mean((colMeans(yrep3) - as.numeric(ds$quality))^2) %&gt;% round(2))\n\nMean Square Error for Model 3: 0.43\n\ncat(\"Width of the 50% interval for Model 2:\", width(yrep2, 0.25, 0.75) %&gt;% round(2))\n\nWidth of the 50% interval for Model 2: 1.09\n\ncat(\"Width of the 50% interval for Model 3:\", width(yrep3, 0.25, 0.75) %&gt;% round(2))\n\nWidth of the 50% interval for Model 3: 0.72"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#quick-tips-to-improve-your-regression-modeling",
    "href": "stats-math/07-lecture/07-lecture.html#quick-tips-to-improve-your-regression-modeling",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "10+ quick tips to improve your regression modeling",
    "text": "10+ quick tips to improve your regression modeling\nThis advice comes from “Regression and Other Stories” by Gelman and Hill\n\n\nThink generatively (+)\nThink about what you are measuring (+)\nThink about variation, replication\nForget about statistical significance\nGraph the relevant and not the irrelevant\nInterpret regression coefficients as comparisons\nUnderstand statistical methods using fake-data simulation\nFit many models\nSet up a computational workflow\nUse transformations\nDo causal inference in a targeted way, not as a byproduct of a large regression\nLearn methods through live examples"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#so-long-and-thanks-for-all-the-fish",
    "href": "stats-math/07-lecture/07-lecture.html#so-long-and-thanks-for-all-the-fish",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "So Long (and Thanks for All the Fish)",
    "text": "So Long (and Thanks for All the Fish)\n\n\nLectures: https://ericnovik.github.io/smac.html\nKeep in touch:\n\neric.novik@nyu.edu\nTwitter: @ericnovik\nhttps://www.linkedin.com/in/enovik/\nPersonal blog: https://ericnovik.com/\n\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#session-5-outline",
    "href": "stats-math/05-lecture/05-partial.html#session-5-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 5 Outline",
    "text": "Session 5 Outline\n\n\nRandom variables\nBernoulli, Binomial, Geometric\nPDF and CDF\nExponential, Normal\nExpectations, Variance\nPoisson Random Variable\nJoint, Marginal, and Conditional\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#random-variables-are-not-random",
    "href": "stats-math/05-lecture/05-partial.html#random-variables-are-not-random",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Random Variables are Not Random",
    "text": "Random Variables are Not Random\n\nStoryPicture\n\n\n\n\nIt would be inconvenient to enumerate all possible events to describe a stochastic system\nA more general approach is to introduce a function that maps sample space \\(S\\) onto the Real line\nFor each possible outcome \\(s\\), random variable \\(X(s)\\) performs this mapping\nThis mapping is deterministic. The randomness comes from the experiment, not from the random variable (RV)\nWhile it makes sense to talk about \\(\\P(A)\\), where \\(A\\) is an event, it does not make sense to talk about \\(\\P(X)\\), but you can say \\(\\P(X(s) = x)\\), which we usually write as \\(\\P(X = x)\\)\nLet \\(X\\) be the number of Heads in two coin flips. You flip the coin twice, and you get \\(HH\\). In this case, \\(s = {HH}\\), \\(X(s) = 2\\), while \\(S = \\{TT, TH, HT, HH\\}\\)\n\n\n\n\n\nRandom variable \\(X\\) for the number of Heads in two flips"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#characterising-random-variables",
    "href": "stats-math/05-lecture/05-partial.html#characterising-random-variables",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Characterising Random Variables",
    "text": "Characterising Random Variables\n\nIntroductionR Conventions\n\n\n\n\nTwo ways of describing an RV are CDF (Cumulative Distribution Function) and PMF (Probability Mass Function) for discrete RVs and PDF (Probability Density Function) for continuous RVs. There are other ways, but we will stick with CDF and P[D/M]F.\nCDF \\(F_X(x)\\) is a function of \\(x\\) and is bounded between 0 and 1:\n\n\n\n\\[\nF_X(x) = \\P(X \\leq x)\n\\]\n\n\n\nPMF (for discrete RVs only) \\(f_X(x)\\) is a function of \\(x\\)\n\n\\[\nf_X(x) = \\P(X = x)\n\\]\n\n\n\nYou can get from \\(f_X\\) to \\(F_X\\) by summing. Let’s say \\(x = 4\\). In that case:\n\n\\[\nF_X(4) = \\P(X \\leq 4) = \\sum_{i = 4,3,2,...}\\P(X = i)\n\\]\n\n\n\n\n\nIn R, PMFs and PDFs start with the letter d. For example dbinom() and dnormal() refer to binomial PMF and normal PDF\nCDFs start with p, so pbinom() and pnorm()\nInverse CDFs or quantile functions, start with q so qbinom() and so on\nRandom number generators start with r, so rbinom()\nA binomial RV, which we will define later, represents the number of successes in N trials. In R, the PMF is dbinom() and CDF is pbinom()\nHere is the full function signature: dbinom(x, size, prob, log = FALSE)\n\nx is the number of successes, size is the number of trials N, prob is the probability of success in each trial \\(\\theta\\), and log is a flag asking if we want the results on the log scale."
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#binomial-rv",
    "href": "stats-math/05-lecture/05-partial.html#binomial-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial RV",
    "text": "Binomial RV\n\nBinomial PMFPMF and CDF PlotsCode to Generate the Plots\n\n\n\n\nBernoulli RV is one coin flip with a set probability of success (say Heads)\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), the PMF can be written directly as \\(\\P(X = x) = \\theta^x (1 - \\theta)^{1-x}, \\, x \\in \\{0, 1\\}\\)\nBinomial can be thought of as the sum of \\(N\\) independent Bernoulli trials. We can also write:\n\n\n\n\\[\n\\text{Bernoulli}(x~|~\\theta) = \\left\\{ \\begin{array}{ll} \\theta &\n\\text{if } x = 1, \\text{ and} \\\\ 1 - \\theta & \\text{if } x = 0\n\\end{array} \\right.\n\\]\n\n\n\nWe can write the Binomial PMF, \\(X \\sim \\text{Binomial}(N, \\theta)\\) this way:\n\n\\[\n\\text{Binomial}(x~|~N,\\theta) = \\binom{N}{x}\n\\theta^x (1 - \\theta)^{N - x}\n\\]\n\n\n\n\\(\\text{Binomial}(x~|~N=4,\\theta = 1/2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\nlibrary(MASS)\n\nN &lt;- 4 # Number of successes out of x trials\n\n# compute and plot the PMF\npmf &lt;- dbinom(x = 0:N, size = N, prob = 1/2)\nd &lt;- data.frame(x =  0:N, y = pmf)\np1 &lt;- ggplot(d, aes(x, pmf))\np1 &lt;- p1 + geom_col(width = .2) + \n  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +\n  ylab(\"P(X = x)\") + xlab(\"x = Number of Heads\") +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n# compute and plot the CDF\nx &lt;- seq(-0.5, 4.5, length = 500)\ncdf &lt;- pbinom(q = x, size = N, prob = 1/2)\nd &lt;- data.frame(q = x, y = cdf)\ndd &lt;- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)\np2 &lt;- ggplot(d, aes(x, cdf)) \np2 &lt;- p2 + geom_point(size = 0.2) + \n  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(CDF: F[X](x) == P(X &lt;= x))) +\n  ylab(expression(P(X &lt;= x))) + xlab(\"x = Number of Heads\")\n\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#binomial-in-r",
    "href": "stats-math/05-lecture/05-partial.html#binomial-in-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial in R",
    "text": "Binomial in R\n\n# What is the probability of getting 2 Heads out of 5 fair trials?\nN &lt;- 5; x &lt;- 2\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 5/16\n\n# What is the binomial PMF: P(X = x), for N = 5, p = 0.5?\nN &lt;- 5; x &lt;- -2:7 # notice we range x over any integers\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]    0    0 1/32 5/32 5/16 5/16 5/32 1/32    0    0\n\n# Verify that the PMF sums to 1\nsum(dbinom(x = x, size = N, prob = 0.5))\n\n[1] 1\n\n# What is the probability of 3 heads or fewer\npbinom(3, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 13/16\n\n# compute the CDF: P(X &lt;= x), for N = 5, p = 0.5\npbinom(x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n# get from the PMF to CDF; cumsum() is the cumulative sum function\ndbinom(x = x, size = N, prob = 0.5) |&gt; cumsum() |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n\n\n\nYour Turn: Suppose the probability of success is 1/3, N = 10. What is the probability of 6 or more successes? Compute it with a PMF first and verify with the CDF."
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#geometric-rv",
    "href": "stats-math/05-lecture/05-partial.html#geometric-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Geometric RV",
    "text": "Geometric RV\n\nPMFCheck ConvergenceExamplesPMF\n\n\n\n\nGeometric is a discrete waiting time distribution, and Exponential is its continuous analog\nIf \\(X\\) is the number of failures before first success \\(X \\sim \\text{Geometric}(\\theta)\\), where \\(\\theta\\) is probability of success\nExample: We keep flipping a coin until we get success, say Heads\n\nSay we flip five times, which means we get the following sequence: T T T T H\nThe probability of this sequence is: \\((\\frac{1}{2})^4 (\\frac{1}{2})^1\\)\nNotice this is the only way to get this sequence\n\nIf \\(x\\) is the number of failures, the PMF is \\(P(X = x) = (1 - \\theta)^x \\theta\\), where \\(x = 0, 1, 2, ...\\)\n\n\n\n\n\nTo check if this is a valid PMF we need to sum over all \\(x\\):\n\n\n\\[\n\\begin{align}\n\\sum_{x = 0}^{\\infty} \\theta (1 - \\theta)^x  =\n\\theta \\sum_{x = 0}^{\\infty} (1 - \\theta)^x \\\\\n\\text{Let } u = 1 - \\theta \\\\\n\\theta \\sum_{x = 0}^{\\infty} u^x = \\theta \\frac{1}{1-u} = \\theta \\frac{1}{1-1 + \\theta} = \\frac{\\theta}{\\theta} = 1\n\\end{align}\n\\]\n\n\n\nThe last bit comes from geometric series for \\(|u| &lt; 1\\)\n\n\n\n\n\nThe probability of T T T H (x = 3 failures) when \\(\\theta = 1/2\\), has to be \\((1/2)^4\\) or \\(1/16\\)\n\n\n\nx &lt;- 3; theta &lt;- 1/2\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 1/16\n\n\n\n\n\nIf \\(\\theta = 1/3\\), the probability of the same sequence has to be \\((2/3)^3 \\cdot 1/3 = 8/81\\)\n\n\nx &lt;- 3; theta &lt;- 1/3\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 8/81\n\n\n\n\n\nThe PMF is unbounded, but it converges to 1 as demonstrated before\n\n\n\n\n\n\nx &lt;-  0:15\ntheta &lt;- 1/5\ny &lt;- dgeom(x = x, prob = theta)\nd &lt;- data.frame(x, y)\np &lt;- ggplot(d, aes(x, y))\np + geom_col(width = 0.2) +\nxlab(\"x = Number of failures before first success\") +\n ylab(expression(P(X == x))) +\nggtitle(\"X ~ Geom(1/5)\", \n subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/smac.html"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#introductions",
    "href": "bayes-course/01-lecture/01-lecture.html#introductions",
    "title": "Bayesian Inference",
    "section": "Introductions",
    "text": "Introductions\n\nWhat is the city of your birth?\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#two-truths-and-a-lie1",
    "href": "bayes-course/01-lecture/01-lecture.html#two-truths-and-a-lie1",
    "title": "Bayesian Inference",
    "section": "Two Truths and a Lie1",
    "text": "Two Truths and a Lie1\n\n\n\nOne person tells three personal statements, one of which is a lie.\nOthers discuss and guess which statement is the lie, and they jointly construct a numerical statement of their certainty in the guess (on a 0–10 scale).\nThe storyteller reveals which was the lie.\nEnter the certainty number and the outcome (success or failure) and submit in the Google form. Rotate through everyone in your group so that each person plays the storyteller role once.\n\n\n\n\n\n\n\n\nGelman, A. (2023). “Two Truths and a Lie” as a Class-Participation Activity. The American Statistician, 77(1), 97–101."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#two-truths-and-a-lie",
    "href": "bayes-course/01-lecture/01-lecture.html#two-truths-and-a-lie",
    "title": "Bayesian Inference",
    "section": "Two Truths and a Lie",
    "text": "Two Truths and a Lie\nhttps://tinyurl.com/two-truths-and\n\n\nWhat do you think the range of certainty scores will look like: will there be any 0’s or 10’s? Will there be a positive relation between x and y: are guesses with higher certainty be more accurate, on average? How strong will the relation be between x and y: what will the curve look like? Give approximate numerical values for the intercept and slope coefficients corresponding to their sketched curves."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Lecture 1: Bayesian Workflow",
    "text": "Lecture 1: Bayesian Workflow\n\n\n\n\nTwo Truths and a Lie\nStatistics vs AI/ML\nBrief history of Bayesian inference\nReview of basic probability\nIntroduction to Bayesian workflow\nBayes’s rule for events\nBinomial model and the Bayesian Crank\nOverview of the Course"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "href": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "title": "Bayesian Inference",
    "section": "Statistics vs. AI/ML",
    "text": "Statistics vs. AI/ML\n⚠️ What follows is an oversimplified opinion.\n\n\n\n\nAI is great for automating tasks that humans find easy\n\nRecognizing faces, cats, and other objects\nIdentifying tumors on a radiology scan\nPlaying Chess and Go\nDriving a car\nPost ChatGPT 4: Generating coherent text/images/video, signs of reasoning\n\n\n\n\nStatistics is great at answering questions that humans find hard\n\nHow fast does a drug clear from the body?\nWhat is the expected tumor size two months after treatment?\nHow would patients respond under a different treatment?\nShould I take this drug?\nShould we (FDA/EMA/…) approve this drug?\n\n\n\n\n\n\n“Machine learning excels when you have lots of training data that can be reasonably modeled as exchangeable with your test data; Bayesian inference excels when your data are sparse and your model is dense.” — Andrew Gelman"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "href": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "title": "Bayesian Inference",
    "section": "Brief History",
    "text": "Brief History\nSummary of the book The Theory That Would Not Die\n\n\n\n\nThomas Bayes (1702(?) — 1761) is credited with the discovery of the “Bayes’s Rule”\nHis paper was published posthumously by Richard Price in 1763\nLaplace (1749 — 1827) independently discovered the rule and published it in 1774\nScientific context: Newton’s Principia was published in 1687\nBayesian wins: German Enigma cipher, search for a missing H-bomb, Federalist papers, Moneyball, FiveThirtyEight\n\n\n\n\n\n\n\n\n\n\nStephen Stigler gives 3:1 in favor of Nicholas Saunderson for the discovery of Bayes’s [sic] rule."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "href": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "title": "Bayesian Inference",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world.”"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "href": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "title": "Bayesian Inference",
    "section": "Modern Examples of Bayesian Analyses",
    "text": "Modern Examples of Bayesian Analyses\n\n\nOpenAI DALL·E: Pierre Simon Laplace in the style of Wassily Kandinsky"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis-hierarchical-models",
    "href": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis-hierarchical-models",
    "title": "Bayesian Inference",
    "section": "Vote Share Analysis (Hierarchical Models)",
    "text": "Vote Share Analysis (Hierarchical Models)\n\n\nGelman, A. (2010). Breaking down the 2008 vote. In Atlas of the 2008 Elections."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#pharmacometrics-ode-based-models",
    "href": "bayes-course/01-lecture/01-lecture.html#pharmacometrics-ode-based-models",
    "title": "Bayesian Inference",
    "section": "Pharmacometrics (ODE Based Models)",
    "text": "Pharmacometrics (ODE Based Models)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#medical-decision-making-bayesian-nonparametrics",
    "href": "bayes-course/01-lecture/01-lecture.html#medical-decision-making-bayesian-nonparametrics",
    "title": "Bayesian Inference",
    "section": "Medical Decision Making (Bayesian Nonparametrics)",
    "text": "Medical Decision Making (Bayesian Nonparametrics)\n\n\nJoensuu, H., Vehtari, A., et al. (2012). Risk of recurrence of gastrointestinal stromal tumour after surgery: An analysis of pooled population-based cohorts. The Lancet Oncology, 13(3), 265–274."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "title": "Bayesian Inference",
    "section": "Review of Probability",
    "text": "Review of Probability\n\n\nA set of all possible outcomes is called a sample space and denoted by \\(\\Omega\\)\nAn outcome of an experiment is denoted by \\(\\omega \\in \\Omega\\)\nWe typically denote events by capital letters, say \\(A \\subseteq \\Omega\\)\nAxioms of probability:\n\n\\(\\P(A) \\geq 0, \\, \\text{for all } A\\)\n\\(\\P(\\Omega) = 1\\)\nIf \\(A_1, A_2, A_3, \\ldots\\) are disjoint: \\(\\P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} \\P(A_i)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example",
    "href": "bayes-course/01-lecture/01-lecture.html#example",
    "title": "Bayesian Inference",
    "section": "Example",
    "text": "Example\n\nRolling diceOmega\n\n\n\n\nYou roll a fair six-sided die twice\nGive an example of an \\(\\omega \\in \\Omega\\)\nHow many elements are in \\(\\Omega\\)? What is \\(\\Omega\\)?\nDefine an event \\(A\\) as the sum of the two rolls less than 11\nHow many elements are in \\(A\\)?\nWhat is \\(\\P(A)\\)?\n\n\n\n\n\n\n\n\nouter(1:6, 1:6, FUN = \"+\")\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    2    3    4    5    6    7\n[2,]    3    4    5    6    7    8\n[3,]    4    5    6    7    8    9\n[4,]    5    6    7    8    9   10\n[5,]    6    7    8    9   10   11\n[6,]    7    8    9   10   11   12\n\n\n\n\n\n\\(\\Omega\\) consists of all pairs:\n\n\\(\\{(1, 1), (1, 2), ... (2, 1), (2, 2), ...\\}\\)\n\n\n\n\n\n\nThere are 36 such pairs\n33 of those pairs result in a sum of less than 11\n\\(\\P(A) = \\frac{33}{36} = \\frac{11}{12}\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "href": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "title": "Bayesian Inference",
    "section": "Random Variables Review",
    "text": "Random Variables Review\n\nReviewMapping\n\n\n\n\nRandom variable is not random – it is a deterministic mapping from the sample space onto the real line; randomness comes from the experiment\nPMF, PDF, CDF (Blitzstein and Hwang, Ch. 3, 5)\nExpectations (Blitzstein and Hwang, Ch. 4)\nJoint Distributions (Blitzstein and Hwang, Ch. 7)\nConditional Expectations (Blitzstein and Hwang, Ch. 9)\n\n\n\n\nRandom variable X for the number of Heads in two flips"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "href": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "title": "Bayesian Inference",
    "section": "Example Simulation",
    "text": "Example Simulation\n\n\nWe will use R’s sample() function to simulate rolls of a die and replicate() function to repeat the rolling process many times\nModern approach: purrr::map(1:n, \\(x) expression)\n\n\n\n\n\n\ndie &lt;- 1:6\nroll &lt;- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\nroll(die, 2) # roll the die twice\n\n[1] 6 3\n\nrolls &lt;- replicate(1e4, roll(die, 2))\nrolls[, 1:8] # print first 8 columns\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    6    5    6    6    2    3    6    3\n[2,]    3    2    4    4    3    2    6    1\n\nroll_sums &lt;- colSums(rolls)\nhead(roll_sums)\n\n[1]  9  7 10 10  5  5\n\nmean(roll_sums &lt; 11) \n\n[1] 0.9151\n\n\n\n\n\n\nGiven a Random Variable \\(Y\\), \\(y^{(1)}, y^{(2)}, y^{(3)}, \\ldots, y^N\\) are simulations or draws from \\(Y\\)\nFundamental bridge (Blitzstein & Hwang p. 164): \\(\\P(A) = \\E(\\I_A)\\)\nComputationally: \\(\\P(Y &lt; 11) \\approx \\frac{1}{N} \\sum^{N}_{n=1} \\I(y^n &lt; 11)\\)\nIn R, roll_sums &lt; 11 creates an indicator variable\nAnd mean() does the average"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-simulation-with-purrr",
    "href": "bayes-course/01-lecture/01-lecture.html#example-simulation-with-purrr",
    "title": "Bayesian Inference",
    "section": "Example Simulation with purrr",
    "text": "Example Simulation with purrr\n\n\n\n\nRoll a die twice, twenty times\n\n\n\n\nsim &lt;- purrr::map(1:20, \\(x) roll(die, 2))\nstr(sim)\n\nList of 20\n $ : int [1:2] 1 3\n $ : int [1:2] 6 5\n $ : int [1:2] 2 5\n $ : int [1:2] 5 5\n $ : int [1:2] 1 6\n $ : int [1:2] 6 3\n $ : int [1:2] 6 4\n $ : int [1:2] 5 5\n $ : int [1:2] 4 1\n $ : int [1:2] 5 4\n $ : int [1:2] 6 5\n $ : int [1:2] 2 5\n $ : int [1:2] 4 6\n $ : int [1:2] 2 6\n $ : int [1:2] 6 5\n $ : int [1:2] 4 5\n $ : int [1:2] 6 6\n $ : int [1:2] 5 6\n $ : int [1:2] 3 3\n $ : int [1:2] 5 2\n\n\n\n\n\n\nRoll a die once, twice, …., twenty times\n\n\n\n\nsim &lt;- purrr::map(1:20, \\(x) roll(die, x))\nstr(sim)\n\nList of 20\n $ : int 2\n $ : int [1:2] 4 3\n $ : int [1:3] 5 1 1\n $ : int [1:4] 5 6 2 6\n $ : int [1:5] 2 2 4 1 1\n $ : int [1:6] 1 6 6 5 4 4\n $ : int [1:7] 5 3 2 2 4 4 3\n $ : int [1:8] 2 6 3 5 3 1 1 1\n $ : int [1:9] 2 1 4 1 5 2 6 4 6\n $ : int [1:10] 3 6 4 5 4 4 6 3 3 6\n $ : int [1:11] 4 1 4 1 5 4 6 3 6 5 ...\n $ : int [1:12] 5 2 5 4 2 6 3 4 1 4 ...\n $ : int [1:13] 6 5 5 5 4 3 2 5 3 6 ...\n $ : int [1:14] 3 5 3 5 1 3 6 6 2 4 ...\n $ : int [1:15] 4 6 6 5 1 2 5 5 3 2 ...\n $ : int [1:16] 3 2 6 4 2 4 2 2 6 6 ...\n $ : int [1:17] 5 2 5 2 5 1 3 4 4 6 ...\n $ : int [1:18] 3 6 1 4 2 4 5 4 2 4 ...\n $ : int [1:19] 5 5 6 3 3 5 4 1 5 4 ...\n $ : int [1:20] 6 2 4 2 3 3 3 1 4 6 ..."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#review-of-conditional-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#review-of-conditional-probability",
    "title": "Bayesian Inference",
    "section": "Review of Conditional Probability",
    "text": "Review of Conditional Probability\n\n\nFor arbitrary \\(A\\) and \\(B\\), if \\(\\P(B) &gt; 0\\):\n\nConditional probability: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)}\\)\nConditional probability: \\(\\P(B \\mid A) = \\frac{\\P(AB)}{\\P(A)}\\)\n\nBayes’s rule: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)} = \\frac{\\P(B \\mid A) \\P(A)}{\\P(B)}\\)\n\\(A\\) and \\(B\\) are independent if observing \\(B\\) does not give you any more information about \\(A\\): \\(\\P(A \\mid B) = \\P(A)\\)\nAnd \\(\\P(A B) = \\P(A) \\P(B)\\) (from Bayes’s rule)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\nTitanic carried approximately 2,200 passengers and sank on 15 April 1912\nLet \\(G: \\{m, f\\}\\) represent Gender and \\(S: \\{n, y\\}\\) represent Survival\n\n\n\n\n\nsurv &lt;- apply(Titanic, c(2, 4), sum) |&gt; \n  as_tibble()\nsurv_prop &lt;- round(surv / sum(surv), 3)\nbind_cols(Sex = c(\"Male\", \"Female\"), \n          surv_prop) |&gt; \n  adorn_totals(c(\"row\", \"col\")) |&gt;\n  knitr::kable(caption = \n               \"Titanic survival proportions\")\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n\n\n\n\\(\\P(G = m \\cap S = y) =\\) 0.167\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) =\\) ??\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) = 0.323\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\n\n\nWhat is \\(\\P(G = m \\mid S = y)\\), probability of being male given survival?\nTo compute that, we only consider the column  where Survival = Yes\n\\(\\P(G = m \\mid S = y) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(S = y)} = \\frac{0.167}{0.323} \\\\ \\approx 0.52\\)\nYou want \\(\\P(S = y \\mid G = m)\\), comparing it to \\(\\P(S = y \\mid G = f)\\)\n\\(\\P(S = y \\mid G = m) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(G = m)} = \\frac{0.167}{0.787} \\\\ \\approx 0.21\\)\n\\(\\P(S = y \\mid G = f) = \\frac{\\P(G = f \\, \\cap \\, S = y)}{\\P(G = f)} = \\frac{0.156}{0.213} \\\\ \\approx 0.73\\)\nHow would you calculate \\(\\P(S = n \\mid G = m)\\)?\n\\(\\P(S = n \\mid G = m) = 1 - \\P(S = y \\mid G = m)\\)\n\n\n\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n “Untergang der Titanic”, as conceived by Willy Stöwer, 1912"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "href": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "title": "Bayesian Inference",
    "section": "Law of Total Probability (LOTP)",
    "text": "Law of Total Probability (LOTP)\nLet \\(A\\) be a partition of \\(\\Omega\\), so that each \\(A_i\\) is disjoint, \\(\\P(A_i &gt;0)\\), and \\(\\cup A_i = \\Omega\\). \\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B \\cap A_i) = \\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)\n\\]\n\n\nImage from Blitzstein and Hwang (2019), Page 55"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "href": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "title": "Bayesian Inference",
    "section": "Bayes’s Rule for Events",
    "text": "Bayes’s Rule for Events\n\n\nWe can combine the definition of conditional probability with the LOTP to come up with Bayes’s rule for events, assuming \\(\\P(B) \\neq 0\\)\n\n\n\\[\n\\P(A \\mid B) = \\frac{\\P(B \\cap A)}{\\P(B)} =\n               \\frac{\\P(B \\mid A) \\P(A)}{\\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)}\n\\]\n\n\nWe typically think of \\(A\\) is some unknown we wish to learn (e.g., the status of a disease) and \\(B\\) as the data we observe (e.g., the result of a diagnostic test)\nWe call \\(\\P(A)\\) prior probability of A (e.g., how prevalent is the disease in the population)\nWe call \\(\\P(A \\mid B)\\), the posterior probability of the unknown \\(A\\) given data \\(B\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "href": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "title": "Bayesian Inference",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(\\P(D^+ \\mid T^+)\\)\n\\(\\text{Specificity } := \\P(T^- \\mid D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - \\P(T^- \\mid D^-) = \\P(T^+ \\mid D^-) = 0.002\\)\n\\(\\text{Sensitivity } := \\P(T^+ \\mid D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - \\P(T^+ \\mid D^+) = \\P(T^- \\mid D^+) = 0.546\\)\nPrevalence: \\(\\P(D^+) = 0.001\\)\n\n\n\n\\[\n\\begin{eqnarray}\n\\P(D^+ \\mid T^+) = \\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+)} & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\sum_{i=1}^{n}\\P(T^+ \\mid D^i) \\P(D^i) } & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+ \\mid D^+) \\P(D^+) + \\P(T^+ \\mid D^-) \\P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "title": "Bayesian Inference",
    "section": "Bayesian Analysis",
    "text": "Bayesian Analysis\n\n\n\n\nSuppose we enrolled five people in an early cancer clinical trial\nEach person was given an active treatment\nFrom previous trials, we have some idea of historical response rates (proportion of people responding)\nAt the end of the trial, \\(Y = y \\in \\{0,1,...,5 \\}\\) people will have responded1 to the treatment\nWe are interested in estimating the probability that the response rate is greater or equal to 50%\n\n\n\n Image from Fokko Smits, Martijn Dirksen, and Ivo Schoots: RECIST 1.1 - and more\n\nPartial response is a decrease in the sum of the longest diameters of target lesions of at least 30% compared to the baseline measurement."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#notation",
    "href": "bayes-course/01-lecture/01-lecture.html#notation",
    "title": "Bayesian Inference",
    "section": "Notation",
    "text": "Notation\nGreek letters will be used for latent parameters, and English letters will be used for observables.\n\n\n\\(\\theta\\): unknowns or parameters to be estimated; could be multivariate, discrete, and continuous (your book uses \\(\\pi\\))\n\\(y\\): observations or measurements to be modelled (\\(y_1, y_2, ...\\))\n\\(\\widetilde{y}\\) : unobserved but observable quantities (in your book \\(y'\\))\n\\(x\\): covariates\n\\(f( \\theta )\\): a prior model, P[DM]F of \\(\\theta\\)\n\\(f_y(y \\mid \\theta, x)\\): an observational model, P[DM]F when it is a function of \\(y\\) (in your book: \\(f(y \\mid \\pi)\\)); we typically drop the \\(x\\) to simplify the notation.\n\\(f_{\\theta}(y \\mid \\theta)\\): is a likelihood function when it is a function of \\(\\theta\\) (in your book: \\(\\L(\\pi \\mid y)\\))\nSome people write \\(\\L(\\theta; y)\\) or simply \\(\\L(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "href": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "title": "Bayesian Inference",
    "section": "General Approach",
    "text": "General Approach\n\n\nBefore observing the data, we need to specify a prior model \\(f(\\theta)\\) on all unknowns \\(\\theta\\)1\nPick a data model \\(f(y \\mid \\theta, x)\\) — this is typically more important than the prior; this includes the model for the conditional mean: \\(\\E(y \\mid x)\\)\nFor more complex models, we construct a prior predictive distribution, \\(f(y)\\)\n\nWe will define this quantity later — it will help us assess if our choice of priors makes sense on the observational scale\n\nAfter we observe data \\(y\\), we treat \\(f(y \\mid \\theta, x)\\) as the likelihood of observing \\(y\\) under all plausible values of \\(\\theta\\), conditioning on \\(x\\) if necessary\nDerive a posterior model for \\(\\theta\\), \\(\\, f(\\theta \\mid y, x)\\) using Bayes’s rule or by simulation\nEvaluate model quality: 1) quality of the inferences; 2) quality of predictions. Revise the model if necessary\nCompute all the quantities of interest from the posterior, such as event probabilities, e.g., \\(\\P(\\theta &gt; 0.5)\\), posterior predictive distribution \\(f(\\widetilde{y} \\mid y)\\), decision functions, etc.\n\n\n\nFor a more complete workflow, see Bayesian Workflow by Gelman et al. (2020)\n\nThere is always a prior, even in frequentist inference."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "href": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "title": "Bayesian Inference",
    "section": "Example Prior Model",
    "text": "Example Prior Model\n\n\nWe will construct a prior model for our clinical trial\nFrom previous trials, we construct a discretized version of the prior distribution of the response rate\nThe most likely value for response rate is 30%\n\n\n\n\n\ndot_plot &lt;- function(x, y) {\n  p &lt;- ggplot(data.frame(x, y), aes(x, y))\n  p + geom_point(aes(x = x, y = y), size = 0.5) +\n    geom_segment(aes(x = x, y = 0, xend = x, \n                     yend = y), linewidth = 0.2) +\n    xlab(expression(theta)) + \n    ylab(expression(f(theta)))\n}\ntheta &lt;- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior &lt;- c(0.05, 0.45, 0.30, 0.15, 0.05)\ndot_plot(theta, prior) +\n  ggtitle(\"Prior probability of response\")\n\n\n\n\n\n\n\n\n\n\n\n\nEven though \\(\\theta\\) is a continuous parameter, we can still specify a discrete prior\nThe posterior will also be discrete and of the same cardinality"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#data-model",
    "href": "bayes-course/01-lecture/01-lecture.html#data-model",
    "title": "Bayesian Inference",
    "section": "Data Model",
    "text": "Data Model\n\n\nWe consider each person’s response rate to be independent, given the treatment\nWe have a fixed number of people in the trial, \\(N = 5\\), and \\(0\\) to \\(5\\) successes\nWe will therefore consider: \\(y | \\theta \\sim \\text{Bin}(N,\\theta) = \\text{Bin}(5,\\theta)\\)\n\\(f(y \\mid \\theta) = \\text{Bin} (y \\mid 5,\\theta) = \\binom{5}{y} \\theta^y (1 - \\theta)^{5 - y}\\) for \\(y \\in \\{0,1,\\ldots,5\\}\\)\nIs this a valid probability distribution as a function of \\(y\\)?\n\n\n\nN = 5; y &lt;- 0:N; theta &lt;- 0.5\n(f_y &lt;- dbinom(x = y, size = N, prob = theta) |&gt;\n    fractions())\n\n[1] 1/32 5/32 5/16 5/16 5/32 1/32\n\nsum(dbinom(x = y, size = N, prob = theta))\n\n[1] 1"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\nWe ran the trial and observed 3 out of 5 responders\nWe can now construct a likelihood function for \\(y = 3\\) as a function of \\(\\theta\\)\nLet’s check if this function is a probability distribution: \\[\nf (y) = \\int_{0}^{1} \\binom{N}{y} \\theta^y (1 - \\theta)^{N - y}\\, d\\theta = \\frac{1}{N + 1}\n\\]\n\n\n\n\ndbinom_theta &lt;- function(theta, N, y) {\n  choose(N, y) * theta^y * (1 - theta)^(N - y) \n}\nintegrate(dbinom_theta, lower = 0, upper = 1, \n          N = 5, y = 3)[[1]] |&gt; fractions()\n\n[1] 1/6\n\n\n\n\n\n\\(f(y)\\) is called marginal distribution of the data or, more aptly, prior predictive distribution\nIt tells us that prior to observing \\(y\\), all values of \\(y\\) are equally likely"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\n\n\n\n\n\n\n\nCompare with the data model:"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "href": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "title": "Bayesian Inference",
    "section": "Compute the Posterior",
    "text": "Compute the Posterior\n\n\n\n\nCompute the likelihood\n\n\\(\\binom{5}{3} \\theta^3 (1 - \\theta)^{2}\\) for \\(\\theta \\in \\{0.10, 0.30, 0.50, 0.70, 0.90\\}\\)\n\nMultiply the likelihood by the prior to compute the numerator\n\n\\(f(y = 3 \\mid \\theta) f(\\theta)\\)\n\nSum the numerator to get the marginal likelihood\n\n\\(f(y = 3) = \\sum_{\\theta} f(y = 3 | \\theta) f(\\theta) \\approx 0.2\\)\n\nFinally, compute the posterior\n\n\\(f(\\theta \\mid y = 3) = \\frac{f(y = 3 \\mid \\theta) f(\\theta)}{\\sum_{\\theta} f(y = 3 | \\theta) f(\\theta)}\\)\n\n\n\n\n\n\nN &lt;- 5; y &lt;- 3\ntheta &lt;- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior &lt;- c(0.05, 0.45, 0.30, 0.15, 0.05)\nlik &lt;- dbinom(y, N, theta)\nlik_x_prior &lt;-  lik * prior\nconstant &lt;- sum(lik_x_prior)\npost &lt;- lik_x_prior / constant\n\n\n\n\n\n\n\n\n\ntheta\nprior\nlik\nlik_x_prior\npost\n\n\n\n\n0.1\n0.05\n0.01\n0.00\n0.00\n\n\n0.3\n0.45\n0.13\n0.06\n0.29\n\n\n0.5\n0.30\n0.31\n0.09\n0.46\n\n\n0.7\n0.15\n0.31\n0.05\n0.23\n\n\n0.9\n0.05\n0.07\n0.00\n0.02\n\n\nTotal\n1.00\n0.83\n0.20\n1.00"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "title": "Bayesian Inference",
    "section": "Computing Event Probability",
    "text": "Computing Event Probability\n\n\n\n\nTo compute event probabilities, we integrate (or sum) the relevant regions of the parameter space \\[\n\\P(\\theta \\geq 0.5) = \\int_{0.5}^{1} f(\\theta \\mid y) \\, d\\theta\n\\]\nIn this case, we only have discrete quantities, so we sum:\n\n\n\n\nprobs &lt;- d |&gt;\n  filter(theta &gt;= 0.50) |&gt;\n  dplyr::select(prior, post) |&gt;\n  colSums() |&gt;\n  round(2)\nprobs\n\nprior  post \n 0.50  0.71 \n\n\n\n\n\n\n\n\n\n\ntheta\nprior\nlik\nlik_x_prior\npost\n\n\n\n\n0.1\n0.05\n0.01\n0.00\n0.00\n\n\n0.3\n0.45\n0.13\n0.06\n0.29\n\n\n0.5\n0.30\n0.31\n0.09\n0.46\n\n\n0.7\n0.15\n0.31\n0.05\n0.23\n\n\n0.9\n0.05\n0.07\n0.00\n0.02\n\n\nTotal\n1.00\n0.83\n0.20\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.5 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.71"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "href": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "title": "Bayesian Inference",
    "section": "What If We Used a Flat Prior?",
    "text": "What If We Used a Flat Prior?\n\n\nFlat or uniform prior means that we consider all values of \\(\\theta\\) equality likely\n\n\n\n\nN &lt;- 5; y &lt;- 3\ntheta &lt;- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior &lt;- c(0.20, 0.20, 0.20, 0.20, 0.20)\nlik &lt;- dbinom(y, N, theta)\nlik_x_prior &lt;-  lik * prior\nconstant &lt;- sum(lik_x_prior)\npost &lt;- lik_x_prior / constant\n\n\n\n\n\n\n\n\ntheta\nprior\nlik\nlik_x_prior\npost\n\n\n\n\n0.1\n0.2\n0.01\n0.00\n0.01\n\n\n0.3\n0.2\n0.13\n0.03\n0.16\n\n\n0.5\n0.2\n0.31\n0.06\n0.37\n\n\n0.7\n0.2\n0.31\n0.06\n0.37\n\n\n0.9\n0.2\n0.07\n0.01\n0.09\n\n\nTotal\n1.0\n0.83\n0.17\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.6 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.83"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\n\nGelman, A. et al. (2020). Bayesian Workflow. ArXiv:2011.01808 [Stat]. http://arxiv.org/abs/2011.01808"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "href": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "title": "Bayesian Inference",
    "section": "Overview of the class",
    "text": "Overview of the class\n\nSyllabus\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "href": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "title": "Bayesian Inference",
    "section": "Conjugate models and Beta-Binomial",
    "text": "Conjugate models and Beta-Binomial\n\n\nBayesian workflow\nBeta distribution\nGreat expectations\nTuning the prior model for the clinical trial analysis\nBinomial likelihood with continuous \\(\\theta\\)\nDeriving the conjugate posterior\nAnalysis of the sex ratio\nCompromise between priors and data model\nCoherence of Bayesian inference\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#homework",
    "href": "bayes-course/02-lecture/02-lecture.html#homework",
    "title": "Bayesian Inference",
    "section": "Homework",
    "text": "Homework\n\nHow did you find the homework?\nHomework review"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow",
    "href": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\n\n\n\nStatistics is like building a bridge — we first simulate the conditions under which our bridge should withstand various forces, then we build a bridge, and once built, we test it before letting people drive on it.\nPrior knowledge (not just prior distributions) here would be strength properties of concrete, optimal shape for the length, expected wind conditions, expected load of traffic, and maybe even expected momentum (mass * velocity) of an out-of-control tanker ramming into one of the supporting columns. The more important our “bridge,” the more testing we do."
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#bayesian-inference-vs-bayesian-workflow1",
    "href": "bayes-course/02-lecture/02-lecture.html#bayesian-inference-vs-bayesian-workflow1",
    "title": "Bayesian Inference",
    "section": "Bayesian Inference vs Bayesian Workflow1",
    "text": "Bayesian Inference vs Bayesian Workflow1\n\n\n\n\nBayesian inference is concerned with computing \\(f(\\theta \\mid y) \\propto f(\\theta) f(y \\mid \\theta)\\)\nBayesian workflow covers all aspects of data analysis:\n\nGather prior information\nBuild the initial model\nRun an inference algorithm (Bayesian inference)\nAdd model complexity, try a different model, perform model selection\nMake a decision\n\n\n\n\n\n\n\n\n\n\n\n\n\nGelman, A., Vehtari, A., Simpson, D., Margossian, C. C., Carpenter, B., Yao, Y., Kennedy, L., Gabry, J., Bürkner, P.-C., & Modrák, M. (2020). Bayesian Workflow. arXiv:2011.01808 [Stat]. http://arxiv.org/abs/2011.01808"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#why-do-we-need-workflow",
    "href": "bayes-course/02-lecture/02-lecture.html#why-do-we-need-workflow",
    "title": "Bayesian Inference",
    "section": "Why do we need workflow",
    "text": "Why do we need workflow\n\n\n\n\nFollowing the principle of engineering, we start with a simple model\nIt will likely fit the data (no guarantees) and will not take too long to run\nThe simple model will likely not be very good, but now we:\n\nHave a straw man against which better models can be evaluated\nCan code the full cycle of data selection, model build, model test, and model diagnostics\n\nWe gradually add components and rerun the tests\nIf data are large, we can subsample the data to test our model\n\n\n\n\n\n\n\n\n\n\nWe want to compare models as we add features\nWe calibrate this workflow, including a selection of the inference algorithm based on cost of being wrong"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#on-with-the-show",
    "href": "bayes-course/02-lecture/02-lecture.html#on-with-the-show",
    "title": "Bayesian Inference",
    "section": "On with the show…",
    "text": "On with the show…\n\n\n\n\nLast time, we presented the discretized version of the prior\nIn practice, we are typically working in continuous space\nIn general, the prior model can be arbitrarily complex\nThere is a family of distributions called the exponential family, for which the prior has the same kernel as the posterior\nThis is called conjugacy\nIn practice, we seldom rely on conjugate relationships\nBeta distribution is conjugate to Binomial"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "href": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "title": "Bayesian Inference",
    "section": "Continous Probability Distributions",
    "text": "Continous Probability Distributions\n\n\nRecall that for the continuous RVs we have, the CDF is defined as \\[\n\\begin{eqnarray}\nF(x) & = & \\int_{-\\infty}^{x} f(t) \\, \\text{d}t, \\, \\text{and} \\\\\nf(x) & = & \\frac{\\text{d}}{\\text{d}x}F(x), \\, \\text{where } F \\text{ is differentiable}\n\\end{eqnarray}\n\\]\nTo compute event probabilities: \\[\n\\begin{eqnarray}\n\\P(a \\le X \\leq b) & = & F(b) − F(a) = \\int_{a}^{b} f(x) \\, \\text{d}x \\\\\n\\P(X \\in A) & = & \\int_{A} f(x) \\, \\text{d}x\n\\end{eqnarray}\n\\]\nAs in the case of PMFs: \\[\n\\begin{eqnarray}\nf(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty} f(x) \\, \\text{d}x = 1\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "title": "Bayesian Inference",
    "section": "Introducing Beta",
    "text": "Introducing Beta\n\n\n\n\nBeta distribution has the following functional form for \\(\\alpha &gt; 0, \\, \\beta &gt; 0, \\, \\theta \\in (0, 1)\\) \\[\n\\begin{eqnarray}\n\\text{Beta}(\\theta \\mid \\alpha,\\beta) & = &\n\\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta^{\\alpha - 1} \\, (1 -\n\\theta)^{\\beta - 1} \\\\\n\\text{B}(a,b) \\ & = & \\ \\int_0^1 u^{a - 1} (1 - u)^{b - 1} \\,\n\\text{d}u \\ \\\\\n& = & \\ \\frac{\\Gamma(a) \\, \\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(x) & = &\n\\int_0^{\\infty} u^{x - 1} \\exp(-u) \\, \\text{d}u\n\\end{eqnarray}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nfactorial(0:9)\n\n [1]      1      1      2      6     24    120    720   5040  40320 362880\n\ngamma(1:10)\n\n [1]      1      1      2      6     24    120    720   5040  40320 362880\n\n\n\n\n\n\nFor positive integrers \\(n\\): \\(\\, \\Gamma(n+1) = n!\\) and in general \\(\\Gamma(z + 1) = z \\Gamma(z)\\)\n\\(\\text{Beta}(1, 1)\\) is equivalent to \\(\\text{Unif(0, 1)}\\). Why? (work it out)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "title": "Bayesian Inference",
    "section": "Visualizing Beta",
    "text": "Visualizing Beta\n\n\nThe mode of Beta, \\(\\text{Mode}(\\theta) = \\argmax_\\theta \\text{Beta}(\\theta \\mid \\alpha, \\beta)\\) is shown in red and the function maximum in blue"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#expectations",
    "title": "Bayesian Inference",
    "section": "Expectations",
    "text": "Expectations\n\n\nRecall the definition of expectations for continuous random variables\nWe often write \\(\\mu\\) or \\(\\mu_X\\) for expected value of \\(X\\) \\[\n\\mu_X = \\E(X) = \\int x \\cdot f(x) \\, \\text{d}x\n\\]\nVariance is a type of expectation, which we often denote by \\(\\sigma^2\\) \\[\n\\sigma_X = \\V(X) = \\E(X - \\mu)^2 = \\int (X - \\mu)^2 f(x) \\, \\text{d}x\n\\]\nIt is often more convenient to write the variance as \\[\n\\V(X) = \\E(X^2) - \\mu^2\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nKeeping in mind that a Gamma function is a continuous analog of the factorial:\n\n\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta) &=& \\int_{0}^{1} \\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\, \\text{d}\\theta \\\\\n&=& \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_{0}^{1}  \\color{red}{\\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1}} \\, \\text{d}\\theta \\\\\n\\end{eqnarray}\n\\]\n\n\n\\[\n\\begin{eqnarray}\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\cdot\n\\color{red}{\\frac{\\Gamma(1 + \\alpha) \\Gamma(\\beta)}{\\Gamma(1 + a + b)}} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{\\Gamma(1 + \\alpha)}{\\Gamma(1 + \\alpha + \\beta)} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{a \\Gamma(\\alpha)}{(\\alpha + \\beta) \\Gamma(\\alpha + \\beta)} \\\\\n&=& \\frac{\\alpha}{\\alpha + \\beta}\n\\end{eqnarray}\n\\]\n\n\n\nWhat is the value of \\(\\int_{0}^{1}  \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta} \\, d\\theta\\)\n\n\n\nCheck the integral \\(\\int_{0}^{1}  \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nWe can find the mode of this distribution by taking the log, differentiating with respect to \\(\\theta\\), and setting the derivative function to zero \\[\n\\begin{eqnarray}\n\\E(\\theta) & = & \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Mode}(\\theta) & = & \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\;\\; \\text{ when } \\; \\alpha, \\beta &gt; 1. \\\\\n\\end{eqnarray}\n\\]\nThe variance of \\(\\theta\\) can be derived using the definition of the variance operator \\[\n\\begin{eqnarray}\n\\V(\\theta) & = & \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{eqnarray}\n\\]\nNotice when \\(\\alpha = \\beta\\), \\(\\E(\\theta) = \\text{Mode}(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\n\n\nWe borrow an example from BDA3: the probability of girl birth given placenta previa (PP)\nPlacenta previa is a condition when the placenta completely or partially covers the opening of the uterus\nA PP study in Germany found that out of 980 births, 437 or \\(\\approx 45\\%\\) were female\nSex ratio in the population has been stable over space and time at 48.5% females with deviations within no more than 1%1\nOur task is to assess the evidence for \\(\\P(\\text{ratio} &lt; .485 \\mid \\text{PP})\\)\n\n\n\n\n\n\n\n\n\nGelman, A., & Weakliem, D. (2009). Of Beauty, Sex and Power. American Scientist, 97(4), 310. https://doi.org/10.1511/2009.79.310"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "href": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "title": "Bayesian Inference",
    "section": "Biomomial Likelihood",
    "text": "Biomomial Likelihood\n\n\nRecall that Likelihood is the function of the parameter \\(\\theta\\), assuming \\(\\theta \\in [0,1]\\) \\[\n\\text{Bin}(y \\mid \\theta) = \\binom{N}{y}\n\\theta^y (1 - \\theta)^{N - y} \\propto \\theta^y (1 - \\theta)^{N - y}\n\\]\nAssuming \\(N = 10\\), the likelihood for \\(\\theta\\), given a few possible values of \\(y\\) successes"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "href": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving the Posterior Distribution",
    "text": "Deriving the Posterior Distribution\n\n\nThe denominator is constant in \\(\\theta\\); we will derive the posterior up to a proportion \\[\n\\begin{eqnarray}\nf(y \\mid \\theta) & \\propto & \\theta^y (1 - \\theta)^{N - y} \\\\\nf(\\theta) & \\propto & \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\nf(\\theta \\mid y) & \\propto & f(y \\mid \\theta) f(\\theta) \\\\\n& = & \\theta^y (1 - \\theta)^{N - y} \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\n& = & \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\\\\nf(\\theta \\mid y) & = & \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\n\\end{eqnarray}\n\\]\n\\(f(\\theta)\\): \\(\\alpha - 1\\) prior successes and \\(\\beta - 1\\) prior failures\nThe last equality comes from matching the kernel \\(\\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1}\\) to the normalized Beta PDF which has a normalizing constant \\(\\frac{\\Gamma (\\alpha +\\beta + N)}{\\Gamma (\\alpha + y) \\Gamma (\\beta + N - y)}\\)\nSince the posterior is in the same family as the prior, we say that Beta is conjugate to Binomial\n\n\n\nCheck the kernel integral, \\(\\int_{0}^{1} \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "title": "Bayesian Inference",
    "section": "Posterior Expectations",
    "text": "Posterior Expectations\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta \\mid y)  & = & \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\cdot \\E(\\theta) + \\frac{n}{\\alpha + \\beta + n}\\cdot\\frac{y}{n} \\\\\n\\V(\\theta \\mid y)  & = & \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)} \\\\\n\\text{Mode}(\\theta \\mid y) & = & \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} = \\frac{\\alpha + \\beta - 2}{\\alpha + \\beta + n - 2} \\cdot\\text{Mode}(\\theta) + \\frac{n}{\\alpha + \\beta + n - 2} \\cdot\\frac{y}{n}\n\\end{eqnarray}\n\\]\n\n\n\nNote that the posterior expectation is between \\(y/n\\) and \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\nAlso note that as sample becomes very large \\(\\E(\\theta \\mid y) \\rightarrow \\frac{y}{n}\\) and \\(\\V(\\theta \\mid y) \\rightarrow 0\\)\nAs before, \\(\\text{Mode}(\\theta \\mid y) = \\E(\\theta \\mid y)\\), when \\(\\alpha = \\beta\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#variance-reduction-rate",
    "href": "bayes-course/02-lecture/02-lecture.html#variance-reduction-rate",
    "title": "Bayesian Inference",
    "section": "Variance Reduction Rate",
    "text": "Variance Reduction Rate\n\npost_v &lt;- function(a, b, y, n) {\n  ((a + y) * (b + n - y)) / ((a + b + n)^2 * (a + b + n + 1))\n}\nn &lt;- seq(10, 500, by = 2)\ny &lt;- 1:246\nv &lt;- post_v(1, 1, y = y, n = n)\nggplot(aes(n, v), data = data.frame(n, v)) +\n  geom_line() + ylab(\"Var\")"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\nLet’s derive the analytical posterior and compare it with the samples from the posterior distribution\nFirst, we will consider the uniform \\(\\text{Beta}(1, 1)\\) prior\nWe are told that data are \\(N = 980\\) and \\(y = 437\\) female births\nThe posterior is \\(\\text{Beta}(1 + 437, 1 + 980 - 437) = \\text{Beta}(438, 544)\\)\n\\(\\E(\\theta \\mid Y=437) = \\frac{\\alpha + 437}{\\alpha + \\beta + 980} = \\frac{438}{982} \\approx 0.446\\)\n\\(\\sqrt{\\V(\\theta \\mid Y=y)} \\approx\\) 0.016\n\n\n\nint &lt;- 0.95; l &lt;- (1 - int)/2; u &lt;- 1 - l\nupper &lt;- qbeta(u, 438, 544) |&gt; round(3)\nlower &lt;- qbeta(l, 438, 544) |&gt; round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.415, 0.477]\n\nevent_prob &lt;- integrate(dbeta, lower = 0, upper = 0.485, shape1 = 438, shape2 = 544)[[1]]\ncat(\"Probability that the ratio &lt; 0.485 under uniform prior =\", round(event_prob, 3))\n\nProbability that the ratio &lt; 0.485 under uniform prior = 0.993\n\npbeta(0.485, shape1 = 438, shape2 = 544) |&gt; round(3)\n\n[1] 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "href": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "title": "Bayesian Inference",
    "section": "Obtaining Quantiles by Sampling",
    "text": "Obtaining Quantiles by Sampling\n\n\nWe can use R’s rbeta() RNG to generate draws from the posterior\n\n\n\n\n\ndraws &lt;- rbeta(n = 1e5, 438, 544)\np &lt;- ggplot(aes(draws), data = data.frame(draws))\np + geom_histogram(bins = 30) + ylab(\"\") +\n  geom_vline(xintercept = 0.485, \n             colour = \"red\", size = 0.3) +\n  ggtitle(\"Draws from Beta(438, 544)\") + \n  theme(axis.text.y = element_blank()) + \n  xlab(expression(theta))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the quantile() function to get the posterior interval and compute the event probability by evaluating the expectation of the indicator function as before\n\n\n\n\nquantile(draws, probs = c(0.025, 0.5, 0.975)) |&gt; round(3)\n\n 2.5%   50% 97.5% \n0.415 0.446 0.477 \n\ncat(\"Probability that the ratio &lt; 0.485 under uniform prior =\", mean(draws &lt; 0.485) |&gt; round(3))\n\nProbability that the ratio &lt; 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "title": "Bayesian Inference",
    "section": "Population Priors",
    "text": "Population Priors\n\n\n\n\nWhat priors should we use if we think the sample is drawn from the sex ratio “hyper-population”?\nWe know that the population mean is 0.485 and the standard deviation is about 0.01\nBack out the parameters of the population Beta distribution\n\n\n\n\\[\n\\begin{eqnarray}\n\\begin{cases}\n\\frac{\\alpha}{\\alpha + \\beta} & = & 0.485 \\\\\n\\sqrt{\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}} & = & 0.01\n\\end{cases}\n\\end{eqnarray}\n\\]\n\n\n\n\\(\\alpha \\approx 1211\\) and \\(\\beta \\approx 1286\\)\n\n\n\n\n\nCheck the result with the simulation\n\n\n\n\nx &lt;- rbeta(1e4, 1211, 1286)\nmean(x) |&gt; round(3)\n\n[1] 0.485\n\nsd(x) |&gt; round(3)\n\n[1] 0.01\n\n\n\n\n\nWe can let Mathematica do the algebra"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "title": "Bayesian Inference",
    "section": "Posterior with Population Priors",
    "text": "Posterior with Population Priors\n\n\n\\(f(\\theta | y) = \\text{Beta}(1211 + 437, 1286 + 543) = \\text{Beta}(1648,1829)\\)\nWe can compare the prior and posterior using summarize_beta_binomial() in the bayesrules package\n\n\n\nlibrary(bayesrules)\nsummarize_beta_binomial(alpha = 1211, beta = 1286, y = 437, n = 980)\n\n      model alpha beta      mean      mode          var          sd\n1     prior  1211 1286 0.4849820 0.4849699 9.998978e-05 0.009999489\n2 posterior  1648 1829 0.4739718 0.4739568 7.168560e-05 0.008466735\n\n\n\n\n\nint &lt;- 0.95; l &lt;- (1 - int)/2; u &lt;- 1 - l\nupper &lt;- qbeta(u, 1648, 1829) |&gt; round(3)\nlower &lt;- qbeta(l, 1648, 1829) |&gt; round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.457, 0.491]\n\n\n\n\n\nevent_prob &lt;- pbeta(0.485, shape1 = 1648, shape2 = 1829)\ncat(\"Probability that the ratio &lt; 0.485 under population prior =\", round(event_prob, 3))\n\nProbability that the ratio &lt; 0.485 under population prior = 0.904\n\n\n\n\nUnder uniform prior 95% posterior interval was [0.415, 0.477]\nAnd probability that the ratio &lt; 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "title": "Bayesian Inference",
    "section": "Visualizing Bayesian Rebalancing",
    "text": "Visualizing Bayesian Rebalancing\n\n\nThe following uses \\(\\text{Beta}(5, 5)\\) prior and N = 10. Guess what color is likelihood, prior, and posterior."
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "href": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "title": "Bayesian Inference",
    "section": "Data Order and Batch Invariance",
    "text": "Data Order and Batch Invariance\n\n\nIn Bayesian analysis, we can update all at once or one datum at a time and everything in between and in any order (assuming exchangeable observations)\nThis is a general result, not just for Beta Binomial (see Section 4.5)\nIn practice, when we don’t have analytic posteriors, this is not so easy to do\nSuppose we observe \\(y = y_1 + y_2\\) and \\(N = N_1 + N_2\\) trials all at once\nFor all at once case, the posterior is in \\(f(\\theta \\mid y) = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\) as before\nNow, suppose we observe \\(y_1\\) successes in \\(n_1\\) trials first\nThe posterior is \\(f(\\theta \\mid y_1) = \\text{Beta}(\\alpha + y_1, \\,\\beta + N_1 - y_1)\\)\nWe now observe, \\(y_2\\) successes in \\(n_2\\) trials. The posterior is \\(f(\\theta \\mid y= y_1 + y_2) = \\text{Beta}(\\alpha + y_1 + y_2, \\,\\beta + N_1 - y_1 + N_2 - y_2)\\\\ = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#general-case",
    "href": "bayes-course/02-lecture/02-lecture.html#general-case",
    "title": "Bayesian Inference",
    "section": "General Case",
    "text": "General Case\n\n\nSuppose we observe data point \\(y_1\\), and then data point \\(y_2\\)1\n\n\n\n\\[\n\\begin{eqnarray}\nf(\\theta \\mid y_1) &=&  \\frac{f(\\theta)f(y_1 \\mid \\theta)}{f(y_1)} \\\\\nf(\\theta \\mid y_2) &=&  \\frac{\\frac{f(\\theta)f(y_1 \\mid \\theta)}{f(y_1)} f(y_2 \\mid \\theta)}{f(y_2)} \\\\\n&=& \\frac{f(\\theta)f(y_1 \\mid \\theta) f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\end{eqnarray}\n\\]\n\n\n\nObserving data point \\(y_2\\), and then data point \\(y_1\\), will results in the same distribution\nWhat if we observed both points at once?\n\n\n\n\\[\n\\begin{split}\nf(\\theta \\mid y_1,y_2)\n& = \\frac{f(\\theta)f(y_1,y_2 \\mid \\theta)}{f(y_1)f(y_2)} \\\\\n& = \\frac{f(\\theta)f(y_1 \\mid \\theta)f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\end{split}\n\\]\n\nThis comes directly from Bayes Rules!: 4.5 Proving data order invariance"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#the-myth-of-objectivity-in-science",
    "href": "bayes-course/02-lecture/02-lecture.html#the-myth-of-objectivity-in-science",
    "title": "Bayesian Inference",
    "section": "The myth of objectivity in science",
    "text": "The myth of objectivity in science\n\n\nAll of science requires choices, and those choices are, by definition, subjective\nNeither Frequentist nor Bayesian inference imbues objectivity into your analysis\nAll priors are subjective, including no priors, which don’t exist (uniform prior on the real line is not “no prior”)\nRemember that important scientific discoveries were made before Statistics became a thing\nA better alternative to objective vs subjective debate:\n\n\n\n\n…objectivity replaced by transparency, consensus, impartiality, and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence1\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html\n\n\nGelman, A., & Hennig, C. (2015). Beyond subjective and objective in statistics. arXiv:1508.05453 [Stat]. http://arxiv.org/abs/1508.05453"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-regression-and-introduction-to-hierarchical-models",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-regression-and-introduction-to-hierarchical-models",
    "title": "Bayesian Inference",
    "section": "Logistic Regression and Introduction to Hierarchical Models",
    "text": "Logistic Regression and Introduction to Hierarchical Models\n\n\nGLMs and logistic regression\nUnderstanding logistic regression\nSimulating data\nPrior predictive simulations\nExample: Diabetes in Pima native americans\nIntroducing hierarchical/multi-level models\nPooling: none, complete, and partial\nExample hierarchical model\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logit-and-inverse-logit",
    "href": "bayes-course/07-lecture/07-lecture.html#logit-and-inverse-logit",
    "title": "Bayesian Inference",
    "section": "Logit and Inverse Logit",
    "text": "Logit and Inverse Logit\n\n\nIn logistic regression, we have to map from probability space to the real line and from the real line back to probability\nLogistic function (log odds) achieves the former: \\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\n\\]\nInverse logit achieves the latter: \\[\n\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}\n\\]\nNotice that \\(\\text{logit}^{-1}(5)\\) is very close to 1 and \\(\\text{logit}^{-1}(-5)\\) is very close to 0\nIn R, you can set logit &lt;- qlogis and invlogit &lt;- plogis"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#glms-and-models-for-01-outcomes",
    "href": "bayes-course/07-lecture/07-lecture.html#glms-and-models-for-01-outcomes",
    "title": "Bayesian Inference",
    "section": "GLMs and Models For 0/1 Outcomes",
    "text": "GLMs and Models For 0/1 Outcomes\n\n\nModeling a probability of an event can be framed in the GLM context (just like with counts)\nThe general setup is that we have:\n\nResponse vector \\(y\\) consisting of zeros and ones\nThe data model is \\(y_i \\sim \\text{Bernoulli}(p_i)\\)\nLinear predictor: \\(\\eta_i = \\alpha + X_i\\beta\\), where \\(X\\) is a matrix\nIn general: \\(\\E(y \\mid X) = g^{-1}(\\eta)\\), where \\(g^{-1}\\)is the inverse link function that maps the linear predictor onto the observational scale\nIn particular: \\(\\E(y_i \\mid X_i) = \\text{logit}^{-1}(\\alpha + X_i\\beta) = p_i = \\P(y_i = 1)\\)\n\n\\(\\text{logit}(p_i) = \\eta_i\\) and \\(p_i = \\text{logit}^{-1}(\\eta_i)\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-posterior",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-posterior",
    "title": "Bayesian Inference",
    "section": "Logistic Posterior",
    "text": "Logistic Posterior\n\n\nTo derive the posterior distribution for Poisson, we consider K regression inputs and independent priors on all \\(K + 1\\) unknowns: \\(\\alpha\\) and \\(\\beta_1, \\beta_2, ..., \\beta_k\\)\nBernoulli likelihood is: \\(f(y \\mid p) = p^y (1 - p)^{1-y}\\) with \\(y \\in \\{0, 1\\}\\)\nAnd each \\(p_i = \\text{logit}^{-1}(\\alpha + X_i\\beta)\\) \\[\nf\\left(\\alpha,\\beta \\mid y,X\\right) \\propto\nf_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot \\\\\n\\prod_{i=1}^N \\left(\\text{logit}^{-1}(\\alpha + X_i\\beta) \\right)^{y_i} \\left(1 - \\text{logit}^{-1}(\\alpha + X_i\\beta)\\right)^{1-y_i}\n\\]\nIn Stan, the likelihood term can be written on a log scale as y ~ bernoulli_logit_glm(x, alpha, beta) or bernoulli_logit_glm_lupmf(y | x, alpha, beta)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-simulation",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-simulation",
    "title": "Bayesian Inference",
    "section": "Logistic Simulation",
    "text": "Logistic Simulation\n\n\nAs before, we can forward simulate data for logistic regression\nWe will fit the data and try to recover the parameters\n\n\n\n\n\nset.seed(123)\nlogit &lt;- qlogis; invlogit &lt;- plogis\nn &lt;- 100\na &lt;- 1.2\nb &lt;- 0.4\nx &lt;- runif(n, -15, 10)\neta &lt;- a + x * b\nPr &lt;- invlogit(eta)\ny &lt;- rbinom(n, 1, Pr)\nsim &lt;- tibble(y, x, Pr)\n\np &lt;- ggplot(aes(x, y), data = sim)\np &lt;- p + geom_point(size = 0.5) +\n  geom_line(aes(x, Pr), linewidth = 0.2) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 0.2, \n             linetype = \"dashed\", alpha = 1/3) +\n  geom_hline(yintercept = invlogit(a), color = \"red\", linewidth = 0.2, \n             linetype = \"dashed\", alpha = 1/3) +\n  geom_hline(yintercept = 0.50, linewidth = 0.2, linetype = \"dashed\", alpha = 1/3) +\n  ggtitle(TeX(\"$y_i \\\\sim Bernoulli(logit^{-1}(1.2 + 0.4x_i))$\")) +\n  annotate(\"text\", x = -5.5, y = invlogit(a) - 0.02,\n           label = TeX(\"Intercept = $logit^{-1}(1.2)$ \\\\approx 0.77\")) +\n  annotate(\"text\", x = -8, y = 0.53,\n           label = TeX(\"$slope_{.5} = \\\\frac{0.4}{4} = 0.10$\")) +\n  ylab(TeX(\"$logit^{-1}(1.2 + 0.4x)$\")); print(p)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#interpreting-logistic-coefficients",
    "href": "bayes-course/07-lecture/07-lecture.html#interpreting-logistic-coefficients",
    "title": "Bayesian Inference",
    "section": "Interpreting Logistic Coefficients",
    "text": "Interpreting Logistic Coefficients\n\n\n\n\nThe intercept is the log odds of an event when \\(x = 0\\), \\(\\text{logit}^{-1}(1.2) = 0.77\\)\nThe slope changes depending on where you are on the curve\nWhen you are near 0.50, the slope of logit(x) is 1/4 and so you can divide your coefficient by 4 to get a rough estimate\nThis implies that if we go from \\(x = -3\\) to \\(x = -2\\), the probability will increase by about 0.10\n\n\n\n\n(invlogit(1.2 + 0.4 * -2) - \n   invlogit(1.2 + 0.4 * -3)) |&gt; \n  round(2)\n\n[1] 0.1"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#fitting-simulated-data",
    "href": "bayes-course/07-lecture/07-lecture.html#fitting-simulated-data",
    "title": "Bayesian Inference",
    "section": "Fitting Simulated Data",
    "text": "Fitting Simulated Data\n\n\nComplex and non-linear models may have a hard time recovering parameters from forward simulations\nThe process for fitting simulated data may give some insight into the data-generating process and priors\n\n\n\n\n\n# fitting from eta = 1.2 +  0.4 * x\nm1 &lt;- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = binomial(link = \"logit\"), \n               data = sim,\n               chains = 4,\n               refresh = 0,\n               iter = 1000)\nsummary(m1)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      y ~ x\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 1.5    0.5  0.9   1.5   2.1  \nx           0.5    0.1  0.4   0.5   0.7  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.5    0.0  0.5   0.5   0.6  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1132 \nx             0.0  1.0  1106 \nmean_PPD      0.0  1.0  1584 \nlog-posterior 0.0  1.0   817 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#generating-probability-data",
    "href": "bayes-course/07-lecture/07-lecture.html#generating-probability-data",
    "title": "Bayesian Inference",
    "section": "Generating Probability Data",
    "text": "Generating Probability Data\n\n\nTo get a sense of the variability in probability we can simulate from the prior distribution on the probability scale\n\n\n\n\n\nprior_pred_logit &lt;- function(x) {\n  a &lt;- rnorm(1, mean = 1.2, sd = 0.5)\n  b &lt;- rnorm(1, mean = 0.4, sd = 0.1)\n  Pr &lt;- invlogit(a + b * x)\n  return(Pr)\n}\nprior_pred &lt;- replicate(50, prior_pred_logit(x)) |&gt;\n  as.data.frame()\n\ndf_long &lt;- prior_pred |&gt;\n  mutate(x = x) |&gt;\n  pivot_longer(cols = -x, names_to = \"line\", values_to = \"y\")\n\np &lt;- ggplot(aes(x, y), data = df_long)\np + geom_line(aes(group = line), linewidth = 0.2, alpha = 1/5) +\n  geom_line(aes(y = Pr), data = sim, linewidth = 0.5, color = 'red') +\n  ylab(TeX(\"$logit^{-1}(\\\\alpha + \\\\beta x)$\")) +\n  ggtitle(TeX(\"Simulating from prior $logit^{-1}(\\\\alpha + \\\\beta x_i))$\"),\n  subtitle = TeX(\"$\\\\alpha \\\\sim Normal(1.2, 0.5)$ and $\\\\beta \\\\sim Normal(0.4, 0.1)$\"))"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThis example comes from US National Institute of Diabetes and Digestive and Kidney Diseases from a population of women who were at least 21 years old, of Pima Indian heritage, and living near Phoenix, Arizona\nIt is available as part of the R package pdp\nThe outcome \\(diabetes\\), is an indicator of the disease\nSome other variables are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npregnant\nglucose\npressure\ntriceps\ninsulin\nmass\npedigree\nage\ndiabetes\n\n\n\n\n6\n148\n72\n35\nNA\n33.6\n0.627\n50\npos\n\n\n1\n85\n66\n29\nNA\n26.6\n0.351\n31\nneg\n\n\n8\n183\n64\nNA\nNA\n23.3\n0.672\n32\npos\n\n\n1\n89\n66\n23\n94\n28.1\n0.167\n21\nneg\n\n\n0\n137\n40\n35\n168\n43.1\n2.288\n33\npos\n\n\n5\n116\n74\nNA\nNA\n25.6\n0.201\n30\nneg"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#missing-value-imputation",
    "href": "bayes-course/07-lecture/07-lecture.html#missing-value-imputation",
    "title": "Bayesian Inference",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\n\n\n\n\nThis dataset contains missing values\nPeople typically either delete them or replace them with average values, or something like that\nNone of these are good approaches\nIn R, we recommend a combination of the mice package and brms, which works nicely with mice\nFrom a Bayesian perspective, a missing value is just another unknown parameter in the model, which can be modeled\n\n\n\n\n\nFollowing is an example of a simple missing value estimation from the Stan manual\n\n\n\n\ndata {\n  int&lt;lower=0&gt; N_obs;\n  int&lt;lower=0&gt; N_mis;\n  array[N_obs] real y_obs;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n  array[N_mis] real y_mis;\n}\nmodel {\n  y_obs ~ normal(mu, sigma);\n  y_mis ~ normal(mu, sigma);\n}\n\n\n\n\nNotice, that you need to make an assumption about the model for missing values"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-1",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-1",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\nCodePlot\n\n\n\n\nIt is well-known that people with high BMI are at risk for Type 2 diabetes\nWe can do some exploratory analysis to check this\n\n\n\n\np1 &lt;- ggplot(pima, aes(x = mass)) + \n  geom_density(aes(group = diabetes, fill = diabetes, color = diabetes), alpha = 1/5) +\n  xlab(\"BMI\")\np2 &lt;- pima |&gt;\n  drop_na() |&gt;\n  mutate(bmi_cut = cut(mass, breaks = seq(15, 70, by = 5))) |&gt;\n  group_by(bmi_cut) |&gt;\n  summarize(p = mean(diabetes == \"pos\"),\n            n = n(),\n            se = sqrt(p * (1 - p) / n),\n            lower = p + se,\n            upper = p - se) |&gt;\n  ggplot(aes(x = bmi_cut, y = p)) + \n  geom_point() + geom_linerange(aes(ymin = lower, ymax = upper), linewidth = 0.2) +\n  xlab(\"BMI range\") + ylab(\"Proportion\") +\n  ggtitle(\"Proportion of diabetics by BMI +/- 1 SE\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-2",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-2",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nBefore building a model, we should scale the variables\n\n\n\n\nlibrary(pdp)\nd &lt;- pima |&gt;\n  as_tibble() |&gt;\n  select(diabetes, age, pedigree, mass, glucose) |&gt;\n  drop_na() |&gt; # in an important analysis, you should not do this; instead, impute\n  mutate(diab = if_else(diabetes == \"pos\", 1, 0),\n         age = (age - mean(age)) / 10,\n         pedigree = (pedigree - mean(pedigree)),\n         bmi = ((mass - mean(mass)) / 10),\n         glucose = ((glucose - mean(glucose)) / sd(glucose))) \nhead(d)\n\n# A tibble: 6 × 7\n  diabetes     age pedigree  mass glucose  diab    bmi\n  &lt;fct&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 pos       1.67      0.154  33.6   0.852     1  0.115\n2 neg      -0.231    -0.122  26.6  -1.21      0 -0.585\n3 pos      -0.131     0.199  23.3   2.00      1 -0.915\n4 neg      -1.23     -0.306  28.1  -1.08      0 -0.435\n5 pos      -0.0312    1.81   43.1   0.492     1  1.06 \n6 neg      -0.331    -0.272  25.6  -0.194     0 -0.685"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-3",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-3",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe will be fitting the following statistical model, where \\(x\\) is the BMI and \\(\\bar x\\) is the average BMI in the sample\nWe divide by ten so that a unit increase in BMI is a meaningful change\nWe need to pick priors on \\(\\alpha\\), and \\(\\beta\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta \\left( \\frac{x_{i} - \\bar x}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(\\mu_\\alpha ,\\ \\sigma_\\alpha) \\\\\n\\beta &\\sim& \\text{Normal}(\\mu_\\beta , \\ \\sigma_\\beta)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-4",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-4",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThe prior on the intercept corresponds to the log odds of developing diabetes when a person has an average BMI, which is about 29 in the US (which is considered high-risk)\nIt is likely that the probability is between 20% and 80%, and so a weakly informative prior can be expressed as \\(\\text{Normal}(0, 0.5)\\), since invlogit(c(-1.5, 1.5)) = [0.18, 0.82]\nSuppose, we also put a weakly informative \\(\\text{Normal}(0, 1)\\) pior on \\(\\beta\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta \\left( \\frac{x_{i} - \\bar x}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(0 ,\\ 0.5) \\\\\n\\beta &\\sim& \\text{Normal}(0 , \\ 1)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-5",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-5",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nLet’s perform prior predictive simulation\n\n\n\n\n\nm_prior &lt;- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0, 1), \n               family = binomial(link = \"logit\"), \n               refresh = 100,\n               prior_PD = TRUE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\n\nsummary(m_prior)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  0.0    0.5 -0.6   0.0   0.6 \nbmi          0.0    1.0 -1.3   0.0   1.3 \n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1215 \nbmi           0.0  1.0  1108 \nlog-posterior 0.0  1.0   932 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-6",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-6",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nd |&gt;\n  add_epred_draws(m_prior, ndraws = 100) |&gt;\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\") +\n  ggtitle(\"Possible Logit Curves Implied by the Prior\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-7",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-7",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThe negative associations are implausible given everything we know about diabetes\nSuppose the risk of diabetes doubles for every 10 \\(kg/m^2\\) (unverified)\nThat would imply average \\(\\beta \\approx 0.7\\), since the multiplicative change in odds is \\(e^{0.7} \\approx 2\\)\nWe will set the standard deviation to 0.2 to avoid negative effects and allow the odds to be as high as 3.5\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta_1 \\left( \\frac{x_{i} - \\bar x}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(0 ,\\ 0.5) \\\\\n\\beta &\\sim& \\text{Normal}(0.7 , \\ 0.2)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-8",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-8",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWith new priors, let’s repeat the prior predictive simulation\n\n\n\n\n\nm_prior &lt;- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0.7, 0.2), \n               family = binomial(link = \"logit\"), \n               refresh = 100,\n               prior_PD = TRUE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m_prior)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  0.0    0.5 -0.6   0.0   0.6 \nbmi          0.7    0.2  0.4   0.7   1.0 \n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1215 \nbmi           0.0  1.0  1108 \nlog-posterior 0.0  1.0   932 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-9",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-9",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nd |&gt;\n  add_epred_draws(m_prior, ndraws = 100) |&gt;\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\") +\n  ggtitle(\"Possible Logit Curves Implied by the Prior\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-10",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-10",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWith this new prior we are ready to fit the model\n\n\n\n\n\nm2 &lt;- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0.7, 0.2), \n               family = binomial(link = \"logit\"), \n               prior_PD = FALSE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m2)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.6    0.1 -0.7  -0.6  -0.5 \nbmi          0.9    0.1  0.8   0.9   1.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1315 \nbmi           0.0  1.0  1299 \nmean_PPD      0.0  1.0  1651 \nlog-posterior 0.0  1.0   780 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-11",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-11",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThere are no sampling problems, but we should still check the diagnostics\n\n\n\n\nlibrary(bayesplot)\np1 &lt;- mcmc_trace(m2)\np2 &lt;- mcmc_acf(m2)\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-12",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-12",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can examine the posterior probability of diabetes among this population (women who were at least 21 years old, and of Pima Indian heritage)\n\n\n\n\nd |&gt;\n  add_epred_draws(m2, ndraws = 100) |&gt;\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-13",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-13",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can also see the posterior predictive distribution of the probability of diabetes\n\n\n\n\nprop_diab &lt;- function(x) {\n  mean(x == 1)\n}\npp_check(m2, nreps = 100,\n         plotfun = \"stat\", stat = \"prop_diab\") + xlab(\"Probability of Diabetes\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-14",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-14",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nAs before, we can compute predictions for new data using posterior_predict or by constructing the posterior distribution directly\nLet’s say we want to predict the probability of diabetes for a person with BMI = 40\n\n\n\n\nbmi_scaled &lt;- (40 - mean(d$mass)) / 10\nyepred_m2 &lt;- posterior_epred(m2, newdata = data.frame(bmi = bmi_scaled))\nquantile(yepred_m2, probs = c(0.05, 0.50, 0.95)) |&gt; round(2)\n\n  5%  50%  95% \n0.47 0.51 0.56 \n\nd_m2 &lt;- as_tibble(m2) |&gt;\n  mutate(log_odds = `(Intercept)` + bmi * bmi_scaled,\n         prob = invlogit(log_odds),\n         ypred = rbinom(2e3, size = 1, prob = prob))\nd_m2[1:3, ]\n\n# A tibble: 3 × 5\n  `(Intercept)`   bmi  log_odds  prob ypred\n          &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1        -0.676 0.897  0.000969 0.500     1\n2        -0.746 0.936 -0.0403   0.490     0\n3        -0.682 0.957  0.0402   0.510     0\n\nquantile(d_m2$prob, probs = c(0.05, 0.50, 0.95)) |&gt; round(2)\n\n  5%  50%  95% \n0.47 0.51 0.56"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-15",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-15",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe will extend this model and evaluate the model performance\nHere we set weakly informative priors on the other three coefficients\n\n\n\n\n\npriors &lt;- normal(location = c(0.7, 0, 0, 0), \n                 scale = c(0.2, 1, 1, 1))\nm3 &lt;- stan_glm(diab ~ bmi + pedigree + \n                 age + glucose,\n               prior_intercept = normal(0, 0.5),\n               prior = priors,\n               family = binomial(link = \"logit\"), \n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m3)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi + pedigree + age + glucose\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   5\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.8    0.1 -0.9  -0.8  -0.7 \nbmi          0.8    0.1  0.7   0.8   1.0 \npedigree     0.8    0.3  0.5   0.8   1.2 \nage          0.3    0.1  0.2   0.3   0.4 \nglucose      1.1    0.1  0.9   1.1   1.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2663 \nbmi           0.0  1.0  2623 \npedigree      0.0  1.0  2128 \nage           0.0  1.0  2551 \nglucose       0.0  1.0  2861 \nmean_PPD      0.0  1.0  2566 \nlog-posterior 0.1  1.0   747 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#shinystan-demo",
    "href": "bayes-course/07-lecture/07-lecture.html#shinystan-demo",
    "title": "Bayesian Inference",
    "section": "ShinyStan Demo",
    "text": "ShinyStan Demo\n\nTo use install.packages(\"shinystan\") and launch_shinystan(m3)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#making-additional-improvements",
    "href": "bayes-course/07-lecture/07-lecture.html#making-additional-improvements",
    "title": "Bayesian Inference",
    "section": "Making Additional Improvements",
    "text": "Making Additional Improvements\n\n\nThere is a good reason to believe that glucose and heredity interact so we will include an interaction term\nAge effects are rarely linear and so we include a B-Spline for non-linear age effects\n\n\n\n\n\nlibrary(splines)\npriors &lt;- normal(location = c(0.7, rep(0, 7)), \n                 scale = c(0.2, rep(1, 7)))\nm4 &lt;- stan_glm(diab ~ bmi + pedigree + \n                 bs(age, df = 4) + \n                 glucose + glucose:pedigree,\n               prior_intercept = normal(0, 0.5),\n               prior = priors,\n               family = binomial(link = \"logit\"), \n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m4)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi + pedigree + bs(age, df = 4) + glucose + glucose:pedigree\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   9\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      -1.6    0.3 -2.0  -1.6  -1.2 \nbmi               0.8    0.1  0.6   0.8   0.9 \npedigree          0.9    0.3  0.6   0.9   1.3 \nbs(age, df = 4)1  0.3    0.4 -0.2   0.3   0.8 \nbs(age, df = 4)2  2.6    0.6  1.9   2.6   3.3 \nbs(age, df = 4)3  0.7    0.7 -0.2   0.7   1.6 \nbs(age, df = 4)4 -0.6    0.8 -1.6  -0.6   0.4 \nglucose           1.1    0.1  1.0   1.1   1.3 \npedigree:glucose -0.6    0.3 -0.9  -0.6  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  1643 \nbmi              0.0  1.0  2228 \npedigree         0.0  1.0  2015 \nbs(age, df = 4)1 0.0  1.0  1758 \nbs(age, df = 4)2 0.0  1.0  1999 \nbs(age, df = 4)3 0.0  1.0  1853 \nbs(age, df = 4)4 0.0  1.0  2086 \nglucose          0.0  1.0  1749 \npedigree:glucose 0.0  1.0  1633 \nmean_PPD         0.0  1.0  1912 \nlog-posterior    0.1  1.0   960 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-16",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-16",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can perform model comparison using several methods\nOne way is to assess classification accuracy under different probability cut points, which is often done in Machine Learning (ROC/AUC)\nA better way is to use LOO (loo and loo_compare in R)\n\n\n\n\n\npar(mar = c(3,3,2,1), \n    mgp = c(2,.7,0), \n    tck = -.01, \n    bg  = \"#f0f1eb\")\nm2_loo &lt;- loo(m2)\nm3_loo &lt;- loo(m3)\nm4_loo &lt;- loo(m4)\nloo_compare(m2_loo, m3_loo, m4_loo)\n\n\n   elpd_diff se_diff\nm4    0.0       0.0 \nm3  -15.6       4.6 \nm2 -104.5      12.4 \n\nplot(m4_loo)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#introduction-to-hierarchical-models",
    "href": "bayes-course/07-lecture/07-lecture.html#introduction-to-hierarchical-models",
    "title": "Bayesian Inference",
    "section": "Introduction to Hierarchical Models",
    "text": "Introduction to Hierarchical Models\n\n\nYou can think about hierarchical (sometimes called multi-level or mixed effects) models from a data or parameter perspective, although the parameter view is more fundamental\nThe basic idea is that want to model all sources of potential variability in the data\nData often arrive in clusters, such as students within schools, patients with hospitals, voters within states, and so on\nSince there are local influences, it is often a good idea to assume that units share similar attributes within clusters as well as between clusters\nThese two sources of variability are likely different and we should treat them as such"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nLet’s look at a classic dataset called sleepstudy from lme4 package\nThese data are from the study described in Belenky et al. (2003), for the most sleep-deprived group (3 hours time-in-bed) and for the first 10 days of the study, up to the recovery period\n\n\n\n\np &lt;- ggplot(aes(Days, Reaction), data = lme4::sleepstudy)\np + geom_point(size = 0.3) + geom_line(linewidth = 0.1) + facet_wrap(vars(Subject)) + ylab(\"Reaction time (ms)\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-1",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-1",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nTo get a better sense of the differences in per-Subject reaction time distributions, we can plot them side by side"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#pooling",
    "href": "bayes-course/07-lecture/07-lecture.html#pooling",
    "title": "Bayesian Inference",
    "section": "Pooling",
    "text": "Pooling\n\n\nPooling has to do with how much regularization we induce on parameter estimates in each cluster; sometimes this is called shrinkage\nComplete pooling ignores the clusters and estimates a global parameter\nEven though reaction time \\(y_i\\), belongs to subject \\(j\\), we ignore the groups and index all the \\(y\\)s together"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#pooling-1",
    "href": "bayes-course/07-lecture/07-lecture.html#pooling-1",
    "title": "Bayesian Inference",
    "section": "Pooling",
    "text": "Pooling\n\n\nComplete pooling is the opposite of no pooling, where we estimate a separate model for each group\nHere, we have \\(n\\) subjects, and \\(y_{ij}\\) refers to the \\(i\\)th reaction time in subject \\(j\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#partial-pooling",
    "href": "bayes-course/07-lecture/07-lecture.html#partial-pooling",
    "title": "Bayesian Inference",
    "section": "Partial Pooling",
    "text": "Partial Pooling\n\n\nPartial pooling is the compromise between the two extremes\nLike any other parameter in a Bayesian model, the global hyperparameter \\(\\tau\\) is given a prior and is learned from the data\nThere could be multiple levels of nesting, say students within schools, within states, etc."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#partial-pooling-compromise",
    "href": "bayes-course/07-lecture/07-lecture.html#partial-pooling-compromise",
    "title": "Bayesian Inference",
    "section": "Partial Pooling Compromise",
    "text": "Partial Pooling Compromise\n\n\nPosterior \\(f(\\theta \\mid y)\\) is is a compromise between prior \\(f(\\theta)\\) and likelihood \\(f(y \\mid \\theta)\\)\nIn the same spirit, the pooled parameter \\(\\theta_j\\) (say reaction time for subject \\(j\\)) is a compromise between within-subject parameters, and among-subject parameters\n\n\n\n\\[\n\\theta_j \\approx \\frac{\\frac{n_j}{\\sigma_{y}^2} \\overline y_j + \\frac{1}{\\sigma_{\\tau}^2} \\overline y_{\\tau}} {\\frac{n_j}{\\sigma_{y}^2} + \\frac{1}{\\sigma_{\\tau}^2}}\n\\]\n\n\n\n\\(\\overline y_j\\) is no-pool estimate of average reaction time for subject \\(j\\), and \\(\\overline y_{\\tau}\\) is complete-pool estimate\n\\(n_j\\) is the number of observations for subject \\(j\\), \\(\\sigma_{y}^2\\) is within subject variance of reaction times, and \\(\\sigma_{\\tau}^2\\) is the between-subject variance"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-2",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-2",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nLet’s build three models for the sleep data, starting with complete pooling, which is what we have been doing all along\nWe will start with the intercept-only model\nThese data contain the same number of observations per Subject, which is unusual, so we will make it more realistic by removing 30% of the measurements\n\n\n\n\n\nset.seed(123)\nn &lt;- nrow(lme4::sleepstudy)\ns &lt;- sample(1:n, n * 0.7)\nd &lt;- lme4::sleepstudy[s, ] \np1 &lt;- ggplot(aes(Days, Reaction), \n            data = d)\np1 &lt;- p1 + geom_jitter(size = 0.3, width = 0.2)\np2 &lt;- ggplot(aes(Reaction), \n            data = d)\np2 &lt;- p2 + geom_histogram(binwidth = 20)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-3",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-3",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nThis is a simple model of the mean reaction time \\(\\mu\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} & \\sim & \\text{Normal}(\\mu, \\ \\sigma^2) \\\\\n\\mu    & \\sim & \\text{Normal}(300, 10^2) \\\\\n\\sigma & \\sim & \\text{Exponential}(0.02)\n\\end{eqnarray}\n\\]\n\n\n\n\nm1 &lt;- stan_glm(Reaction ~ 1,\n               prior_intercept = normal(300, 10),\n               prior_aux = exponential(0.02),\n               family = gaussian,\n               data = d, \n               iter = 5000,\n               refresh = 0,\n               seed = 123)\nsummary(m1)\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Reaction ~ 1\n algorithm:    sampling\n sample:       10000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n predictors:   1\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept) 294.5    4.2 289.2 294.5 299.9\nsigma        52.9    3.4  48.7  52.7  57.3\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 294.4    6.3 286.4 294.4 302.5\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  6347 \nsigma         0.0  1.0  6890 \nmean_PPD      0.1  1.0  8264 \nlog-posterior 0.0  1.0  4152 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-4",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-4",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can compare the predictions from the complete-pooling model to the reaction times of each subject\n\n\n\n\nnew &lt;- d |&gt; group_by(Subject) |&gt; summarise(Reaction = mean(Reaction)) |&gt;\n  arrange(Reaction)\nnew_pred &lt;- posterior_predict(m1, newdata = new)\nppc_intervals(new$Reaction, yrep = new_pred) +\n  scale_x_continuous(labels = new$Subject,  breaks = 1:nrow(new)) +\n  xlab(\"Subject\") + ylab(\"Reaction\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-5",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-5",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nNext, we will fit a no-pool model, which means we are fitting 18 separate models, one for each subject\nYou can fit them separately, by making 18 calls to stan_glm, or you can do in all at once\nSince we will not run 18 separate regressions, it will be simpler to estimate one global variance parameter \\(\\sigma\\), which means there is some variance pooling\nThis will not affect the inferences for \\(\\mu_j\\) which is our main focus here\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} & \\sim & \\text{Normal}(\\mu_j, \\ \\sigma^2) \\\\\n\\mu_j    & \\sim & \\text{Normal}(300, s_j^2) \\\\\n\\sigma & \\sim & \\text{Exponential}(0.02)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-6",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-6",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nm2 &lt;- stan_glm(Reaction ~ Subject - 1,\n               prior = normal(300, 10, autoscale = TRUE),\n               prior_aux = exponential(0.02),\n               family = gaussian,\n               data = d, \n               iter = 1000,\n               refresh = 0,\n               seed = 123)\nsummary(m2)\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Reaction ~ Subject - 1\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n predictors:   18\n\nEstimates:\n             mean   sd    10%   50%   90%\nSubject308 353.4   17.1 331.0 353.8 375.1\nSubject309 213.0   14.6 194.4 212.8 231.8\nSubject310 223.9   12.9 207.0 224.0 240.2\nSubject330 305.2   12.7 288.6 305.1 321.6\nSubject331 310.0   15.3 290.1 310.2 329.3\nSubject332 272.0   15.2 252.6 271.8 291.7\nSubject333 306.1   16.9 284.3 306.0 327.7\nSubject334 296.9   12.2 281.1 297.1 312.5\nSubject335 248.6   13.2 231.4 248.8 265.1\nSubject337 359.4   14.3 341.2 359.4 378.3\nSubject349 282.0   15.6 262.2 282.0 301.1\nSubject350 336.6   14.5 317.6 336.7 354.8\nSubject351 280.5   18.3 256.7 280.8 303.6\nSubject352 346.1   16.5 325.2 345.9 367.1\nSubject369 294.7   14.3 276.4 294.6 313.6\nSubject370 262.1   14.9 243.7 262.2 281.4\nSubject371 295.4   11.7 280.3 295.2 309.9\nSubject372 317.6   11.9 302.7 317.6 333.0\nsigma       37.4    2.6  34.4  37.2  40.7\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.2    4.8 287.1 293.2 299.2\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\nSubject308    0.3  1.0  2742 \nSubject309    0.3  1.0  3019 \nSubject310    0.3  1.0  2411 \nSubject330    0.3  1.0  2263 \nSubject331    0.3  1.0  2563 \nSubject332    0.3  1.0  2924 \nSubject333    0.3  1.0  2560 \nSubject334    0.2  1.0  2928 \nSubject335    0.2  1.0  3386 \nSubject337    0.3  1.0  2424 \nSubject349    0.3  1.0  2629 \nSubject350    0.3  1.0  2490 \nSubject351    0.4  1.0  2272 \nSubject352    0.3  1.0  2909 \nSubject369    0.3  1.0  2915 \nSubject370    0.3  1.0  2594 \nSubject371    0.2  1.0  2399 \nSubject372    0.2  1.0  2760 \nsigma         0.1  1.0  1113 \nmean_PPD      0.1  1.0  2215 \nlog-posterior 0.1  1.0   719 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-7",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-7",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can compare the predictions from the no-pooling model to the reaction times of each subject\n\n\n\n\nnew_pred &lt;- posterior_predict(m2, newdata = new)\nppc_intervals(new$Reaction, yrep = new_pred) +\n  scale_x_continuous(labels = new$Subject,  breaks = 1:nrow(new)) +\n  xlab(\"Subject\") + ylab(\"Reaction\")\n\n\n\n\n\n\n\n\n\n\n\nWhat are some of the limitations of this approach?"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-hierarchical-model",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-hierarchical-model",
    "title": "Bayesian Inference",
    "section": "Building a Hierarchical Model",
    "text": "Building a Hierarchical Model\n\n\n\n\n\n\n\n\\(\\mu_j\\) is an average reaction time for subject \\(j\\)\n\\(\\sigma_y\\) is the within-subject variability of reaction times\n\\(\\mu\\) is the global average of reaction times across all subject\n\\(\\sigma_{\\mu}\\) is subject to subject variability of reaction times\nTop-level parameters get fixed priors and induce the degree of pooling"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-8",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-8",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can now write out the full model giving priors to all global parameters\nNotice, how the model could be extended if instead of \\(\\mu_j\\) we had our typical linear predictor\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} &\\sim& \\text{Normal}(\\mu_j, \\ \\sigma_y^2) \\\\\n\\mu_{j} &\\sim& \\text{Normal}(\\mu, \\ \\sigma_{\\mu}^2) \\\\\n\\mu &\\sim& \\text{Normal}(300, 50^2) \\\\\n\\sigma_y &\\sim& \\text{Exponential}(0.02) \\\\\n\\sigma_{\\mu} &\\sim& \\text{Exponential}(1)\n\\end{eqnarray}\n\\]\n\n\n\nThis model can also be written such that \\(\\mu_j = \\mu + b_j\\), where each \\(b_j \\sim \\text{Normal}(0, \\sigma_{\\mu}^2)\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-9",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-9",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\n\n\nm3 &lt;- stan_glmer(Reaction ~ (1 | Subject),\n                 prior_intercept = normal(300, 50),\n                 prior_aux = exponential(0.02),\n                 prior_covariance = decov(reg = 1, \n                   conc = 1, shape = 1, scale = 1),\n                 family = gaussian, data = d, iter = 1500,\n                 refresh = 0,\n                 seed = 123)\n\n\n\n\n\n(Intercept) = \\(\\mu\\)\nsigma = \\(\\sigma_y\\)\nSigma[Subject:(Intercept),(Intercept)] = \\(\\sigma^2_{\\mu}\\)\nb[(Intercept) Subject:XYZ] = \\(b_j\\), and so \\(\\mu_j = \\mu + \\beta_j\\)\n\n\n\n\n\nsummary(m3)\n\n\nModel Info:\n\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      Reaction ~ (1 | Subject)\n algorithm:    sampling\n sample:       3000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n groups:       Subject (18)\n\nEstimates:\n                                         mean   sd     10%    50%    90% \n(Intercept)                             294.2    9.8  281.8  294.5  306.5\nb[(Intercept) Subject:308]               49.3   17.5   27.3   49.0   71.5\nb[(Intercept) Subject:309]              -71.7   16.5  -92.5  -71.5  -50.8\nb[(Intercept) Subject:310]              -62.7   15.1  -82.3  -62.5  -43.9\nb[(Intercept) Subject:330]               10.0   15.0   -8.8    9.8   29.0\nb[(Intercept) Subject:331]               13.3   16.4   -7.2   13.4   33.9\nb[(Intercept) Subject:332]              -19.2   16.4  -39.7  -19.1    2.1\nb[(Intercept) Subject:333]                9.8   18.0  -13.0    9.8   32.4\nb[(Intercept) Subject:334]                2.7   14.8  -16.2    2.6   21.6\nb[(Intercept) Subject:335]              -40.4   15.4  -60.1  -40.2  -21.1\nb[(Intercept) Subject:337]               57.4   16.1   37.2   57.2   77.4\nb[(Intercept) Subject:349]              -10.8   16.3  -31.4  -10.8    9.9\nb[(Intercept) Subject:350]               37.1   15.5   17.3   37.1   56.8\nb[(Intercept) Subject:351]              -10.9   18.2  -34.6  -11.0   12.7\nb[(Intercept) Subject:352]               43.1   17.3   21.1   42.8   65.0\nb[(Intercept) Subject:369]                0.5   16.3  -20.3    0.3   21.2\nb[(Intercept) Subject:370]              -28.2   16.3  -48.9  -28.3   -6.9\nb[(Intercept) Subject:371]                0.7   14.3  -17.1    0.6   19.3\nb[(Intercept) Subject:372]               21.3   14.1    3.8   21.3   39.0\nsigma                                    37.5    2.5   34.3   37.4   40.8\nSigma[Subject:(Intercept),(Intercept)] 1685.8  706.9  968.5 1538.3 2541.1\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.0    4.9 286.8 292.9 299.4\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                       mcse Rhat n_eff\n(Intercept)                             0.5  1.0  383 \nb[(Intercept) Subject:308]              0.5  1.0 1285 \nb[(Intercept) Subject:309]              0.5  1.0 1171 \nb[(Intercept) Subject:310]              0.5  1.0  987 \nb[(Intercept) Subject:330]              0.5  1.0  821 \nb[(Intercept) Subject:331]              0.6  1.0  889 \nb[(Intercept) Subject:332]              0.5  1.0 1103 \nb[(Intercept) Subject:333]              0.5  1.0 1267 \nb[(Intercept) Subject:334]              0.5  1.0  797 \nb[(Intercept) Subject:335]              0.5  1.0 1092 \nb[(Intercept) Subject:337]              0.5  1.0  965 \nb[(Intercept) Subject:349]              0.5  1.0 1196 \nb[(Intercept) Subject:350]              0.5  1.0  885 \nb[(Intercept) Subject:351]              0.4  1.0 1648 \nb[(Intercept) Subject:352]              0.5  1.0 1246 \nb[(Intercept) Subject:369]              0.5  1.0  928 \nb[(Intercept) Subject:370]              0.5  1.0 1020 \nb[(Intercept) Subject:371]              0.5  1.0  819 \nb[(Intercept) Subject:372]              0.5  1.0  754 \nsigma                                   0.0  1.0 2630 \nSigma[Subject:(Intercept),(Intercept)] 23.6  1.0  895 \nmean_PPD                                0.1  1.0 3211 \nlog-posterior                           0.2  1.0  617 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#computing-mu_j",
    "href": "bayes-course/07-lecture/07-lecture.html#computing-mu_j",
    "title": "Bayesian Inference",
    "section": "Computing \\(\\mu_j\\)",
    "text": "Computing \\(\\mu_j\\)\n\n\nmuj &lt;- m3 |&gt;\n  spread_draws(`(Intercept)`, b[ ,Subject]) |&gt;\n  mutate(mu_j = `(Intercept)` + b) |&gt;\n  select(Subject, mu_j) |&gt;\n  mean_qi(.width = 0.90)\n\nhead(muj)\n\n# A tibble: 6 × 7\n  Subject      mu_j .lower .upper .width .point .interval\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Subject:308  344.   318.   369.    0.9 mean   qi       \n2 Subject:309  223.   200.   245.    0.9 mean   qi       \n3 Subject:310  231.   211.   252.    0.9 mean   qi       \n4 Subject:330  304.   285.   324.    0.9 mean   qi       \n5 Subject:331  308.   284.   330.    0.9 mean   qi       \n6 Subject:332  275.   252.   298.    0.9 mean   qi"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-10",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-10",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can see the effects of hierarchical pooling below\nSubjects that are farther away from \\(\\E(\\mu) = 294\\)(ms) and the ones that have fewer observations are pooled more towards the global mean"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model",
    "title": "Bayesian Inference",
    "section": "Building a Full Hierarchical Model",
    "text": "Building a Full Hierarchical Model\n\n\nThe model we have built so far is for average reaction time only\nWhat we would like to do, is to build a pooled regression model for the reaction time of each subject\nThe mechanics are similar but now we need a model for the slopes and intercepts for each subject, their covariance, and priors\n\n\n\n\nm4 &lt;- stan_glmer(Reaction ~ Days + (Days | Subject),\n                 prior_intercept = normal(300, 50),\n                 prior = normal(0, 2, autoscale = TRUE),\n                 prior_aux = exponential(0.02),\n                 prior_covariance = decov(reg = 1, \n                   conc = 1, shape = 1, scale = 1),\n                 family = gaussian, data = d, iter = 1500,\n                 cores = 4, seed = 123, refresh = 0)\nsummary(m4)\n\n\nModel Info:\n\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      Reaction ~ Days + (Days | Subject)\n algorithm:    sampling\n sample:       3000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n groups:       Subject (18)\n\nEstimates:\n                                         mean   sd     10%    50%    90% \n(Intercept)                             256.3    7.7  246.6  256.2  265.9\nDays                                      8.9    1.7    6.7    8.9   11.1\nb[(Intercept) Subject:308]               42.1   22.7   15.0   39.7   72.8\nb[Days Subject:308]                       0.9    4.4   -4.9    1.2    6.2\nb[(Intercept) Subject:309]              -43.1   14.0  -61.2  -43.0  -25.6\nb[Days Subject:309]                      -7.9    3.1  -11.8   -7.9   -3.8\nb[(Intercept) Subject:310]              -42.8   13.5  -60.4  -42.3  -26.0\nb[Days Subject:310]                      -5.0    2.9   -8.7   -5.0   -1.3\nb[(Intercept) Subject:330]               23.8   14.4    5.7   23.3   42.0\nb[Days Subject:330]                      -4.1    2.8   -7.8   -4.0   -0.6\nb[(Intercept) Subject:331]               19.2   12.9    3.0   19.0   35.8\nb[Days Subject:331]                       0.7    2.9   -3.1    0.7    4.4\nb[(Intercept) Subject:332]                2.8   12.6  -12.8    2.3   19.2\nb[Days Subject:332]                      -5.1    2.9   -8.9   -5.0   -1.4\nb[(Intercept) Subject:333]               11.7   14.9   -7.0   11.2   31.0\nb[Days Subject:333]                       0.3    3.1   -3.7    0.4    4.2\nb[(Intercept) Subject:334]               -9.7   13.2  -26.9   -9.3    7.0\nb[Days Subject:334]                       2.4    2.7   -1.0    2.4    5.8\nb[(Intercept) Subject:335]               -7.2   14.4  -25.5   -7.5   11.5\nb[Days Subject:335]                      -8.9    2.8  -12.5   -8.9   -5.2\nb[(Intercept) Subject:337]               32.9   13.6   15.6   32.9   50.4\nb[Days Subject:337]                       9.9    3.0    6.0    9.9   13.7\nb[(Intercept) Subject:349]              -25.4   18.1  -49.2  -24.0   -3.1\nb[Days Subject:349]                       1.5    3.4   -2.9    1.4    6.0\nb[(Intercept) Subject:350]              -10.7   16.7  -32.6  -10.1   10.6\nb[Days Subject:350]                       8.1    3.1    4.3    8.0   12.3\nb[(Intercept) Subject:351]                3.2   16.9  -17.9    2.7   24.5\nb[Days Subject:351]                      -4.1    3.7   -8.8   -4.0    0.5\nb[(Intercept) Subject:352]               26.9   16.4    6.8   26.4   47.7\nb[Days Subject:352]                       3.4    3.2   -0.7    3.5    7.4\nb[(Intercept) Subject:369]               -0.4   13.1  -17.0   -0.4   15.6\nb[Days Subject:369]                       1.1    2.8   -2.4    1.1    4.7\nb[(Intercept) Subject:370]              -31.5   14.5  -50.2  -31.0  -13.4\nb[Days Subject:370]                       5.0    3.5    0.4    5.0    9.5\nb[(Intercept) Subject:371]               -1.9   12.8  -18.0   -1.8   14.3\nb[Days Subject:371]                       0.1    2.6   -3.1    0.1    3.4\nb[(Intercept) Subject:372]               10.2   12.6   -5.7   10.3   26.5\nb[Days Subject:372]                       2.4    2.6   -0.9    2.4    5.7\nsigma                                    22.1    1.8   20.0   22.0   24.4\nSigma[Subject:(Intercept),(Intercept)]  845.2  460.0  381.3  751.0 1416.9\nSigma[Subject:Days,(Intercept)]           9.2   64.9  -68.9   14.1   77.8\nSigma[Subject:Days,Days]                 44.8   22.7   22.0   39.6   73.6\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.3    2.8 289.8 293.2 296.9\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                       mcse Rhat n_eff\n(Intercept)                             0.2  1.0 1300 \nDays                                    0.0  1.0 1246 \nb[(Intercept) Subject:308]              0.6  1.0 1616 \nb[Days Subject:308]                     0.1  1.0 1862 \nb[(Intercept) Subject:309]              0.3  1.0 2340 \nb[Days Subject:309]                     0.1  1.0 2460 \nb[(Intercept) Subject:310]              0.3  1.0 1957 \nb[Days Subject:310]                     0.1  1.0 2338 \nb[(Intercept) Subject:330]              0.4  1.0 1533 \nb[Days Subject:330]                     0.1  1.0 1666 \nb[(Intercept) Subject:331]              0.3  1.0 2157 \nb[Days Subject:331]                     0.1  1.0 2760 \nb[(Intercept) Subject:332]              0.3  1.0 1876 \nb[Days Subject:332]                     0.1  1.0 2272 \nb[(Intercept) Subject:333]              0.3  1.0 2132 \nb[Days Subject:333]                     0.1  1.0 2255 \nb[(Intercept) Subject:334]              0.3  1.0 1935 \nb[Days Subject:334]                     0.1  1.0 1922 \nb[(Intercept) Subject:335]              0.3  1.0 1693 \nb[Days Subject:335]                     0.1  1.0 1648 \nb[(Intercept) Subject:337]              0.3  1.0 2140 \nb[Days Subject:337]                     0.1  1.0 2307 \nb[(Intercept) Subject:349]              0.4  1.0 1636 \nb[Days Subject:349]                     0.1  1.0 1769 \nb[(Intercept) Subject:350]              0.4  1.0 1649 \nb[Days Subject:350]                     0.1  1.0 1450 \nb[(Intercept) Subject:351]              0.4  1.0 2235 \nb[Days Subject:351]                     0.1  1.0 2194 \nb[(Intercept) Subject:352]              0.3  1.0 2721 \nb[Days Subject:352]                     0.1  1.0 2473 \nb[(Intercept) Subject:369]              0.3  1.0 2130 \nb[Days Subject:369]                     0.1  1.0 2044 \nb[(Intercept) Subject:370]              0.4  1.0 1427 \nb[Days Subject:370]                     0.1  1.0 1486 \nb[(Intercept) Subject:371]              0.3  1.0 1885 \nb[Days Subject:371]                     0.1  1.0 2096 \nb[(Intercept) Subject:372]              0.3  1.0 2141 \nb[Days Subject:372]                     0.1  1.0 1802 \nsigma                                   0.0  1.0 1532 \nSigma[Subject:(Intercept),(Intercept)] 13.9  1.0 1099 \nSigma[Subject:Days,(Intercept)]         2.1  1.0  931 \nSigma[Subject:Days,Days]                0.6  1.0 1364 \nmean_PPD                                0.1  1.0 2854 \nlog-posterior                           0.3  1.0  518 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model-1",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model-1",
    "title": "Bayesian Inference",
    "section": "Building a Full Hierarchical Model",
    "text": "Building a Full Hierarchical Model"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#predicting-for-a-new-subject",
    "href": "bayes-course/07-lecture/07-lecture.html#predicting-for-a-new-subject",
    "title": "Bayesian Inference",
    "section": "Predicting for a New Subject",
    "text": "Predicting for a New Subject\n\n\nWe saw that we can make predictions for reaction times of subjects that were part of the model\nBut we can also make predictions for unobserved subjects by drawing from the “population” distribution, as opposed to subject-specific parameters\n\n\n\n\n\nnew_subj &lt;- data.frame(Days = 0:9, \n              Subject = as.factor(rep(400, 10)))\nypred_subj &lt;- posterior_predict(m4, \n                        newdata = new_subj)\nnew_subj |&gt;\n  add_predicted_draws(m4) |&gt;\n  ggplot(aes(x = Days)) +\n  stat_lineribbon(aes(y = .prediction), \n                  width = c(.9, .8, .5), \n                  alpha = 0.25) +\n  ylab(\"Reaction time\") + \n  ggtitle(\"Prediction for an unobserved subject\") +\n  scale_fill_brewer(palette = \"Greys\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#course-evaluations",
    "href": "bayes-course/07-lecture/07-lecture.html#course-evaluations",
    "title": "Bayesian Inference",
    "section": "Course Evaluations",
    "text": "Course Evaluations\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#homework-feedback",
    "href": "bayes-course/05-lecture/05-lecture.html#homework-feedback",
    "title": "Bayesian Inference",
    "section": "Homework Feedback",
    "text": "Homework Feedback\n\nHow was Homework 4?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#bayesian-linear-regression-and-model-evaluation",
    "href": "bayes-course/05-lecture/05-lecture.html#bayesian-linear-regression-and-model-evaluation",
    "title": "Bayesian Inference",
    "section": "Bayesian linear regression and model evaluation",
    "text": "Bayesian linear regression and model evaluation\n\n\n\n\nIntroducing linear regression\nPrior predictive simulations\nSampling from the posterior\nExample of linear regression in Stan\nEvaluating the quality of the draws\nPosterior predictions\nCross-validation, ELPD, and LOO\n\n\n\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "href": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\n\n\nWe borrow this example from Richard McElreath’s Statistical Rethinking\nThe data sets provided have been produced between 1969 to 2008, based on Nancy Howell’s observations of the !Kung San\nFrom Wikipedia: “The ǃKung are one of the San peoples who live mostly on the western edge of the Kalahari desert, Ovamboland (northern Namibia and southern Angola), and Botswana.”\n\n\n\n\n\n\n\n\n\n\nUnivercity of Toronto Data Sets"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\n\n\nData sample and summary:\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n151.765\n47.82561\n63\n1\n\n\n139.700\n36.48581\n63\n0\n\n\n136.525\n31.86484\n65\n0\n\n\n156.845\n53.04191\n41\n1\n\n\n145.415\n41.27687\n51\n0\n\n\n163.830\n62.99259\n35\n1\n\n\n\n\n\n     height           weight            age       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.00  \n 1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00  \n Median :148.59   Median :40.058   Median :27.00  \n Mean   :138.26   Mean   :35.611   Mean   :29.34  \n 3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00  \n Max.   :179.07   Max.   :62.993   Max.   :88.00  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice a non-linearity\nThinking about why this should be, can give you an insight into how to model these data"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\nFor now, we will focus on the linear subset of the data\nWe will demonstrate the non-linear model at the end\nWe will restrict our attention to adults (age &gt; 18)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#general-approach",
    "href": "bayes-course/05-lecture/05-lecture.html#general-approach",
    "title": "Bayesian Inference",
    "section": "General Approach",
    "text": "General Approach\n\n\nAssess the scope of the inferences that you will get with this model\nUnless you are doing causal inference, you should interpret your coefficients as comparisons (RAOS, Page 84)\nSet up reasonable priors and likelihood\nFor more complex models, perform a forward simulation with fixed parameter values and try to recover them by running an inference algorithm. See this example for a standard epidemiological model.\nPerform a prior predictive simulation\nPossibly adjust your priors\nFit the model to data\nAssess the quality of the inference and the quality of the model\nImprove or fix your model and go back to #3"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe will build a predictive model for adult Weight \\(y\\) given Height \\(x\\) using the Howell dataset\nInitial stab at the model: \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x_i \\\\\n\\alpha & \\sim & \\text{Normal}(\\alpha_{l}, \\, \\alpha_s) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWe have to specify \\(\\alpha_{l}\\) and \\(\\alpha_s\\), where l and s signify location and scale, and r, the rate of the exponential\nIf we work on the original scale for \\(x\\), it is awkward to choose a prior for the intercept : it corresponds to the weight of the person with zero height\nThis can be fixed by subtracting the average height from \\(x\\)\nWhy is \\(\\text{Exp}(r)\\) is a reasonable prior for \\(\\sigma\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe define a new variable, the centered version of \\(x\\): \\(x^c_i = x_i - \\bar{x}\\)\nNow \\(\\alpha\\) corresponds to the weight of an average person\nChecking Wikipedia reveals that the average weight of a person in Africa is about 60 kg\nThey don’t state the standard deviation, but it is unlikely that an African adult would weigh less than 30 kg and more than 120 kg so we will set the prior sd = 10 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the slope \\(\\beta\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nIn this dataset, the units of \\(\\beta\\) are \\(\\frac{kg}{cm}\\), since the units of height are \\(cm\\)\nFirst thing, \\(\\beta\\) should be positive. Why?\nSecond, \\(\\beta\\) is likely less than 1. Why?\nWe can consult height-weight tables for the expected value and variance\nIn the dataset, \\(\\E(\\beta) = 0.55\\) with a standard error of 0.006, but since we are uncertain how applicable that is to !Kung, we will allow the prior to vary more \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the error term \\(\\sigma\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe know that \\(\\sigma\\) must be positive, so a possible choice for the prior is \\(\\text{Normal}^+\\), Exponential, etc.\nAt this stage, the key is to rule out implausible values, not to get something precise, particularly since we have enough data (&gt; 340 observations)\nFrom the background data, the residual standard error was 4.6, which implies the exponential rate parameter of about 1/5 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(0.2) \\\\\n\\end{eqnarray}\n\\]\nWe are now ready to perform a prior predictive simulation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nThe simulation follows the generative process defined by the model\n\n\n\n\n\n\nd &lt;- d |&gt;\n  mutate(height_c = height - mean(height))\nround(mean(d$height_c), 2)\n\n[1] 0\n\nhead(d)\n\n# A tibble: 6 × 5\n  height weight   age  male height_c\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1   152.   47.8    63     1    -2.83\n2   140.   36.5    63     0   -14.9 \n3   137.   31.9    65     0   -18.1 \n4   157.   53.0    41     1     2.25\n5   145.   41.3    51     0    -9.18\n6   164.   63.0    35     1     9.23\n\nprior_pred &lt;- function(data) {\n  alpha &lt;- rnorm(1, 60, 10)\n  beta &lt;- rnorm(1, 0.55, 0.1)\n  sigma &lt;- rexp(1, 0.2)\n  l &lt;- nrow(data); y &lt;- numeric(l)\n  for (i in 1:l) {\n    mu &lt;- alpha + beta * data$height_c[i]\n    y[i] &lt;- rnorm(1, mu, sigma)\n  }\n  return(y)\n}\n\n\n\n\n\nn &lt;- 100\npr_p &lt;- replicate(n = n, prior_pred(d))\n# using library(purrr) functional primitives:\n# pr_p &lt;- map(1:n, \\(i) prior_pred(d))\ndim(pr_p)\n\n[1] 352 100\n\nround(pr_p[1:12, 1:8], 2)\n\n       [,1]  [,2]  [,3]   [,4]  [,5]  [,6]  [,7]  [,8]\n [1,] 75.17 45.38 63.41  79.34 70.60 53.38 59.64 71.54\n [2,] 66.90 43.28 60.23  56.40 64.75 45.41 52.28 61.20\n [3,] 58.87 32.81 34.94  71.38 63.30 73.13 66.56 61.37\n [4,] 64.57 47.99 73.31  40.31 72.81 52.06 29.27 79.36\n [5,] 69.54 41.90 61.59  -4.10 66.94 60.39 44.22 72.73\n [6,] 85.82 53.97 68.87 104.92 76.18 60.51 56.68 76.91\n [7,] 62.48 51.62 68.41  25.53 69.20 64.53 70.15 70.65\n [8,] 77.69 62.25 72.45  63.32 78.10 67.48 63.22 81.76\n [9,] 80.35 43.70 54.51  13.69 68.65 46.92 48.20 70.36\n[10,] 85.94 51.44 66.29 111.06 76.71 64.20 53.96 78.94\n[11,] 85.61 41.28 53.84  60.63 71.72 83.42 47.55 76.78\n[12,] 74.94 55.23 61.77  34.21 70.00 53.53 59.05 68.94"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\ndata_mean &lt;- mean(d$weight)\nmean_dist &lt;- colMeans(pr_p)\nggplot(data.frame(data_mean, mean_dist), aes(mean_dist)) +\n  geom_histogram() +\n  geom_vline(xintercept = data_mean, color = 'red') +\n  xlab(\"Distribution of Weight means (kg) under the prior\") + ylab(\"\") +\n  theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-2",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-2",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nTo get a sense for the possible regression lines implied by the prior, we can fit a linear model to each simulation draw, and plot the lines over observations\n\n\n\n\n\nintercepts &lt;- numeric(n)\nslopes &lt;- numeric(n)\nfor (i in 1:n) {\n  coefs &lt;- coef(lm(pr_p[, i] ~ d$height_c))\n  intercepts[i] &lt;- coefs[1]\n  slopes[i] &lt;- coefs[2]\n}\n\n# using library(purrr) functional primitives:\n# df &lt;- pr_p |&gt; map_dfr(\\(y) coef(lm(y ~ d$height_c)))\n\np &lt;- ggplot(aes(height_c, weight), data = d)\np + geom_point(size = 0.5) + ylim(20, 90) + \n  geom_abline(slope = slopes, \n              intercept = intercepts, \n              alpha = 1/6) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"Kalahari !Kung San people\", \n          subtitle = \"Prior predictive simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice about this prior?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "href": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving a Posterior Distribution",
    "text": "Deriving a Posterior Distribution\n\n\nWe have seen how to derive the posterior and posterior predictive distribution\nThree dimentional posterior: \\(f(\\alpha, \\beta, \\sigma)\\). What happened to \\(\\mu\\)?\nWe construct the posterior from the prior and data likelihood (for each \\(y_i\\)): \\[\n\\begin{eqnarray}\n&\\text{Prior: }f(\\alpha, \\beta, \\sigma) = f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\\\\n&\\text{Likelihood: }f(y \\mid \\alpha, \\beta, \\sigma) = \\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\\\\n&\\text{Posterior: }f(\\alpha,\\beta,\\sigma \\mid y) = \\frac{f_1(\\alpha) f(_2\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right]}\n{\\int\\int\\int f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right] d\\alpha \\, d\\beta \\, d\\sigma}\n\\end{eqnarray}\n\\]\nTo be more precise, we would indicate that \\(f_1, f_2\\) and \\(f_4\\) are Normal with different parameters, and \\(f_3\\) is \\(\\text{Exp}(0.2)\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "href": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "title": "Bayesian Inference",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nEven though our prior is slightly off, 300+ observations is a lot in this case (big data!), and so we proceed to model fitting\nWe will use stan_glm() function in rstanarm\nrstanarm has default priors, but you should specify your own:\n\n\n\nlibrary(rstanarm)\nlibrary(bayesplot)\noptions(mc.cores = parallel::detectCores())\n\nm1 &lt;- stan_glm(\n  weight ~ height_c,\n  data = d,\n  family = gaussian,\n  prior_intercept = normal(60, 10),\n  prior = normal(0.55, 0.1),\n  prior_aux = exponential(0.2),\n  chains = 4,\n  iter = 500,\n  seed = 1234\n)\n\n\n\nBy default, rstanarm samples from the posterior. To get back the prior predictive distribution (instead of doing it in R) use prior_PD = TRUE"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "href": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "title": "Bayesian Inference",
    "section": "Looking at the Model Summary",
    "text": "Looking at the Model Summary\n\n\nsummary(m1)\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ height_c\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 352\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 45.0    0.2 44.7  45.0  45.3 \nheight_c     0.6    0.0  0.6   0.6   0.7 \nsigma        4.2    0.2  4.0   4.2   4.5 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 45.0    0.3 44.6  45.0  45.4 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  824  \nheight_c      0.0  1.0  994  \nsigma         0.0  1.0  867  \nmean_PPD      0.0  1.0  970  \nlog-posterior 0.1  1.0  459  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\nIf you can examine the priors by running prior_summary(m1)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "title": "Bayesian Inference",
    "section": "Evaluting Quality of the Inferences",
    "text": "Evaluting Quality of the Inferences\n\n\nneff_ratio(m1) |&gt; round(2)\n\n(Intercept)    height_c       sigma \n       0.82        0.99        0.87 \n\n\n\n\n\nrhat(m1) |&gt; round(2)\n\n(Intercept)    height_c       sigma \n          1           1           1 \n\n\n\n\n\nmcmc_trace(m1, size = 0.3)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "href": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "title": "Bayesian Inference",
    "section": "Same Model in Stan",
    "text": "Same Model in Stan\n\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] x;\n  vector[N] y;\n  int&lt;lower=0, upper=1&gt; prior_PD;\n}\nparameters {\n  real alpha;\n  real beta;\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  vector[N] mu = alpha + beta * x;\n}\nmodel {\n  alpha ~ normal(60, 10);\n  beta ~ normal(0.55, 0.1);\n  sigma ~ exponential(0.2);\n  if (!prior_PD) {\n    y ~ normal(mu, sigma);\n  }\n}\ngenerated quantities {\n  array[N] real yrep = normal_rng(mu, sigma);\n}\n\n\n\n\nYou can pass prior_PD as a flag to enable drawing from the prior predictive distribution"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "title": "Bayesian Inference",
    "section": "Prior vs Posterior",
    "text": "Prior vs Posterior\n\n\nComparing the prior to the posterior tells us how much the model learned from data\nIt also helps us to validate if our priors were reasonable\nIn rstanarm, you can use the posterior_vs_prior function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour prior should cover the plausible range of parameter values\nWhen we don’t have a lot of data and parameters are complex, setting good priors takes work, but there are guidelines"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\nlibrary(tidybayes)\n\ndraws &lt;- spread_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(head(round(draws, 2)))\n\n\n\n\n.chain\n.iteration\n.draw\n(Intercept)\nheight_c\nsigma\n\n\n\n\n1\n1\n1\n44.97\n0.62\n4.09\n\n\n1\n2\n2\n45.06\n0.61\n4.21\n\n\n1\n3\n3\n45.02\n0.63\n4.13\n\n\n1\n4\n4\n45.07\n0.64\n4.32\n\n\n1\n5\n5\n44.66\n0.64\n4.38\n\n\n1\n6\n6\n45.31\n0.62\n4.12\n\n\n\n\n\n\n\n\nspread_draws will arrange the inferences in columns (wide format)\ngather_draws will arrange the inferences in rows (long format), which is usually more convenient for plotting and computation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\noptions(digits = 3)\ndraws &lt;- gather_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(tail(draws, 4))\n\n\n\n\n.chain\n.iteration\n.draw\n.variable\n.value\n\n\n\n\n4\n247\n997\nsigma\n4.09\n\n\n4\n248\n998\nsigma\n4.22\n\n\n4\n249\n999\nsigma\n4.20\n\n\n4\n250\n1000\nsigma\n4.11\n\n\n\n\n\n\n\n\ndraws |&gt; mean_qi(.width = 0.90) |&gt; knitr::kable() # also see ?median_qi(), etc\n\n\n\n\n.variable\n.value\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\n(Intercept)\n45.000\n44.635\n45.36\n0.9\nmean\nqi\n\n\nheight_c\n0.624\n0.576\n0.67\n0.9\nmean\nqi\n\n\nsigma\n4.250\n3.996\n4.53\n0.9\nmean\nqi\n\n\n\n\n\n\n\n\nFrom the above table: \\(\\E(y \\mid x^c) (\\text{kg}) = 45 (\\text{kg}) + 0.62 (\\text{kg/cm}) \\cdot x^c (\\text{cm})\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "title": "Bayesian Inference",
    "section": "Variability in Parameter Inferences",
    "text": "Variability in Parameter Inferences\n\n\nThe code in section 9.4 in the book doesn’t work as the function and variable names have changed\n\n\n\n\ndpred &lt;- d |&gt; \n  # same as add_epred_draws for lin reg not for other GLMs\n  add_linpred_draws(m1, ndraws = 50)\nhead(dpred, 3)\n\n# A tibble: 3 × 10\n# Groups:   height, weight, age, male, height_c, .row [1]\n  height weight   age  male height_c  .row .chain .iteration .draw .linpred\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n1   152.   47.8    63     1    -2.83     1     NA         NA     1     43.3\n2   152.   47.8    63     1    -2.83     1     NA         NA     2     42.9\n3   152.   47.8    63     1    -2.83     1     NA         NA     3     43.0\n\n\n\n\n\n\np &lt;- dpred |&gt; ggplot(aes(x = height_c, y = weight)) +\n  geom_line(aes(y = .linpred, group = .draw), \n            alpha = 0.1) + \n  geom_point(data = d, size = 0.05) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"100 draws from the slope/intercept posterior\")\nprint(p)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nSuppose we are interested in predicting the weight of a person with a height of 160 cm\nThis corresponds to the centered height of 5.4: (\\(160 - \\bar{x}\\))\nWe can now compute the distribution of the mean weight of a 160 cm person (reflecting variability in the slope and intercept only):\n\n\\(\\mu = \\alpha + \\beta \\cdot 5.4\\), for each posterior draw\n\nAnd a predictive distribution:\n\n\\(y_{\\text{pred}}  \\sim \\text{Normal}(\\mu, \\sigma)\\)\n\n\n\n\n\ndraws &lt;- spread_draws(m1, `(Intercept)`, height_c, sigma)\ndraws &lt;- draws |&gt;\n  mutate(mu = `(Intercept)` + height_c * 5.4,\n         y_pred = rnorm(nrow(draws), mu, sigma))\ndraws[1:3, 4:8]\n\n# A tibble: 3 × 5\n  `(Intercept)` height_c sigma    mu y_pred\n          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1          45.0    0.619  4.09  48.3   43.5\n2          45.1    0.607  4.21  48.3   50.2\n3          45.0    0.628  4.13  48.4   46.6"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nWe can compare predictive and average densities\nLeft panel showing the densities on their own\nRight panel showing the same densities in the context of raw observations\n\n\n\n\nmqi &lt;- draws |&gt; median_qi(.width = 0.90)\nselect(mqi, contains(c('mu', 'y_pred'))) |&gt; round(2)\n\n    mu mu.lower mu.upper y_pred y_pred.lower y_pred.upper\n1 48.4     47.9     48.8   48.2         41.2         55.2"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "href": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "title": "Bayesian Inference",
    "section": "RStanArm Prediction Functions",
    "text": "RStanArm Prediction Functions\n\n\nposterior_linpred returns \\(D \\times N\\) matrix with D draws and N data points\n\n\\(\\eta_n = \\alpha + \\sum_{p=1}^P \\beta_p x_{np}\\), where \\(P\\) is the total number of regression inputs\n\nposterior_epred returns an \\(D \\times N\\) matrix that applies the inverse link (in GLMs) to the linear predictor \\(\\eta\\)\n\n\\(\\mu_n = \\E(y | x_n)\\); this is the same as \\(\\eta\\) in Lin Regression\n\nposterior_predict returns an \\(D \\times N\\) matrix of predictions: \\(y \\mid \\mu_n\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "href": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "title": "Bayesian Inference",
    "section": "Predictions in RStanArm",
    "text": "Predictions in RStanArm\n\n\nPosterior linear predictor\n\n\n\neta &lt;- posterior_linpred(m1, newdata = data.frame(height_c = 5.4))\nquantile(eta, probs = c(0.05, 0.50, 0.95)) |&gt; round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\nglue::glue('From the R simulation, 90% interval for eta = [{mqi$mu.lower |&gt; round(2)}, {mqi$mu.upper |&gt; round(2)}]')\n\nFrom the R simulation, 90% interval for eta = [47.91, 48.82]\n\n\n\n\nPosterior conditional mean\n\n\n\nmu &lt;- posterior_epred(m1, newdata = data.frame(height_c = 5.4))\nquantile(mu, probs = c(0.05, 0.50, 0.95)) |&gt; round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\n\n\n\nPosterior prediction\n\n\n\ny_pred &lt;- posterior_predict(m1, newdata = data.frame(height_c = 5.4))\nquantile(y_pred, probs = c(0.05, 0.50, 0.95)) |&gt; round(2)\n\n  5%  50%  95% \n41.6 48.5 55.0 \n\nglue::glue('From the R simulation, 90% interval for y_pred = [{mqi$y_pred.lower |&gt; round(2)}, {mqi$y_pred.upper |&gt; round(2)}]')\n\nFrom the R simulation, 90% interval for y_pred = [41.24, 55.25]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-model-quality",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-model-quality",
    "title": "Bayesian Inference",
    "section": "Evaluting Model Quality",
    "text": "Evaluting Model Quality"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model",
    "title": "Bayesian Inference",
    "section": "Evaluating Quality of the Model",
    "text": "Evaluating Quality of the Model\n\n\nThere are at least two stages of model evaluation: 1) the quality of the draws and 2) the quality of predictions\nThere is also a question of model fairness\n\nHow were the data collected?\nPeople will likely interpret the results causally, even if not appropriate\nHow will the model be used?\nExample: Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)\n\nJust because the draws have good statistical properties (e.g., good mixing, low auto-correlation, etc.), it does not mean the model will perform well\nModel performance is assessed on how well it can make predictions, minimize costs and/or maximize benefits. Predictive accuracy is a common way of evaluating model performance."
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model-1",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model-1",
    "title": "Bayesian Inference",
    "section": "Evaluating Quality of the Model",
    "text": "Evaluating Quality of the Model\n\n\nOnce we are satisfied that the draws are statistically well-behaved, we can focus on evaluating predictive performance\nWe typically care about predictive performance out-of-sample\nIn general, a good model is well-calibrated and makes sharp predictions (Gneiting et al. 2007)1\nFor in-sample assessment, we perform Posterior Predictive Checks or PPCs\nTo assess out-of-sample performance, we rely on cross-validation or its approximations\nIf you are making causal (counterfactual) predictions, naive cross-validation will not work. Why?\n\n\nGneiting, T., Balabdaoui, F., and Raftery, A. E. (2007) Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2), 243–268."
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#model-evaluation",
    "href": "bayes-course/05-lecture/05-lecture.html#model-evaluation",
    "title": "Bayesian Inference",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nHere is the high-level plan of attack:\n\n\nFit a linear model to the full !Kung dataset (not just adults) and let RStanArm pick the priors (don’t do this at home)\nWe know that this model is not quite right\nEvaluate the model fit\nFix the model by thinking about the relationship between height and weight\nEvaluate the improved model\nCompare the linear model to the improved model"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#model-evaluation-1",
    "href": "bayes-course/05-lecture/05-lecture.html#model-evaluation-1",
    "title": "Bayesian Inference",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\nThe following fits a linear model to the full dataset (not just adults)\n\n\n\n\nm2 &lt;- stan_glm(\n  weight ~ height,\n  data = d,\n  family = gaussian,\n  chains = 4,\n  iter = 600,\n  seed = 1234\n)\nsummary(m2)\n\n\n\n\nThere were no sampling problems\nAnd posterior draws look well-behaved\nBut how good is this model?\n\n\n\n\n\n\nModel Info:\n\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ height\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 544\n predictors:   2\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept) -33.8    1.1 -35.1 -33.7 -32.4\nheight        0.5    0.0   0.5   0.5   0.5\nsigma         5.0    0.1   4.8   5.0   5.2\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 35.6    0.3 35.2  35.6  36.0 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1198 \nheight        0.0  1.0  1228 \nsigma         0.0  1.0   836 \nmean_PPD      0.0  1.0  1126 \nlog-posterior 0.1  1.0   497 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nThe idea behind PPCs is to compare the distribution of the observation to posterior predictions\nWe already saw an example of how to do it by hand\nHere, we will do using functions in RStanArm\n\n\n\n\n\nlibrary(bayesplot)\n# bayesplot::pp_check(m2) &lt;-- shortcut\n\n# predict for every observed point\nyrep &lt;- posterior_predict(m2) \n\n# select 50 draws at random\ns &lt;- sample(nrow(yrep), 50)\n\n# plot data against predictive densities\nppc_dens_overlay(d$weight, yrep[s, ])"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-1",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-1",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nWe can also compute the distributions of test statistics:\n\n\n\n\n\nlibrary(gridExtra)\np1 &lt;- ppc_stat(d$weight, yrep, stat = \"mean\"); p2 &lt;- ppc_stat(d$weight, yrep, stat = \"sd\")\nq25 &lt;- function(y) quantile(y, 0.25); q75 &lt;- function(y) quantile(y, 0.75)\np3 &lt;- ppc_stat(d$weight, yrep, stat = \"q25\"); p4 &lt;- ppc_stat(d$weight, yrep, stat = \"q75\"); \ngrid.arrange(p1, p2, p3, p4, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\nppc_stat is a shorthand for computation on each posterior (predictive) draw\nFor example, ppc_stat(d$weight, yrep, stat = \"mean\") is equivalent to:\n\nSetting \\(T(y) = \\text{mean(d\\$weight)}\\) and \\(T_{\\text{yrep}} = \\text{rowMeans(yrep)}\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-2",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-2",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nSince we have a distribution at each observation point, we can plot the predictive distribution at each observation\nWe will first randomly select a subset of 50 people\nEach column of yrep is a prediction for each observation point\n\n\n\n\n\ns &lt;- sample(ncol(yrep), 50)\n\nbayesplot::ppc_intervals(\n  d$weight[s],\n  yrep = yrep[, s],\n  x = d$height[s],\n  prob = 0.5,\n  prob_outer = 0.90\n) + xlab(\"Height (cm)\")  + \n  ylab(\"Weight (kg)\") +\n  ggtitle(\"Predicted vs observed weight\")"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\n\n\nWe looked at some visual evidence for in-sample model accuracy\nThe model is clearly doing a poor job of capturing observed data, particularly in the lower quantiles of the predictive distribution\nIn a situation like this, we would typically proceed to model improvements\nOften, the model is not as bad as this one, and we would like to get some quantitative measures of model fit1\nWe do this so we can assess the relative performance of the next set of models\n\n\n\n\n\n\n\n\n\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-1",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-1",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nOne way to assess model accuracy is to compute an average square deviation from point prediction: mean square error\n\n\\(\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{n=1}^{N} (y_n - \\E(y_n|\\theta))^2}\\)\nOr its scaled version, which divides the summand by \\(\\V(y_i | \\theta)\\)\n\nThese measures do not work well for non-normal models\nOn the plus side, it is intuitive (why?) and computation is easy\nNote that if we just average the errors, we will get zero by definition\n\n\n\n\n# avarage error\nmean(d$weight - colMeans(yrep)) |&gt; round(2)\n\n[1] -0.01\n\n# root mean square error\nmean((d$weight - colMeans(yrep))^2) |&gt; sqrt() |&gt; round(2)\n\n[1] 4.98\n\n# your book reports median absolute error\nmedian(abs(d$weight - colMeans(yrep)))\n\n[1] 3.58"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-2",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-2",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nIn a probabilistic prediction, we can assess model calibration\nCalibration says that, for example, 50% intervals contain approximately 50% of the observations, and so on\nA well-calibrated model may still be a bad model (see below) as the uncertainty may be too wide; for two models with the same calibration, we prefer the one with lower uncertainty\n\n\n\n\ninside &lt;- function(y, obs) return(obs &gt;= y[1] & obs &lt;= y[2])\ncalib  &lt;- function(y, data, interval) {\n  mid &lt;- interval / 2\n  l &lt;- 0.5 - mid; u &lt;- 0.5 + mid\n  intervals &lt;- apply(y, 2, quantile, probs = c(l, u))\n  is_inside &lt;- numeric(ncol(intervals))\n  for (i in 1:ncol(intervals)) {\n    is_inside[i] &lt;- inside(intervals[, i], data[i])\n  }\n return(mean(is_inside))\n}\ncalib(yrep, d$weight, 0.50)\n\n[1] 0.478\n\ncalib(yrep, d$weight, 0.90)\n\n[1] 0.912"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-3",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-3",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nA popular “proper” scoring rule for probabilistic prediction is log score\nThis is equivalent to the log predictive density \\(\\log f(y \\mid \\theta)\\)\nWe would like to estimate the model’s ability to predict data it has not seen, even though we do not observe the true data-generating process\nThis quantity is approximated by log-pointwise-predictive-density \\[\n\\text{lppd} = \\sum_{n=1}^{N} \\log \\left( \\frac{1}{S} \\sum_{s=1}^{S} f(y_n \\mid \\theta_{-n, s}) \\right)\n\\]\nTo compute this quantity, fit the model N times, dropping one point at a time (-n)\nS is the number of posterior samples, and \\(\\theta_{-n, s}\\), is the s-th sample without the n-th data point\nPSIS-LOO (Pareto Smoothed Importance Sampling, Vehtari et al, 2017) provides a computationally efficient way to approximate LOO, without refitting the model N times"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-4",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-4",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\nlibrary(loo)\nlm2 &lt;- loo(m2)\nlm2\n\n\nComputed from 1200 by 544 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo  -1648.7 14.3\np_loo         3.0  0.3\nlooic      3297.3 28.6\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.68).\nSee help('pareto-k-diagnostic') for details.\n\nplot(lm2)\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Bayes and SMaC",
    "section": "",
    "text": "This website contains teaching materials for Bayesian Inference course (APSTA-GE 2123) and Statistics, Math, and Computing (SMaC) bootcamp, both taught at NYU. If you find any errors, please email me at eric.novik@nyu.edu."
  }
]