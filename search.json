[
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#introductions",
    "href": "bayes-course/01-lecture/01-lecture.html#introductions",
    "title": "Bayesian Inference",
    "section": "Introductions",
    "text": "Introductions\nWhat is your first name and where were you born?"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Lecture 1: Bayesian Workflow",
    "text": "Lecture 1: Bayesian Workflow\n\n\nOverview of the Course\nStatistics vs AI/ML\nBrief history of Bayesian inference\nReview of basic probability\nIntroduction to Bayesian workflow\nBayes’s rule for events\nBinomial model and the Bayesian Crank\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "href": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "title": "Bayesian Inference",
    "section": "Overview of the class",
    "text": "Overview of the class\n\nSyllabus"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "href": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "title": "Bayesian Inference",
    "section": "Statistics vs. AI/ML",
    "text": "Statistics vs. AI/ML\n⚠️ What follows is an oversimplified opinion.\n\n\n\n\nAI is great for automating tasks that humans find easy\n\nRecognizing faces, cats, and other objects\nIdentifying tumors on a radiology scan\nPlaying Chess and Go\nDriving a car\nGenerating reasonable-sounding text\n\n\n\n\nStatistics is great at answering questions that humans find hard\n\nHow fast does a drug clear from the body?\nWhat is the expected tumor size 2 months after treatment?\nHow would patients respond under a different treatment?\nShould I take this drug?\n\n\n\n\n\n“Machine learning excels when you have lots of training data that can be reasonably modeled as exchangeable with your test data; Bayesian inference excels when your data are sparse and your model is dense.” — Andrew Gelman"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "href": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "title": "Bayesian Inference",
    "section": "Brief History",
    "text": "Brief History\nSummary of the book The Theory That Would Not Die\n\n\n\n\nThomas Bayes (1702(?) — 1761) is credited with the discovery of the “Bayes’s Rule”\nHis paper was published posthumously by Richard Price in 1763\nLaplace (1749 — 1827) independently discovered the rule and published it in 1774\nScientific context: Newton’s Principia was published in 1687\nBayesian wins: German Enigma cipher, search for a missing H-bomb, Federalist papers, Moneyball, political forecasting, …\n\n\n\n\n\n\n\n\n\n\n\nStephen Stigler gives 3:1 in favor of Nicholas Saunderson for the discovery of Bayes’s [sic] rule."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "href": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "title": "Bayesian Inference",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world.”"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "href": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "title": "Bayesian Inference",
    "section": "Modern Examples of Bayesian Analyses",
    "text": "Modern Examples of Bayesian Analyses\n\n\nOpenAI DALL·E: Pierre Simon Laplace in the style of Wassily Kandinsky"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis",
    "title": "Bayesian Inference",
    "section": "Vote Share Analysis",
    "text": "Vote Share Analysis\n\n\nGelman, A. (2010). Breaking down the 2008 vote. In Atlas of the 2008 Elections."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#pharmacometrics",
    "href": "bayes-course/01-lecture/01-lecture.html#pharmacometrics",
    "title": "Bayesian Inference",
    "section": "Pharmacometrics",
    "text": "Pharmacometrics"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#medical-decision-making",
    "href": "bayes-course/01-lecture/01-lecture.html#medical-decision-making",
    "title": "Bayesian Inference",
    "section": "Medical Decision Making",
    "text": "Medical Decision Making\n\n\nJoensuu, H., Vehtari, A., et al. (2012). Risk of recurrence of gastrointestinal stromal tumour after surgery: An analysis of pooled population-based cohorts. The Lancet Oncology, 13(3), 265–274."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "title": "Bayesian Inference",
    "section": "Review of Probability",
    "text": "Review of Probability\n\n\nA set of all possible outcomes is called a sample space and denoted by \\(\\Omega\\)\nAn outcome of an experiment is denoted by \\(\\omega \\in \\Omega\\)\nWe typically denote events by capital letters, say \\(A \\subseteq \\Omega\\)\nAxioms of probability:\n\n\\(\\P(A) \\geq 0, \\, \\text{for all } A\\)\n\\(\\P(\\Omega) = 1\\)\nIf \\(A_1, A_2, A_3, \\ldots\\) are disjoint: \\(\\P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} \\P(A_i)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example",
    "href": "bayes-course/01-lecture/01-lecture.html#example",
    "title": "Bayesian Inference",
    "section": "Example",
    "text": "Example\n\nRolling diceOmega\n\n\n\n\nYou roll a fair six-sided die twice\nGive an example of an \\(\\omega \\in \\Omega\\)\nHow many elements are in \\(\\Omega\\)? What is \\(\\Omega\\)?\nDefine an event \\(A\\) as the sum of the two rolls less than 11\nHow many elements are in \\(A\\)?\nWhat is \\(\\P(A)\\)?\n\n\n\n\n\n\n\nouter(1:6, 1:6, FUN = \"+\")\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    2    3    4    5    6    7\n[2,]    3    4    5    6    7    8\n[3,]    4    5    6    7    8    9\n[4,]    5    6    7    8    9   10\n[5,]    6    7    8    9   10   11\n[6,]    7    8    9   10   11   12\n\n\n\n\n\n\\(\\Omega\\) consists of all pairs \\(\\{(1, 1), (1, 2), ... (2, 1), (2, 2), ...\\}\\)\nThere are 36 such pairs\n33 of those pairs result in a sum of less than 11\nTherefore, \\(\\P(A) = \\frac{33}{36} = \\frac{11}{12}\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "href": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "title": "Bayesian Inference",
    "section": "Random Variables Review",
    "text": "Random Variables Review\n\nReviewMapping\n\n\n\n\nRandom variable is not random – it is a deterministic mapping from the sample space onto the real line; randomness comes from the experiment\nPMF, PDF, CDF (Blitzstein and Hwang, Ch. 3, 5)\nExpectations (Blitzstein and Hwang, Ch. 4)\nJoint Distributions (Blitzstein and Hwang, Ch. 7)\nConditional Expectations (Blitzstein and Hwang, Ch. 9)\n\n\n\n\n\nRandom variable X for the number of Heads in two flips"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "href": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "title": "Bayesian Inference",
    "section": "Example Simulation",
    "text": "Example Simulation\n\n\nWe will use R’s sample() function to simulate rolls of a die and replicate() function to repeat the rolling process many times\nModern approach: purrr::map(1:n, \\(i) expression)\n\n\n\n\n\ndie <- 1:6\nroll <- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\nroll(die, 2) # roll the die twice\n\n[1] 2 4\n\nrolls <- replicate(1e4, roll(die, 2))\nrolls[, 1:6]\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    3    3    1    4    3    6\n[2,]    2    1    6    3    6    6\n\nroll_sums <- colSums(rolls)\nhead(roll_sums)\n\n[1]  5  4  7  7  9 12\n\nmean(roll_sums < 11) \n\n[1] 0.9197\n\n\n\n\n\nGiven a Random Variable \\(Y\\), \\(y^{(1)}, y^{(2)}, y^{(3)}, \\ldots, y^N\\) are simulations or draws from \\(Y\\)\nFundamental bridge (Blitzstein & Hwang p. 164): \\(\\P(A) = \\E(\\I_A)\\)\nComputationally: \\(\\P(Y < 11) \\approx \\frac{1}{N} \\sum^{N}_{n=1} \\I(y^n < 11)\\)\nIn R, roll_sums < 11 creates an indicator variable\nAnd mean() does the average"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "title": "Bayesian Inference",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nFor arbitrary \\(A\\) and \\(B\\), if \\(\\P(B) > 0\\):\n\nConditional probability: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)}\\)\nConditional probability: \\(\\P(B \\mid A) = \\frac{\\P(AB)}{\\P(A)}\\)\n\nBayes’s rule: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)} = \\frac{\\P(B \\mid A) \\P(A)}{\\P(B)}\\)\n\\(A\\) and \\(B\\) are independent if observing \\(B\\) does not give you any more information about \\(A\\): \\(\\P(A \\mid B) = \\P(A)\\)\nAnd \\(\\P(A B) = \\P(A) \\P(B)\\) (from Bayes’s rule)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\nTitanic carried approximately 2,200 passengers and sank on 15 April 1912\nLet \\(G: \\{m, f\\}\\) represent Gender and \\(S: \\{n, y\\}\\) represent Survival\n\n\n\n\nsurv <- apply(Titanic, c(2, 4), sum) |> \n  as_tibble()\nsurv_prop <- round(surv / sum(surv), 3)\nbind_cols(Sex = c(\"Male\", \"Female\"), \n          surv_prop) |> \n  adorn_totals(c(\"row\", \"col\")) |>\n  knitr::kable(caption = \n               \"Titanic survival proportions\")\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n\n\n\n\\(\\P(G = m \\cap S = y) =\\) 0.167\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) =\\) ??\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) = 0.323\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\n\n\nWhat is \\(\\P(G = m \\mid S = y)\\), probability of being male given survival?\nTo compute that, we only consider the column  where S = Yes\n\\(\\P(G = m \\mid S = y) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(S = y)} = \\frac{0.167}{0.323} \\\\ \\approx 0.52\\)\nYou want \\(\\P(S = y \\mid G = m)\\), comparing it to \\(\\P(S = y \\mid G = f)\\)\n\\(\\P(S = y \\mid G = m) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(G = m)} = \\frac{0.167}{0.787} \\\\ \\approx 0.21\\)\n\\(\\P(S = y \\mid G = f) = \\frac{\\P(G = f \\, \\cap \\, S = y)}{\\P(G = f)} = \\frac{0.156}{0.213} \\\\ \\approx 0.73\\)\nHow would you calculate \\(\\P(S = n \\mid G = m)\\)?\n\\(\\P(S = n \\mid G = m) = 1 - \\P(S = y \\mid G = m)\\)\n\n\n\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n “Untergang der Titanic”, as conceived by Willy Stöwer, 1912"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "href": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "title": "Bayesian Inference",
    "section": "Law of Total Probability (LOTP)",
    "text": "Law of Total Probability (LOTP)\n\\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B \\cap A_i) = \\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)\n\\]\n\n\nImage from Blitzstein and Hwang (2019), Page 55"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "href": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "title": "Bayesian Inference",
    "section": "Bayes’s Rule for Events",
    "text": "Bayes’s Rule for Events\n\n\nWe can combine the definition of conditional probability with the LOTP to come up with Bayes’s rule for events, assuming \\(\\P(B) \\neq 0\\)\n\n\\[\n\\P(A \\mid B) = \\frac{\\P(B \\cap A)}{\\P(B)} =\n               \\frac{\\P(B \\mid A) \\P(A)}{\\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)}\n\\]\n\nWe typically think of \\(A\\) is some unknown we wish to learn (e.g., the status of a disease) and \\(B\\) as the data we observe (e.g., the result of a diagnostic test)\nWe call \\(\\P(A)\\) prior probability of A (e.g., how prevalent is the disease in the population)\nWe call \\(\\P(A \\mid B)\\), the posterior probability of the unknown \\(A\\) given data \\(B\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "href": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "title": "Bayesian Inference",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(\\P(D^+ \\mid T^+)\\)\n\\(\\text{Specificity } := \\P(T^- \\mid D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - \\P(T^- \\mid D^-) = \\P(T^+ \\mid D^-) = 0.002\\)\n\\(\\text{Sensitivity } := \\P(T^+ \\mid D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - \\P(T^+ \\mid D^+) = \\P(T^- \\mid D^+) = 0.546\\)\nPrevalence: \\(\\P(D^+) = 0.001\\)\n\n\\[\n\\begin{eqnarray}\n\\P(D^+ \\mid T^+) = \\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+)} & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\sum_{i=1}^{n}\\P(T^+ \\mid D^i) \\P(D^i) } & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+ \\mid D^+) \\P(D^+) + \\P(T^+ \\mid D^-) \\P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "title": "Bayesian Inference",
    "section": "Bayesian Analysis",
    "text": "Bayesian Analysis\n\n\n\n\nSuppose, we enrolled 5 people in an early cancer clinical trial\nEach person was given an active treatment\nFrom previous trials, we have some idea of historical response rates (proportion of people responding)\nAt the end of the trial, \\(Y = y \\in \\{0,1,...,5 \\}\\) people will have responded1 to the treatment\nWe are interested in estimating the probability that the response rate is greater great or equal to 50%\n\n\n\n Image from Fokko Smits, Martijn Dirksen, and Ivo Schoots: RECIST 1.1 - and more\n\n\nPartial response is a decrease in the sum of the longest diameters of target lesions of at least 30% compared to the baseline measurement."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "href": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "title": "Bayesian Inference",
    "section": "Notation Confusion",
    "text": "Notation Confusion\n\n\n\\(\\theta\\): unknowns or parameters to be estimated; could be multivariate, discrete, and continuous (your book uses \\(\\pi\\))\n\\(y\\): observations or measurements to be modelled (\\(y_1, y_2, ...\\))\n\\(\\widetilde{y}\\) : unobserved but observable quantities (in your book \\(y'\\))\n\\(x\\): covariates\n\\(f( \\theta )\\): a prior model, P[DM]F of \\(\\theta\\)\n\\(f_y(y \\mid \\theta, x)\\): an observational model, P[DM]F when it is a function of \\(y\\) (in your book: \\(f(y \\mid \\pi)\\)); we typically drop the \\(x\\) to simplify the notation.\n\\(f_{\\theta}(y \\mid \\theta)\\): is a likelihood function when it is a function of \\(\\theta\\) (in your book: \\(\\L(\\pi \\mid y)\\))\nSome people write \\(\\L(\\theta; y)\\) or simply \\(\\L(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "href": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "title": "Bayesian Inference",
    "section": "General Approach",
    "text": "General Approach\n\n\nBefore observing the data, we need to specify a prior model \\(f(\\theta)\\) on all unknowns \\(\\theta\\)1\nPick a data model \\(f(y | \\theta, x)\\) — this is typically more important than the prior; this includes the model for the conditional mean: \\(\\E(y | x)\\)\nFor more complex models, we construct a prior predictive distribution, \\(f(y)\\)\n\nWe will define this quantity later — it will help us assess if our choice of priors makes sense on the observational scale: \\(f(\\theta) \\rightarrow f(y)\\)\n\nAfter we observe data \\(y\\), we treat \\(f(y | \\theta, x)\\) as the likelihood of observing \\(y\\) under all plausible values of \\(\\theta\\)\nDerive a posterior model for \\(\\theta\\), \\(\\, f(\\theta | y, x)\\) using Bayes’s rule or by simulation\nEvaluate model quality: 1) quality of the inferences; 2) quality of predictions. Revise the model if necessary\nCompute all the quantities of interest from the posterior, such as event probabilities, e.g., \\(\\P(\\theta > 0.5)\\), posterior predictive distribution \\(f(\\widetilde{y} | y)\\), decision functions, etc.\n\n\n\nFor a more complete workflow, see Bayesian Workflow by Gelman et al. (2020)\n\nThere is always a prior, even in frequentist inference."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "href": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "title": "Bayesian Inference",
    "section": "Example Prior Model",
    "text": "Example Prior Model\n\n\nWe will construct a prior model for our clinical trial\nFrom previous trials, we construct a discretized version of the prior distribution of the resposne rate\nThe most likely value for response rate is 30%\n\n\n\ndot_plot <- function(x, y) {\n  p <- ggplot(data.frame(x, y), aes(x, y))\n  p + geom_point(aes(x = x, y = y), size = 0.5) +\n    geom_segment(aes(x = x, y = 0, xend = x, \n                     yend = y), linewidth = 0.2) +\n    xlab(expression(theta)) + \n    ylab(expression(f(theta)))\n}\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\ndot_plot(theta, prior) +\n  ggtitle(\"Prior probability of response\")\n\n\n\n\n\n\n\n\n\n\n\nEven though \\(\\theta\\) is a continuous parameter, we can still specify a discrete prior\nThe posterior will also be discrete and of the same cardinality"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#data-model",
    "href": "bayes-course/01-lecture/01-lecture.html#data-model",
    "title": "Bayesian Inference",
    "section": "Data Model",
    "text": "Data Model\n\n\nWe consider each person’s response rate to be independent, given the treatment\nWe have a fixed number of people in the trial, \\(N = 5\\), and \\(0\\) to \\(5\\) successes\nWe will therefore consider: \\(y | \\theta \\sim \\text{Bin}(N,\\theta) = \\text{Bin}(5,\\theta)\\)\n\\(f(y \\mid \\theta) = \\text{Bin} (y \\mid 5,\\theta) = \\binom{5}{y} \\theta^y (1 - \\theta)^{5 - y}\\) for \\(y \\in \\{0,1,\\ldots,5\\}\\)\nIs this a valid probability distribution as a function of \\(y\\)?\n\n\nN = 5; y <- 0:N; theta <- 0.5\n(f_y <- dbinom(x = y, size = N, prob = theta) |>\n    fractions())\n\n[1] 1/32 5/32 5/16 5/16 5/32 1/32\n\nsum(dbinom(x = y, size = N, prob = theta))\n\n[1] 1"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\nWe ran the trial and observed 3 out of 5 responders\nWe can now construct a likelihood function for \\(y = 3\\) as a function of \\(\\theta\\)\nLet’s check if this function is a probability distribution: \\[\nf (y) = \\int_{0}^{1} \\binom{N}{y} \\theta^y (1 - \\theta)^{N - y}\\, d\\theta = \\frac{1}{N + 1}\n\\]\n\n\ndbinom_theta <- function(theta, N, y) {\n  choose(N, y) * theta^y * (1 - theta)^(N - y) \n}\nintegrate(dbinom_theta, lower = 0, upper = 1, \n          N = 5, y = 3)[[1]] |> fractions()\n\n[1] 1/6\n\n\n\n\\(f(y)\\) is called marginal distribution of the data or, more aptly, prior predictive distribution\nIt tells us that prior to observing \\(y\\), all values of \\(y\\) are equally likely"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\n\n\n\n\n\n\n\nCompare with the data model"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "href": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "title": "Bayesian Inference",
    "section": "Compute the Posterior",
    "text": "Compute the Posterior\n\n\n\n\nFirst, we compute the likelihood\n\n\\(\\binom{5}{3} \\theta^3 (1 - \\theta)^{2}\\) for \\(\\theta \\in \\{0.10, 0.30, 0.50, 0.70, 0.90\\}\\)\n\nNext, we multiply the likelihood by the prior, to get the numerator\n\n\\(f(y = 3 \\mid \\theta) f(\\theta)\\)\n\nSum the numerator to get the marginal likelihood\n\n\\(f(y = 3) = \\sum_{\\theta} f(y = 3 | \\theta) f(\\theta) \\approx 0.2\\)\n\nFinally, compute the posterior\n\n\\(f(\\theta \\mid y = 3) = \\frac{f(y = 3 \\mid \\theta) f(\\theta)}{\\sum_{\\theta} f(y = 3 | \\theta) f(\\theta)}\\)\n\n\n\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "title": "Bayesian Inference",
    "section": "Computing Event Probability",
    "text": "Computing Event Probability\n\n\n\n\nTo compute event probabilities, we integrate (or sum) the relevant regions of the parameter space \\[\n\\P(\\theta \\geq 0.5) = \\int_{0.5}^{1} f(\\theta \\mid y) \\, d\\theta\n\\]\nIn this case, we only have discrete quantities, so we sum:\n\n\n\nprobs <- d |>\n  filter(theta >= 0.50) |>\n  dplyr::select(prior, post) |>\n  colSums() |>\n  round(2)\nprobs\n\nprior  post \n 0.50  0.71 \n\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00 \n  \n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.5 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.71"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "href": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "title": "Bayesian Inference",
    "section": "What If We Used a Flat Prior?",
    "text": "What If We Used a Flat Prior?\n\nFlat or uniform prior means that we consider all values of \\(\\theta\\) equality likely\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.20, 0.20, 0.20, 0.20, 0.20)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.2 \n    0.01 \n    0.00 \n    0.01 \n  \n  \n    0.3 \n    0.2 \n    0.13 \n    0.03 \n    0.16 \n  \n  \n    0.5 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.7 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.9 \n    0.2 \n    0.07 \n    0.01 \n    0.09 \n  \n  \n    Total \n    1.0 \n    0.83 \n    0.17 \n    1.00 \n  \n\n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.6 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.83"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\n\nGelman, A. et al. (2020). Bayesian Workflow. ArXiv:2011.01808 [Stat]. http://arxiv.org/abs/2011.01808"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#homework",
    "href": "bayes-course/01-lecture/01-lecture.html#homework",
    "title": "Bayesian Inference",
    "section": "Homework",
    "text": "Homework\nHomework is due on Monday before 5 pm.\n\nExercise 2.3 (Binomial practice)\nExercise 2.10 (LGBTQ students: rural and urban)\nExercise 2.15 (Cuckoo birds)\nExercise 2.19 (Cuckoo birds redux)\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "title": "Bayesian Inference",
    "section": "Conjugate Models and Posterior Sampling",
    "text": "Conjugate Models and Posterior Sampling\n\n\nGamma-Poisson family\nNormal-Normal family\nIntroduction to posterior sampling on a grid\nIntroduction to Stan\nBasic Markov Chain diagnostics\nEffective sample size\nComputing the R-Hat statistic\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson",
    "title": "Bayesian Inference",
    "section": "Poisson",
    "text": "Poisson\n\n\nPoisson distribution arises when we are interested in counts\nIn practice, we rarely use Poisson due to its restrictive nature\nPoisson RV \\(Y\\) is paremeterized with a rate of occurance \\(\\lambda\\): \\(Y|\\lambda \\sim \\text{Pois}(\\lambda)\\)\n\n\\[\nf(y \\mid \\lambda) =  \\frac{e^{-\\lambda} \\lambda^y}{y!}\\;\\; \\text{ for } y \\in \\{0,1,2,\\ldots\\}\n\\]\n\nNotice, the Tailor series for \\(e^\\lambda = \\sum_{y=0}^{\\infty} \\frac{\\lambda^y}{y!}\\) immediately validates that \\(f\\) is a PDF\nAlso, \\(\\E(Y | \\lambda) = \\V(Y | \\lambda) = \\lambda\\), which is the restrictive case mentioned about — real count data seldom satisfied this constraint"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "title": "Bayesian Inference",
    "section": "Visualizing Poission",
    "text": "Visualizing Poission\n\nNotice that as the rate increases, so does the expected number of events as well as variance, which immediately follows from \\(\\E(Y) = \\V(Y) = \\lambda\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#motivating-example",
    "href": "bayes-course/03-lecture/03-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nServing in Prussian cavalry in 1800s was a perilous affair\nAside from the usual dangers of military service, you were at risk of being killed by a horse kick\nData from the book The Law of Small Numbers by Ladislaus Bortkiewicz (1898)\nBortkiewicz was a Russian economist and statistician of Polish ancestry"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "title": "Bayesian Inference",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\n\n\nAssume we observe \\(Y_1, Y_2, ..., Y_n\\) independant Poisson random variables\nThe joint lilelihood, a function of the parameter \\(\\lambda\\) for \\(y_i \\in \\mathbb{Z}^+\\) and \\(\\lambda > 0\\), is given by:\n\n\\[\n\\begin{eqnarray}\nf(y \\mid \\lambda) & = & \\prod_{i=1}^n f(y_i \\mid \\lambda) = f(y_1 \\mid \\lambda)  f(y_2 \\mid \\lambda) \\cdots f(y_n \\mid \\lambda)  =  \\prod_{i=1}^{n}\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\\\\n& = &\\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\cdot \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\cdots \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n& =  &\\frac{\\left(\\lambda^{y_1}\\lambda^{y_2} \\cdots \\lambda^{y_n}\\right) \\left(e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}\\right)}{y_1! y_2! \\cdots y_n!} \\\\\n& = &\\frac{\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i!} \\propto\n\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\n\\end{eqnarray}\n\\]\n\nWe call \\(\\sum_{i=1}^{n} y_i\\) a sufficient statistic"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "title": "Bayesian Inference",
    "section": "Conjugate Prior for Poisson",
    "text": "Conjugate Prior for Poisson\n\n\nThe likelihood has a form of \\(\\lambda^{x} e^{-y\\lambda}\\) so we expect the conjugate prior to be of the same form\nGamma PDF satisfied this condition: \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nMatching up the exponents, we can interpret \\((\\alpha - 1)\\) as the total number of incidents \\(\\sum y_i\\) out of \\(\\beta\\) observations \\(n\\)\nFull Gamma density is \\(\\text{Gamma}(\\lambda|\\alpha,\\beta)=\\frac{\\beta^{\\alpha}} {\\Gamma(\\alpha)} \\, \\lambda^{\\alpha - 1}e^{-\\beta \\, \\lambda}\\) for \\(\\lambda, \\alpha, \\beta \\in \\mathbb{R}^+\\)\n\n\\[\n\\begin{equation}\n\\begin{split}\n\\E(\\lambda) & = \\frac{\\alpha}{\\beta} \\\\\n\\text{Mode}(\\lambda) & = \\frac{\\alpha - 1}{\\beta} \\;\\; \\text{ for } \\alpha \\ge 1 \\\\\n\\V(\\lambda) & = \\frac{\\alpha}{\\beta^2} \\\\\n\\end{split}\n\\end{equation}\n\\]\n\nWhen \\(\\alpha = 1\\), \\(\\lambda \\sim \\text{Exp}(\\beta)\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "title": "Bayesian Inference",
    "section": "Visualizing Gamma",
    "text": "Visualizing Gamma\n\n\nNotice that variance, mean, and mode are increasing with \\(\\alpha\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "title": "Bayesian Inference",
    "section": "Gamma Posterior",
    "text": "Gamma Posterior\n\n\nPrior is \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nLikelihood is \\(f(y | \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\\)\n\n\\[\n\\begin{eqnarray}\nf(\\lambda \\mid y) & \\propto & \\text{prior} \\cdot \\text{likelihood} \\\\\n& = & \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\cdot  \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda} \\\\\n& = & \\lambda^{\\alpha + \\sum_{i=1}^{n} y_i - 1} \\cdot e^{-\\beta\\lambda - n\\lambda} \\\\\n& = & \\lambda^{\\color{red}{\\alpha + \\sum_{i=1}^{n} y_i} - 1} \\cdot e^{-\\color{red}{(\\beta + n)} \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma}\\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + n \\right)\n\\end{eqnarray}\n\\]\n\nAs before, we can guess the Gamma kernel without doing the integration"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "title": "Bayesian Inference",
    "section": "Checking the Constant",
    "text": "Checking the Constant\n\n\nGamma prior integration constant: \\(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\)\nThe posterior kernel integration constant must be: \\(\\frac{\\Gamma(\\alpha + \\sum y_i)}{(\\beta + n)^{\\alpha + \\sum y_i}}\\)\nWe can sanity check this in Mathematica where \\(t = \\sum y_i\\):"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n\nFrom 1875 to 1894, there were 14 different cavalry corps\nEach reported a number of deaths by horse kick every year\nThere are 20 years x 14 corps making 280 observations\n\n\n\n\n\n\n\n\n  \n    \n    \n      Year\n      GC\n      C1\n      C2\n      C3\n      C4\n      C5\n      C6\n      C7\n      C8\n      C9\n      C10\n      C11\n      C14\n      C15\n    \n  \n  \n    1875\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n    1876\n2\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n    1877\n2\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\n2\n0\n    1878\n1\n2\n2\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n    1879\n0\n0\n0\n1\n1\n2\n2\n0\n1\n0\n0\n2\n1\n0\n    1880\n0\n3\n2\n1\n1\n1\n0\n0\n0\n2\n1\n4\n3\n0\n    1881\n1\n0\n0\n2\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n    1882\n1\n2\n0\n0\n0\n0\n1\n0\n1\n1\n2\n1\n4\n1"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n# this flattens the data frame into a vector\ndd <- unlist(d[, -1]) \np <- ggplot(aes(y), data = data.frame(y = dd))\np1 <- p + geom_histogram() +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Total reported counts of deaths\")\np2 <- p + geom_histogram(aes(y = ..count../sum(..count..))) +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Proportion of reported counts of deaths\")\n\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\nPrior and PosteriorPlots\n\n\n\nLet’s assume that before seeing the data, your friend told you that last year, in 1874, there were no deaths reported in his corps\nThat would imply \\(\\beta = 1\\) and \\(\\alpha - 1 = 0\\) or \\(\\alpha = 1\\)\nThe prior on lambda would therefore be \\(\\text{Gamma}(1, 1)\\)\n\n\nN <- length(dd)\nsum_yi <- sum(dd)\ny_bar <- sum_yi/N\ncat(\"Total number of observations N =\", N)\n\nTotal number of observations N = 280\n\ncat(\"Total number of deaths =\", sum_yi)\n\nTotal number of deaths = 196\n\ncat(\"Average number of deaths =\", y_bar)\n\nAverage number of deaths = 0.7\n\n\n\nThe posterior is \\(\\text{Gamma}\\left( \\alpha + \\sum y_i, \\, \\beta + N \\right) = \\text{Gamma}(197, \\, 281)\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "href": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "title": "Bayesian Inference",
    "section": "Likelihood Dominates",
    "text": "Likelihood Dominates\n\n\n\nWith so much data relative to prior observations, the likelihood completely dominates the prior\n\n\nplot_gamma_poisson(1, 1, sum_y = sum_yi, n = N) + \n  xlim(0, 3)\n\n\n\n\n\n\n\n\n\n\nmap <- (1 + sum_yi - 1) / (1 + N) \nsd_post <- sqrt((1 + sum_yi) / (1 + N)^2) \ncat(\"Mode or MAP =\", map |> round(3))\n\nMode or MAP = 0.698\n\ncat(\"Average rate =\", y_bar)\n\nAverage rate = 0.7\n\ncat(\"Posterior sd =\", sd_post |> round(2))\n\nPosterior sd = 0.05"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "title": "Bayesian Inference",
    "section": "Checking the Fit",
    "text": "Checking the Fit\n\nWe can plug the MAP estimate for \\(\\lambda\\) into the Poisson PMF\nLater, we will learn how to account for uncertainty in \\(\\lambda\\) using the posterior predictive distribution\n\n\n# prediction not accounting for uncertainty in lambda\nmap <- (1 + sum_yi - 1) / (1 + N) \ndeaths <- 0:4\npred <- dpois(deaths, lambda = map)\nactual <- as.numeric(table(dd) / N)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "href": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "title": "Bayesian Inference",
    "section": "Adding Exposure",
    "text": "Adding Exposure\n\n\nPoisson likelihood seems to work well for these data\nIt is likely that each of the 14 corps had about the same number of soldier-horses, a common military practice\nSuppose each corps has a different number of cavalrymen\nWe need to introduce an exposure variable \\(x_i\\) for each corps unit \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(x_i \\lambda)\\\\\n\\lambda & \\sim & \\text{Gamma}(\\alpha, \\beta) \\\\\nf(y \\mid \\lambda) & \\propto & \\lambda^{\\sum_{i=1}^{n} y_i}e^{- (\\sum_{i=1}^{n} x_i) \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma} \\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + \\sum_{i=1}^{n} x_i \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\nThe last conjugate distribution we will introduce is Normal\nWe will only consider a somewhat unrealistic case of known variance \\(\\sigma \\in \\mathbb{R}^+\\) and unknown mean \\(\\mu \\in \\mathbb{R}\\)\nNormal PDF is for one observation \\(y\\) is given by: \\[\n\\begin{eqnarray}\n\\text{Normal}(y \\mid \\mu,\\sigma) & = &\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\E(Y) & = & \\text{ Mode}(Y) = \\mu \\\\\n\\V(Y) & = & \\sigma^2 \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\nNormal arises when many independent small contributions are added up\nIt is a limiting distribution of means of an arbitrary distribution\n\n\n\nplot_xbar <- function(n_repl, n_samples) {\n  x <- seq(0.6, 1.4, len = 100)\n  xbar <- replicate(n_repl, \n                    mean(rexp(n_samples, rate = 1)))\n  mu <- dnorm(x, mean = 1, sd = 1/sqrt(n_samples))\n  p <- ggplot(aes(x = xbar), \n              data = tibble(xbar))\n  p + geom_histogram(aes(y = ..density..), \n                     bins = 30, alpha = 0.6) +\n    geom_line(aes(x = x, y = mu), \n              color = 'red', \n              linewidth = 0.3, \n              data = tibble(x, y)) +\n    ylab(\"\") + theme(axis.text.y = element_blank())\n}\n\np1 <- plot_xbar(1e4, 100) + \n  ggtitle(\"Sampling means from rexp(100, 1)\")\np2 <- plot_xbar(1e4, 300) + \n  ggtitle(\"Sampling means from rexp(300, 1)\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "title": "Bayesian Inference",
    "section": "Joint Normal Likelihood",
    "text": "Joint Normal Likelihood\n\n\nAfter observing data \\(y\\), we can compute the joint normal likelihood, assuming \\(\\sigma\\) is known \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n& \\propto & \\prod_{i=1}^{n} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2 \\right) \\\\  \n& = & \\exp \\left( {-\\frac{\\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2}}\\right) \\\\\n&\\propto& \\exp\\left({-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\\right)\n\\end{eqnarray}\n\\]\nThe last line is derived by expanding the square and dropping terms that don’t depend on \\(\\mu\\); \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "title": "Bayesian Inference",
    "section": "Normal Prior",
    "text": "Normal Prior\n\n\nWe can now define the prior on \\(\\mu\\)\nWe will choose \\(\\mu\\) to be normal: \\(\\mu \\sim \\text{Normal}(\\theta, \\tau^2)\\) \\[\n\\begin{eqnarray}\nf(\\mu \\mid \\theta, \\tau) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\nf(\\mu \\mid y) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "title": "Bayesian Inference",
    "section": "Normal Posterior",
    "text": "Normal Posterior\n\\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\nf(\\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\nf(\\mu \\mid y) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\n& = & \\exp \\left( -\\frac{1}{2} \\left(  \\frac{(\\mu - \\theta)^2}{\\tau^2} + \\frac{(y - \\mu)^2}{\\sigma^2} \\right)\\right) \\\\\n& = &  \\exp \\left(  -\\frac{1}{2\\tau_1^2} \\left( \\mu - \\mu_1 \\right)^2    \\right)\n\\end{eqnarray}\n\\]\n\\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{1}{\\sigma^2} y}{\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} = \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}\n\\] ## Multivariate {.smaller} - For multivariate outcomes:\n\\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{n}{\\sigma^2} \\overline{y}}{\\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "title": "Bayesian Inference",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nAriannaThe Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArianna Rosenbluth Dies at 93, The New York Times"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\n\n\nMost posterior distributions do not have an analytical form\nIn those cases, we must resort to sampling methods\nSampling in high dimensions requires specialized algorithms\nHere, we will look at one-dimensional sampling on the grid (of parameter values)\nAt the end of this lecture, we look at some output of a state-of-the-art HMC NUTS sampler\nNext week, we will examine the Metropolis-Hastings-Rosenbluth algorithm"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\nWe already saw how given samples from the target distribution, we can compute quantities of interest\nThe grid approach to getting samples:\n\nGenerate discreet points in parameter space \\(\\theta\\)\nDefine our likelihood function \\(f(y|\\theta)\\) and prior \\(f(\\theta)\\)\nFor each point on the grid, compute the product \\(f(y|\\theta)f(\\theta)\\)\nNormalize the product to sum to 1\nSample from the resulting distribution in proportion to the posterior probability\n\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "href": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "title": "Bayesian Inference",
    "section": "Early Clinical Trial Example",
    "text": "Early Clinical Trial Example\n\nDeriving \\(f(\\theta, y)\\)R ImplementationGraphs\n\n\n\n\nFor simplicity we will assume uniform \\(\\text{Beta}(1, 1)\\) prior \\[\n\\begin{eqnarray}\nf(\\theta)f(y\\mid \\theta) &\\propto& \\theta^{a -1}(1 - \\theta)^{b-1} \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^0(1 - \\theta)^0 \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^{\\sum_{i=1}^{n} y_i} \\cdot (1 - \\theta)^{\\sum_{i=1}^{n} (1- y_i)}\n\\end{eqnarray}\n\\]\nOn the log scale: \\(\\log f(\\theta, y) \\propto \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\nRecall we had 3 responders out of 5 patients\nModel: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\nlp <- function(theta, data) {\n# log(theta) * sum(data$y) + log(1 - theta) * sum(1 - data$y)\n  lp <- 0\n  for (i in 1:data$N) {\n    lp <- lp + log(theta) * data$y[i] + log(1 - theta) * (1 - data$y[i])\n  }\n  return(lp)\n}\ndata <- list(N = 5, y = c(0, 1, 1, 0, 1))\n# generate theta parameter grid\ntheta <- seq(0.01, 0.99, len = 100)\n# compute log likelihood and prior for every value of the grid\nlog_lik <- lp(theta, data); log_prior <- log(dbeta(theta, 1, 1))\n# compute log posterior\nlog_post <- log_lik + log_prior\n# covert back to the original scale and normalize\npost <- exp(log_post); post <- post / sum(post)\n# sample theta in proportion to the posterior probability\ndraws <- sample(theta, size = 1e5, replace = TRUE, prob = post)\n\n\n\n\nFrom the first lecture, we know the posterior is \\(\\text{Beta}(1 + 3, 1 + 5 - 3) = \\text{Beta}(4, 3)\\)\nWe can compare this density to the posterior draws\n\n\nbeta_dens <- dbeta(theta, 4, 3)\n(mle <- sum(data$y) / data$N)\n\n[1] 0.6"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "href": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "title": "Bayesian Inference",
    "section": "Monte Carlo Integration and MCMC",
    "text": "Monte Carlo Integration and MCMC\n\nIntroductionExample 1Example 2MCMC\n\n\n\n\nWe already saw a special case of MC integration\nSuppose we can draw samples from PDF \\(f\\), \\((\\theta^{(1)}, \\theta^{(2)},..., \\theta^{(N)})\\)\nIf we want to compute an expectation of some function \\(h\\): \\[\n\\begin{eqnarray}\n\\E_f[h(\\theta)] &=& \\int h(\\theta)f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} h \\left( \\theta^{(i)} \\right)\n\\end{eqnarray}\n\\]\nThe Law of Large Numbers tells us that these approximations improve with \\(N\\)\nIn practice, the challenge is obtaining draws from \\(f\\)?\nThat’s where MCMC comes in\n\n\n\n\n\n\nSuppose we want to estimate the mean and variance of standard normal\nIn case of the mean, \\(h(\\theta) := \\theta\\) and variance, \\(h(\\theta) := \\theta^2\\), and \\(f(\\theta) = \\frac{1}{\\sqrt{2 \\pi} \\ } e^{-\\theta^2/2}\\) \\[\n\\begin{eqnarray}\n\\E[\\theta] &=& \\int_{-\\infty}^{\\infty} \\theta f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\theta^{(i)} \\\\\n\\E[\\theta^2] &=& \\int_{-\\infty}^{\\infty} \\theta^2 f(\\theta)\\, d\\theta &\\approx&\n\\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\theta^{(i)} \\right)^2\n\\end{eqnarray}\n\\]\n\n\nN <- 1e5\ntheta <- rnorm(N, 0, 1)          # draw theta from N(0, 1)\n(1/N * sum(theta)) |> round(2)   # E(theta),   same as mean(theta)\n\n[1] 0\n\n(1/N * sum(theta^2)) |> round(2) # E(theta^2), same as mean(theta^2)\n\n[1] 1\n\n\n\n\n\n\n\nSuppose we want to estimate a CDF of Normal(0, 1) at some point \\(t\\)\nWe let \\(h(\\theta) = \\I(\\theta < t)\\), where \\(\\I\\) is an indicator function that returns 1 when \\(\\theta < t\\) and \\(0\\) otherwise \\[\n\\begin{eqnarray}\n\\E[h(\\theta)] = \\E[\\I(\\theta < t)] &=& \\int_{-\\infty}^{\\infty} \\I(\\theta < t) f(\\theta)\\, d\\theta =\n\\int_{-\\infty}^{t}f(\\theta)\\, d\\theta = \\Phi(t) \\\\\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\I(\\theta^{(i)} < t) \\\\\n\\end{eqnarray}\n\\]\n\n\n\npnorm(1, 0, 1) |> round(2)          # Evalute N(0, 1) CDF at 1\n\n[1] 0.84\n\nN <- 1e5\ntheta <- rnorm(N, 0, 1)             # draw theta from N(0, 1)\n(1/N * sum(theta < 1)) |> round(2)  # same as mean(theta < 1)\n\n[1] 0.84\n\n\n\n\n\n\nMCMC is a very general method for computing expectations (integrals)\nIt produces dependant (autocorrelated) samples, but for a good algorithm, the dependence is manageable\nStan’s MCMC algorithm is very efficient (more on that later) and requires all parameters to be continuous (data can be discrete)\nIt solves the problem of drawing from distribution \\(f(\\theta)\\) where \\(f\\) is not one of the fundamental distributions and \\(\\theta\\) is high dimentional\nWhat is high-dimensional? Modern algorithms like NUTS can jointly sample tens of thousands and more parameters\nThat’s 10,000+ dimensional integrals of complicated functions!\n\n\n\n\n\n\nInclude an example where h(x) is an indicator function, and we want to evaluate Normal CDF at some point t."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "title": "Bayesian Inference",
    "section": "Introduction to Stan",
    "text": "Introduction to Stan\n\n\nStan is a procedural, statically typed, Turning complete, probabilistic programming language\nStan language expresses a probabilistic model\nStan transpiler converts it to C++\nStan inference algorithms perform parameter estimation\nOur model: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\n// 01-bernoulli.stan\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}\n\n\n\n// 02-bernoulli.stan\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  theta ~ beta(1, 1);\n  y ~ bernoulli(theta);\n}"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#extra-credit-homework",
    "href": "bayes-course/03-lecture/03-lecture.html#extra-credit-homework",
    "title": "Bayesian Inference",
    "section": "Extra Credit Homework",
    "text": "Extra Credit Homework\n\nModify the program to incorporate Beta(2, 2) without using theta ~ beta(2, 2)\nModify the program to account for an arbitrary Beta(a, b) distribution\nVerify that you got the right result by comparing Stan’s posterior means and posterior standard deviations to the means and standard deviations under the conjugate model. You can also modify the R code to get a third point of comparison.\n\n\nHint: Pass parameters a and b in the data block\n\n\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "title": "Bayesian Inference",
    "section": "Running Stan",
    "text": "Running Stan\n\nlibrary(cmdstanr)\n\nm1 <- cmdstan_model(\"stan/01-bernoulli.stan\") # compile the model\ndata <- list(N = 5, y = c(0, 1, 1, 0, 1))\nf1 <- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 500  # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <num>  <num> <num> <num>  <num>  <num> <num>    <num>    <num>\n1 lp__     -5.26  -4.99  0.671 0.289 -6.64  -4.78   1.00     964.    1020.\n2 theta     0.579  0.587 0.167 0.180  0.285  0.842  1.00     721.     985."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "href": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "title": "Bayesian Inference",
    "section": "Working With Posterior Draws",
    "text": "Working With Posterior Draws\n\n\n\nlibrary(tidybayes)\n\ndraws <- gather_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy long format\n\n# A tibble: 6 × 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable .value\n   <int>      <int> <int> <chr>      <dbl>\n1      4        495  1995 lp__       -4.85\n2      4        496  1996 lp__       -4.93\n3      4        497  1997 lp__       -5.01\n4      4        498  1998 lp__       -5.83\n5      4        499  1999 lp__       -4.79\n6      4        500  2000 lp__       -4.99\n\ndraws |> \n  group_by(.variable) |> \n  summarize(mean = mean(.value))\n\n# A tibble: 2 × 2\n  .variable   mean\n  <chr>      <dbl>\n1 lp__      -5.26 \n2 theta      0.579\n\nmedian_qi(draws, .width = 0.90)\n\n# A tibble: 2 × 7\n  .variable .value .lower .upper .width .point .interval\n  <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 lp__      -4.99  -6.64  -4.78     0.9 median qi       \n2 theta      0.587  0.285  0.842    0.9 median qi       \n\n\n\n\ndraws <- spread_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy wide format\n\n# A tibble: 6 × 5\n  .chain .iteration .draw theta  lp__\n   <int>      <int> <int> <dbl> <dbl>\n1      4        495  1995 0.503 -4.85\n2      4        496  1996 0.467 -4.93\n3      4        497  1997 0.443 -5.01\n4      4        498  1998 0.306 -5.83\n5      4        499  1999 0.599 -4.79\n6      4        500  2000 0.688 -4.99\n\ntheta <- seq(0.01, 0.99, len = 100)\np <- ggplot(aes(theta), data = draws)\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 30, alpha = 0.6) +\n    geom_line(aes(theta, beta_dens), \n              linewidth = 0.5, color = 'red',\n              data = tibble(theta, beta_dens)) +\n  ylab(\"\") + xlab(expression(theta)) +\n  ggtitle(\"Posterior draws from Stan\")"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "title": "Bayesian Inference",
    "section": "What’s a Chain",
    "text": "What’s a Chain"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "title": "Bayesian Inference",
    "section": "Dynamic Simulation",
    "text": "Dynamic Simulation\nhttps://chi-feng.github.io/mcmc-demo/app.html"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "href": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "title": "Bayesian Inference",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\n\n\n\nIn general, you want to assess (1) the quality of the draws and (2) the quality of predictions\nThere are no guarantees in either case, but the former is easier than the latter\nThe folk theorem of statistical computing: computational problems often point to problems in the model (AG)\nWe will address quality of the draws now and the quality of predictions later in the course"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "title": "Bayesian Inference",
    "section": "Good Chain Bad Chain",
    "text": "Good Chain Bad Chain\n\nlibrary(bayesplot)\ndraws <- f1$draws(\"theta\")\ncolor_scheme_set(\"viridis\")\np1 <- mcmc_trace(draws, pars = \"theta\") + ylab(expression(theta)) +\n  ggtitle(\"Good Chain\")\nbad_post <- readRDS(\"data/bad_post.rds\")\nbad_draws <- bad_post$draws(\"mu\")\np2 <- mcmc_trace(bad_draws, pars = \"mu\") + ylab(expression(theta)) +\n  ggtitle(\"Bad Chain\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "href": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "title": "Bayesian Inference",
    "section": "Effective Sample Size and Autocorrelation",
    "text": "Effective Sample Size and Autocorrelation\n\nESSACF\n\n\n\n\nMCMC generates dependent draws from the target distribution\nDependent samples are less efficient as you need more of them to estimate the quantity of interest\nESS or \\(N_{eff}\\) is approximately how many independent samples you have\nTypically, \\(N_{eff} < N\\) for MCMC, as there is some autocorrelation\nESS should be considered relative to \\(N\\): \\(\\frac{N_{eff}}{N}\\)\nGenerally, we don’t like to see \\(\\text{ratio} < 0.10\\)\n\n\n\nneff_ratio(f1, pars = \"theta\") |> round(2)\n\ntheta \n 0.36 \n\nneff_ratio(bad_post, pars = \"mu\") |> round(2)\n\n  mu \n0.09"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "href": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "title": "Bayesian Inference",
    "section": "Computing R-Hat",
    "text": "Computing R-Hat\n\n\nAutocorrelation assesses the quality of a single chain\nSplit R-Hat estimates the extent to which the chains are consistent with one another\nIt does it by assessing the mixing of chains by comparing variances within and between chains (technically sequences, as chains are split up) \\[\n\\begin{eqnarray}\n\\hat{R} = \\sqrt{\\frac{\\frac{n-1}{n}\\V_W + \\frac{1}{n}\\V_B}{\\V_W}} = \\sqrt{\\frac{\\V_{\\text{total}}}{\\V_{W}}}\n\\end{eqnarray}\n\\]\n\\(\\V_W\\) is within chain variance and \\(\\V_B\\) is between chain variance\nWe don’t like to see R-hats greater than 1.02 and really don’t like them greater than 1.05\n\n\n\nbayesplot::rhat(f1, pars = \"theta\") |> round(3)\n\ntheta \n1.001 \n\nbayesplot::rhat(bad_post, pars = \"sigma\")  |> round(3)\n\nsigma \n1.043 \n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "href": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "title": "Bayesian Inference",
    "section": "Conjugate models and Beta-Binomial",
    "text": "Conjugate models and Beta-Binomial\n\n\nBeta distribution\nGreat expectations\nTuning the prior model for the clinical trial analysis\nBinomial likelihood with continuous \\(\\theta\\)\nDeriving the conjugate posterior\nAnalysis of the sex ratio\nCompromise between priors and data model\nCoherence of Bayesian inference\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#homework",
    "href": "bayes-course/02-lecture/02-lecture.html#homework",
    "title": "Bayesian Inference",
    "section": "Homework",
    "text": "Homework\n\nHow did you find the homework?\nHomework review"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introduction",
    "href": "bayes-course/02-lecture/02-lecture.html#introduction",
    "title": "Bayesian Inference",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\nLast time, we presented the discretized version of the prior\nIn practice, we are typically working in continuous space\nIn general, the prior model can be arbitrarily complex\nThere is a family of distributions called the exponential family, for which the prior has the same kernel as the posterior\nThis is called conjugacy\nIn practice, we seldom rely on conjugate relationships\nBeta distribution is conjugate to Binomial"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "href": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "title": "Bayesian Inference",
    "section": "Continous Probability Distributions",
    "text": "Continous Probability Distributions\n\n\nRecall that for the continuous RVs we have, the CDF is defined as \\[\n\\begin{eqnarray}\nF(x) & = & \\int_{-\\infty}^{x} f(t) \\, dt, \\, \\text{and} \\\\\nf(x) & = & F'(x), \\, \\text{where } F \\text{ is differentiable}\n\\end{eqnarray}\n\\]\nTo compute event probabilities: \\[\n\\begin{eqnarray}\n\\P(a \\le X \\leq b) & = & F(b) − F(a) = \\int_{a}^{b} f(x) \\, dx \\\\\n\\P(X \\in A) & = & \\int_{A} f(x) \\, dx\n\\end{eqnarray}\n\\]\nAs in the case of PMFs: \\[\n\\begin{eqnarray}\nf(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "title": "Bayesian Inference",
    "section": "Introducing Beta",
    "text": "Introducing Beta\n\n\n\n\nBeta distribution has the following functional form for \\(\\alpha > 0, \\, \\beta > 0, \\, \\theta \\in (0, 1)\\) \\[\n\\begin{eqnarray}\n\\text{Beta}(\\theta \\mid \\alpha,\\beta) & = &\n\\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta^{\\alpha - 1} \\, (1 -\n\\theta)^{\\beta - 1} \\\\\n\\text{B}(a,b) \\ & = & \\ \\int_0^1 u^{a - 1} (1 - u)^{b - 1} \\,\ndu \\ = \\ \\frac{\\Gamma(a) \\, \\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(x) & = &\n\\int_0^{\\infty} u^{x - 1} \\exp(-u) \\, du\n\\end{eqnarray}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFor positive integrers \\(n\\): \\(\\, \\Gamma(n+1) = n!\\) and in general \\(\\Gamma(z + 1) = z \\Gamma(z)\\)\n\\(\\text{Beta}(1, 1)\\) is equivalent to \\(\\text{Unif(0, 1)}\\)\nThe above makes Beta a natural choice for priors on the probability scale"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "title": "Bayesian Inference",
    "section": "Visualizing Beta",
    "text": "Visualizing Beta\n\n\nThe mode of Beta, \\(\\text{Mode}(\\theta) = \\argmax_\\theta \\text{Beta}(\\theta \\mid \\alpha, \\beta)\\) is shown in red and the function maximum in blue"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#expectations",
    "title": "Bayesian Inference",
    "section": "Expectations",
    "text": "Expectations\n\n\nRecall the definition of expectations for continuous random variables\nWe often write \\(\\mu\\) or \\(\\mu_X\\) for expected value of \\(X\\) \\[\n\\mu_X = \\E(X) = \\int x \\cdot f(x) \\, dx\n\\]\nVariance is a type of expectation, which we often denote by \\(\\sigma^2\\) \\[\n\\sigma_X = \\V(X) = \\E(X - \\mu)^2 = \\int (X - \\mu)^2 f(x) \\, dx\n\\]\nIt is often more convenient to write the variance as \\[\n\\V(X) = \\E(X^2) - \\mu^2\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nKeeping in mind that a Gamma function is a continuous analog of the factorial: \\[\n\\begin{eqnarray}\n\\E(\\theta) &=& \\int_{0}^{1} \\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta \\\\\n&=& \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_{0}^{1}  \\color{red}{\\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1}} \\, d\\theta \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\cdot\n\\color{red}{\\frac{\\Gamma(1 + \\alpha) \\Gamma(\\beta)}{\\Gamma(1 + a + b)}} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{\\Gamma(1 + \\alpha)}{\\Gamma(1 + \\alpha + \\beta)} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{a \\Gamma(\\alpha)}{(\\alpha + \\beta) \\Gamma(\\alpha + \\beta)} \\\\\n&=& \\frac{\\alpha}{\\alpha + \\beta}.\n\\end{eqnarray}\n\\]\nWhat is the value of \\(\\int_{0}^{1} \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta} \\, d\\theta\\)\n\n\n\nCheck the integral \\(\\int_{0}^{1} \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nWe can find the mode of this distribution by taking the log, differentiating with respect to \\(\\theta\\), and setting the derivative function to zero \\[\n\\begin{eqnarray}\n\\E(\\theta) & = & \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Mode}(\\theta) & = & \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\;\\; \\text{ when } \\; \\alpha, \\beta > 1. \\\\\n\\end{eqnarray}\n\\]\nThe variance of \\(\\theta\\) can be derived using the definition of the variance operator \\[\n\\begin{eqnarray}\n\\V(\\theta) & = & \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{eqnarray}\n\\]\nNotice when \\(\\alpha = \\beta\\), \\(\\E(\\theta) = \\text{Mode}(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\n\n\nWe borrow an example from BDA3: the probability of girl birth given placenta previa (PP)\nPlacenta previa is a condition when the placenta completely or partially covers the opening of the uterus\nA PP study in Germany found that out of 980 births, 437 or \\(\\approx 45\\%\\) were female\nSex ratio in the population has been stable over space and time at 48.5% females with deviations within no more than 1%1\nOur task is to assess the evidence for \\(\\P(\\text{ratio} < .485 \\mid \\text{PP})\\)\n\n\n\n\n\n\n\n\n\n\nGelman, A., & Weakliem, D. (2009). Of Beauty, Sex and Power. American Scientist, 97(4), 310. https://doi.org/10.1511/2009.79.310"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "href": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "title": "Bayesian Inference",
    "section": "Biomomial Likelihood",
    "text": "Biomomial Likelihood\n\n\nRecall that Likelihood is the function of the parameter \\(\\theta\\), assuming \\(\\theta \\in [0,1]\\) \\[\n\\text{Bin}(y \\mid \\theta) = \\binom{N}{y}\n\\theta^y (1 - \\theta)^{N - y} \\propto \\theta^y (1 - \\theta)^{N - y}\n\\]\nAssuming \\(N = 10\\), the likelihood for \\(\\theta\\), given a few possible values of \\(y\\) successes"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "href": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving the Posterior Distribution",
    "text": "Deriving the Posterior Distribution\n\n\nThe denominator is constant in \\(\\theta\\); we will derive the posterior up to a proportion \\[\n\\begin{eqnarray}\nf(y \\mid \\theta) & \\propto & \\theta^y (1 - \\theta)^{N - y} \\\\\nf(\\theta) & \\propto & \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\nf(\\theta \\mid y) & \\propto & f(y \\mid \\theta) f(\\theta) \\\\\n& = & \\theta^y (1 - \\theta)^{N - y} \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\n& = & \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\\\\nf(\\theta \\mid y) & = & \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\n\\end{eqnarray}\n\\]\nThe last equality comes from matching the kernel \\(\\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1}\\) to the normalized Beta PDF which has a normalizing constant \\(\\frac{\\Gamma (\\alpha +\\beta + N)}{\\Gamma (\\alpha + y) \\Gamma (\\beta + N - y)}\\)\nSince the posterior is in the same family as the prior, we say that Beta is conjugate to Binomial\n\n\n\nCheck the kernel integral, \\(\\int_{0}^{1} \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "title": "Bayesian Inference",
    "section": "Posterior Expectations",
    "text": "Posterior Expectations\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta \\mid Y=y)  & = & \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\cdot \\E(\\theta) + \\frac{n}{\\alpha + \\beta + n}\\cdot\\frac{y}{n} \\\\\n\\V(\\theta \\mid Y=y)  & = & \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)} \\\\\n\\text{Mode}(\\theta \\mid Y=y) & = & \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} = \\frac{\\alpha + \\beta - 2}{\\alpha + \\beta + n - 2} \\cdot\\text{Mode}(\\theta) + \\frac{n}{\\alpha + \\beta + n - 2} \\cdot\\frac{y}{n}\n\\end{eqnarray}\n\\]\n\n\n\nNote tha the posterior expectation is between \\(y/n\\) and \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\nAlso note that as sample becomes very large \\(\\E(\\theta \\mid Y=y) \\rightarrow \\frac{y}{n}\\) and \\(\\V(\\theta \\mid Y=y) \\rightarrow 0\\)\nAs before, \\(\\text{Mode}(\\theta \\mid Y=y) = \\E(\\theta \\mid Y=y)\\), when \\(\\alpha = \\beta\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\nLet’s derive the analytical posterior and compare it with the samples from the posterior distribution\nFirst, we will consider the uniform \\(\\text{Beta}(1, 1)\\) prior\nWe are told that data are \\(N = 980\\) and \\(y = 437\\) female births\nThe posterior is \\(\\text{Beta}(1 + 437, 1 + 980 - 437) = \\text{Beta}(438, 544)\\)\n\\(\\E(\\theta \\mid Y=437) = \\frac{\\alpha + 437}{\\alpha + \\beta + 980} = \\frac{438}{982} \\approx 0.446\\)\n\\(\\sqrt{\\V(\\theta \\mid Y=y)} \\approx\\) 0.016\n\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 438, 544) |> round(3)\nlower <- qbeta(l, 438, 544) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.415, 0.477]\n\nevent_prob <- integrate(dbeta, lower = 0, upper = 0.485, shape1 = 438, shape2 = 544)[[1]]\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993\n\npbeta(0.485, shape1 = 438, shape2 = 544) |> round(3)\n\n[1] 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "href": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "title": "Bayesian Inference",
    "section": "Obtaining Quantiles by Sampling",
    "text": "Obtaining Quantiles by Sampling\n\n\nWe can use R’s rbeta() RNG to generate draws from the posterior\n\n\n\n\n\ndraws <- rbeta(n = 1e5, 438, 544)\np <- ggplot(aes(draws), data = data.frame(draws))\np + geom_histogram(bins = 30) + ylab(\"\") +\n  geom_vline(xintercept = 0.485, \n             colour = \"red\", size = 0.3) +\n  ggtitle(\"Draws from Beta(438, 544)\") + \n  theme(axis.text.y = element_blank()) + \n  xlab(expression(theta))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the quantile() function to get the posterior interval and compute the event probability by evaluating the expectation of the indicator function as before\n\n\n\n\nquantile(draws, probs = c(0.025, 0.5, 0.975)) |> round(3)\n\n 2.5%   50% 97.5% \n0.415 0.446 0.477 \n\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", mean(draws < 0.485) |> round(3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "title": "Bayesian Inference",
    "section": "Population Priors",
    "text": "Population Priors\n\n\n\n\nWhat priors should we use if we think the sample is drawn from the sex ratio “hyper-population”?\nWe know that the population mean is 0.485 and the standard deviation is about 0.01\nBack out the parameters of the population Beta distribution\n\n\n\n\\[\n\\begin{eqnarray}\n\\begin{cases}\n\\frac{\\alpha}{\\alpha + \\beta} & = & 0.485 \\\\\n\\sqrt{\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}} & = & 0.01\n\\end{cases}\n\\end{eqnarray}\n\\]\n\n\n\n\\(\\alpha \\approx 1211\\) and \\(\\beta \\approx 1286\\)\n\n\n\n\n\nCheck the result with the simulation\n\n\n\n\nx <- rbeta(1e4, 1211, 1286)\nmean(x) |> round(3)\n\n[1] 0.485\n\nsd(x) |> round(3)\n\n[1] 0.01\n\n\n\n\n\nWe can let Mathematica do the algebra"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "title": "Bayesian Inference",
    "section": "Posterior with Population Priors",
    "text": "Posterior with Population Priors\n\n\n\\(f(\\theta | y) = \\text{Beta}(1211 + 437, 1286 + 543) = \\text{Beta}(1648,1829)\\)\nWe can compare the prior and posterior using summarize_beta_binomial() in the bayesrules package\n\n\n\nlibrary(bayesrules)\nsummarize_beta_binomial(alpha = 1211, beta = 1286, y = 437, n = 980)\n\n      model alpha beta      mean      mode          var          sd\n1     prior  1211 1286 0.4849820 0.4849699 9.998978e-05 0.009999489\n2 posterior  1648 1829 0.4739718 0.4739568 7.168560e-05 0.008466735\n\n\n\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 1648, 1829) |> round(3)\nlower <- qbeta(l, 1648, 1829) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.457, 0.491]\n\n\n\n\n\nevent_prob <- pbeta(0.485, shape1 = 1648, shape2 = 1829)\ncat(\"Probability that the ratio < 0.485 under population prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under population prior = 0.904\n\n\n\n\nUnder uniform prior 95% posterior interval was [0.415, 0.477]\nAnd probability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "title": "Bayesian Inference",
    "section": "Visualizing Bayesian Rebalancing",
    "text": "Visualizing Bayesian Rebalancing\n\n\nThe following uses \\(\\text{Beta}(5, 5)\\) prior and N = 10:"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "href": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "title": "Bayesian Inference",
    "section": "Data Order and Batch Invariance",
    "text": "Data Order and Batch Invariance\n\n\nIn Bayesian analysis, we can update all at once or one datum at a time and everything in between and in any order (assuming exchangeable observations)\nThis is a general result, not just for Beta Binomial (see Section 4.5)\nIn practice, when we don’t have analytic posteriors, this is not so easy to do\nSuppose we observe \\(y = y_1 + y_2\\) and \\(N = N_1 + N_2\\) trials all at once\nAlso assume we start with \\(\\text{Beta}(1, 1)\\) prior\nFor all at once case, the posterior is in \\(f(\\theta \\mid y) = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\) as before\nNow, suppose we observe \\(y_1\\) successes in \\(n_1\\) trials first\nThe posterior is \\(f(\\theta \\mid y_1) = \\text{Beta}(\\alpha + y_1, \\,\\beta + N_1 - y_1)\\)\nWe now observe, \\(y_2\\) successes in \\(n_2\\) trials. The posterior is \\(f(\\theta \\mid y= y_1 + y_2) = \\text{Beta}(\\alpha + y_1 + y_2, \\,\\beta + N_1 - y_1 + N_2 - y_2)\\\\ = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#general-case",
    "href": "bayes-course/02-lecture/02-lecture.html#general-case",
    "title": "Bayesian Inference",
    "section": "General Case",
    "text": "General Case\n\n\nSuppose we observe data point \\(y_1\\), and then data point \\(y_2\\)\n\n\n\n\\[\nf(\\theta \\mid y_1,y_2) =  \\frac{f(\\theta)f(y_1 \\mid \\theta)f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\]\n\n\n\nObserving data point \\(y_2\\), and then data point \\(y_1\\), will results in the same distribution\nWhat if observed both points at once?\n\n\n\n\\[\n\\begin{split}\nf(\\theta \\mid y_1,y_2)\n& = \\frac{f(\\theta)f(y_1,y_2 \\mid \\theta)}{f(y_1)f(y_2)} \\\\\n& = \\frac{f(\\theta)f(y_1 \\mid \\theta)f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\end{split}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow-chapter-2",
    "href": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow-chapter-2",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow Chapter 2",
    "text": "Bayesian Workflow Chapter 2\n\n\n\n\nWhat do we do before fitting a model\n\nChoosing an initial model\nModular construction\nScaling and transforming the parameters\nPrior predictive checking\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "href": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "title": "Bayesian Inference",
    "section": "MCMC, Posterior inference, and Prediction",
    "text": "MCMC, Posterior inference, and Prediction\n\n\nMetropolis-Hastings-Rosenbluth algorithm\nR implementation and testing\nComputational issues\nWhy does MHR work\nPosterior predictive distribution\nPosterio predictive simulation in Stan\nOptimization and code breaking with MCMC\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "title": "Bayesian Inference",
    "section": "High Level Outline of MHR",
    "text": "High Level Outline of MHR\n\n\n\n\nThe idea is to spend “more time” in the in area of high posterior volume\nPick a random starting point at \\(i=1\\) with \\(\\theta^{(1)}\\)\nPropose a next possible value of \\(\\theta\\), call it \\(\\theta'\\) from an (easy) proposal distribution\nEvaluate if you should accept or reject the proposal\nIf accepted, go to \\(\\theta^{'(2)}\\), otherise stay at \\(\\theta^{(2)}\\)\nRinse and repeat\n\n\n\n\n\nIf we can get an independant draw from \\(\\theta\\), we just take it\nThat ammounts to regular Monte Carlo sampling, like rnorm(1, mean = 0, sd = 1)\nOtherwise, we need a rule for evaluating when to accept and when to reject the proposal\nWe still need a way to evaluate the density at the proposed value (it will be part of the rule)\nThe big idea: if the proposed \\(\\theta'\\) has a higher plausability than the current \\(\\theta\\), accept \\(\\theta'\\); if not, sometimes accept \\(\\theta'\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "href": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "title": "Bayesian Inference",
    "section": "Normal-Normal with Known \\(\\sigma\\)",
    "text": "Normal-Normal with Known \\(\\sigma\\)\n\n\nWe will start with a known posterior\nLet \\(y \\sim \\text{Normal}(\\mu, 0.5)\\)\nLet the prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nLet’s say we observe \\(y = 5\\)\nWe can immediately compute the posterior \\(\\mu | y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\nnormal_normal_post <- function(y, sd, prior_mu, prior_sd) {\n# for the case where sd is known\n  prior_prec <- 1/prior_sd^2 \n  data_prec <- 1/sd^2\n  n <- length(y)\n  post_mu <- (prior_prec * prior_mu + data_prec * n * mean(y)) /\n             (prior_prec + n * data_prec)\n  post_prec <- prior_prec + n * data_prec\n  post_sd <- sqrt(1/post_prec)\n  return(list(post_mu = post_mu, post_sd = post_sd))\n}\n\nnormal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2) |>\n  unlist() |> round(2)\n\npost_mu post_sd \n   4.71    0.49"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "title": "Bayesian Inference",
    "section": "Metropolis-Hastings-Rosenbluth (MHR)",
    "text": "Metropolis-Hastings-Rosenbluth (MHR)\n\nMHRNormal Symmetry\n\n\n\n\nWe are trying to generate draws: \\(\\left( \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)}\\right)\\) implied by the target density \\(f\\)\n\nPick the first value of \\(\\theta\\) randomly or deterministically\nDraw \\(\\theta'\\) from a proposal distribution \\(q(\\theta'|\\theta)\\)\nCompute the unnormalized \\(f(\\theta'|y) \\propto f(y|\\theta') f(\\theta') = f_{\\text{prop}}\\)\nCompute the unnormalized \\(f(\\theta|y) \\propto f(y|\\theta) f(\\theta) = f_{\\text{current}}\\)\nCompute \\(\\text{ratio}_f = \\frac{f_{\\text{prop}}}{f_{\\text{current}}}\\) and \\(\\text{ratio}_q = \\frac{q(\\theta|\\theta')}{q(\\theta'|\\theta)}\\)\nAsseptance probability \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\cdot \\text{ratio}_q \\right\\rbrace\\)\nIf \\(q\\) is symmetric, we can drop \\(\\text{ratio}_q\\), in which case \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\right\\rbrace\\)\nIf \\(\\text{ratio} \\geq 1\\), accept \\(\\theta'\\), else flip a coin with \\(\\text{Pr}(X=1) = \\alpha\\) and accept \\(\\theta'\\) if \\(X=1\\), stay with current \\(\\theta\\), if \\(X=0\\)\n\n\n\n\n\nWhy in case of normals, \\(q(\\theta'|\\theta) = q(\\theta|\\theta')\\)\n\n\n\n\n\n\n\n\n\n\n\ndnorm(1, mean = 0)\n\n[1] 0.2419707\n\ndnorm(0, mean = 1)\n\n[1] 0.2419707"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "title": "Bayesian Inference",
    "section": "MHR in R",
    "text": "MHR in R\n\nMHR IterationMHR for N Interations\n\n\n\nmetropolis_iteration_1 <- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # this is just another way of flipping a coint\n    next_value <- proposal_mean  \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}\n\n\n\n\nmhr <- function(y, f, N, start, ...) {\n# y: new observation\n# f: function that implements one MHR iteration\n# N: number of iterations\n# start: initial value of the chain\n# ...: additional arguments to f\n  \n  draws <- numeric(N)\n  draws[1] <- f(y, current_mean = start, ...)\n  \n  for (i in 2:N) {\n    draws[i] <- f(y, current_mean = draws[i - 1], ...)\n  }\n  \n  return(draws)\n}\n\ny <- 5; N <- 5e3; start <- 3\nd <- mhr(y, N, f = metropolis_iteration_1, start, proposal_scale = 2, sd = 0.5,  \n         prior_mean = 0, prior_sd = 2)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "href": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "title": "Bayesian Inference",
    "section": "Markov Chain Animation",
    "text": "Markov Chain Animation\n   Autocorrelation function (ACF)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#compararing-samples-to-the-true-posterior",
    "href": "bayes-course/04-lecture/04-lecture.html#compararing-samples-to-the-true-posterior",
    "title": "Bayesian Inference",
    "section": "Compararing Samples to the True Posterior",
    "text": "Compararing Samples to the True Posterior\n\n\nnp <- normal_normal_post(y = y, \n                         sd = 0.5, \n                         prior_mu = 0, \n                         prior_sd = 2)\n\ntheta <- seq(np$post_mu + 3*np$post_sd, \n             np$post_mu - 3*np$post_sd, \n             len = 100)\n\ndn <- dnorm(theta, mean = np$post_mu, \n                     sd = np$post_sd)\n\np <- ggplot(aes(d), data = tibble(d = d))\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 25, alpha = 0.6) +\n  geom_line(aes(theta, dn), linewidth = 0.5, \n            color = 'red', \n            data = tibble(theta, dn)) + \n  ylab(\"\") + xlab(expression(theta)) + \n  ggtitle(\"MHR draws vs Normal(4.71, 0.49)\")"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "href": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "title": "Bayesian Inference",
    "section": "What is Wrong with this Code",
    "text": "What is Wrong with this Code\n\nmetropolis_iteration_1 <- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # this is just another way of flipping a coint\n    next_value <- proposal_mean  \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "href": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "title": "Bayesian Inference",
    "section": "What if Y is a vector?",
    "text": "What if Y is a vector?\n\n\nRecall the likelihood of many exchangable observations, is the product of their individual likelihoods \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\nf(\\mu \\mid y) & \\propto & \\text{prior} \\cdot \\prod_{i=1}^{n}\\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\end{eqnarray}\n\\]\nThis suggests the following change:\n\n\n\nmetropolis_iteration_2 <- function(y, current_mean, proposal_scale, sd,\n                                 prior_mean, prior_sd) {\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, sd = prior_sd) * \n                   prod(dnorm(y, mean = proposal_mean, sd = sd))   \n  f_current     <- dnorm(current_mean, mean = prior_mean, sd = prior_sd) *        \n                   prod(dnorm(y, mean = current_mean, sd = sd))\n\n  if ((f_proposal || f_current) == 0) {\n    return(\"Error: underflow\") # on my computer double.xmin = 2.225074e-308\n  }\n  ratio <- f_proposal / f_current \n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # definitely go if f_mu_prime > f_mu: alpha = 1\n    next_value <- proposal_mean   # if alpha < 1, go if alpha > U(0, 1)\n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "href": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "title": "Bayesian Inference",
    "section": "Handling Underflow",
    "text": "Handling Underflow\n\nset.seed(123)\ny <- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\nreplicate(20, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n [1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n [5] \"Error: underflow\" \"1.66235834591796\" \"Error: underflow\" \"2.55673000751367\"\n [9] \"Error: underflow\" \"Error: underflow\" \"2.51354952759192\" \"Error: underflow\"\n[13] \"Error: underflow\" \"Error: underflow\" \"3.02905649937656\" \"Error: underflow\"\n[17] \"Error: underflow\" \"2.82278258359193\" \"2.67485301455861\" \"Error: underflow\""
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "Always Compute on the Log Scale",
    "text": "Always Compute on the Log Scale\n\n\nFor one observation \\(y\\): \\[\n\\begin{eqnarray}\n\\log f(y \\mid \\mu) & \\propto & \\log \\left( \\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\right) \\\\\n& = & -\\log(\\sigma) - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2\n\\end{eqnarray}\n\\]\n\n\ndnorm_log <- function(y, mean = 0, sd = 1) {\n# dropping constant terms; not needed for MCMC\n  -log(sd) - 0.5 * ((y - mean) / sd)^2\n}\n\n\nFor multiple observations, you can just sum the log-likelihood"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "MHR on the Log Scale",
    "text": "MHR on the Log Scale\n\nmetropolis_iteration_log <- function(y, current_mean, proposal_scale, \n                                     sd, prior_mean, prior_sd) {\n  \n  # draw a proposal q(proposal_mean | current_mean)\n  proposal_mean  <- rnorm(1, current_mean,  proposal_scale)           \n  \n  # construct a proposal: f(mu') * \\prod f(y_i | mu') on the log scale\n  proposal_lik   <- sum(dnorm_log(y, mean = proposal_mean, sd = sd))\n  proposal_prior <- dnorm_log(proposal_mean, mean = prior_mean, sd = prior_sd)\n  f_proposal     <- proposal_prior + proposal_lik\n  \n  # construct a current: f(mu) * \\prod f(y_i | mu) on the log scale\n  current_lik    <- sum(dnorm_log(y, mean = current_mean, sd = sd))\n  current_prior  <- dnorm_log(current_mean, mean = prior_mean, sd = prior_sd)\n  f_current      <- current_prior + current_lik\n  \n  # ratio on the log scale = difference of the logs\n  log_ratio <- f_proposal - f_current\n  log_alpha <- min(log_ratio, 0)     \n  \n  if (log_alpha > log(runif(1))) {  \n    next_value <- proposal_mean     \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "href": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "title": "Bayesian Inference",
    "section": "Checking the Work",
    "text": "Checking the Work\n\nset.seed(123)\ny <- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\nreplicate(4, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n\nreplicate(4, metropolis_iteration_log(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] 1.000000 1.000000 1.423961 2.379762\n\nd <- mhr(y, N, f = metropolis_iteration_log, start, proposal_scale = 2, sd = 0.55,  prior_mean = 0, prior_sd = 1)\nmean(d) |> round(2)\n\n[1] 2.02\n\nsd(d) |> round(2)\n\n[1] 0.03"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nTo show why the aldorithm works you need to show:\n\nThe chain has stationary distribution and it is unique\nThe stationary distribution is our target distribution \\(f\\theta | y)\\)\n\nCondition (a) requires some theory of Markov Chains, but the conditions are mild and are generally satisfied in practice. (See Chapters 11 and 12 in Blitzstein and Hwang for more)\n\nWe will show an outline of the proof for (b)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nWe will only consider the case of symmetric q\nSuppose you sample two points from the PMF \\(f(\\theta | y)\\), \\(\\theta_a\\) and \\(\\theta_b\\) and assume we are at time \\(t-1\\)\nLet the probability of going from \\(\\theta_a\\) to \\(\\theta_b\\) be: \\(\\P(\\theta^t = \\theta_b) \\mid \\theta^{t-1} = \\theta_a) := p_{a b}\\) and the reverse jump: \\(\\P(\\theta^t = \\theta_a) \\mid \\theta^{t-1} = \\theta_b) := p_{b a}\\)\nWe want to show: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} ,\\, \\text{where} \\\\\np_{a  b} &=& q(\\theta_b \\mid \\theta_a) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace \\\\\np_{b  a} &=& q(\\theta_a \\mid \\theta_b) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nFor symmetric q: \\(q(\\theta_b | \\theta_a) = q(\\theta_a | \\theta_b)\\) and: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{\\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace}{\\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace}\n\\end{eqnarray}\n\\]\nConsider the case when \\(f(\\theta_b \\mid y) > f(\\theta_a \\mid y)\\) \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{1}{\\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)}} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\end{eqnarray}\n\\]\nWhen \\(f(\\theta_a \\mid y) > f(\\theta_b \\mid y)\\) \\[\n\\frac{p_{a  b}}{p_{b  a}} = \\frac{\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}}{1} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nRecall the prior predictive distribution, before observing \\(y\\), that appears in the denominator of the Bayes’s rule: \\[\nf(y) = \\int f(y, \\theta) \\, d\\theta = \\int f(\\theta) f(y \\mid \\theta) \\, d\\theta\n\\]\nA posterior predictive distribution, \\(f(\\tilde{y} | y)\\) is obtained in a similar manner \\[\n\\begin{eqnarray}\nf(\\tilde{y} \\mid y) &=& \\int f(\\tilde{y}, \\theta \\mid y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta, y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta \\\\\n\\end{eqnarray}\n\\]\n\\(f(\\tilde{y} \\mid \\theta, y) = f(\\tilde{y} \\mid \\theta)\\) since \\(y \\perp\\!\\!\\!\\perp \\tilde{y} \\mid \\theta\\) (conditional indepedence)\nTwo sources of variability are accounted for: sampling variabily in \\(\\tilde{y}\\) weighted by posterior variabily in \\(\\theta\\)\nGiven draws from \\(f(\\theta \\mid y)\\), \\(\\theta^{(m)} \\sim f(\\theta \\mid y)\\), we can compute the integral in a usual way: \\(f(\\tilde{y} \\mid y) \\approx \\frac{1}{M} \\sum_{m = 1}^M f(\\tilde{y} \\mid \\theta^{(m)})\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nFor simple models we can often evalute \\(\\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta\\) directly\nWe will use an inderect approach using our example Normal-Normal model with known variance\nSince we already know \\(f(\\theta | y)\\), we will sample \\(\\theta\\) from it, and then sample \\(\\tilde{y}\\), from \\(f(\\tilde{y} \\mid \\theta)\\)\nRecall, \\(y \\sim \\text{Normal}(\\mu, 0.5)\\) with prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nFor \\(y = 5\\), the posterior \\(\\mu | y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\nppd <- function(post_mu, post_sd) {\n  mu <- rnorm(1, mean = post_mu, sd = post_sd)\n  y  <- rnorm(1, mean = mu, sd = 0.5)\n}\npd <- normal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2)\ny <- replicate(1e5, ppd(pd$post_mu, pd$post_sd))\ncat(\"y_tilde | y ~ Normal(\", round(mean(y), 2), \",\", round(sd(y), 2), \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71,0.7)\n\n\n\nIt can be shown that the posterior predictive distribution will have the same mean as the posterior distrution and the variance as the sum of the posterior variance and data variance:\n\n\nppd_mu <- pd$post_mu |> round(2)\nppd_sigma <- sqrt(pd$post_sd^2 + 0.5^2) |> round(2)\ncat(\"y_tilde | y ~ Normal(\", ppd_mu, \", \", ppd_sigma, \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71, 0.7)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\n\nYou can compute posterior predictive distribution in R, Stan, and rstanarm\nHere, we will see how to do it in Stan\nWe will show an example in rstanarm in the next lecture\n\n\n\ndata {\n  real y;\n  real<lower=0> sigma;\n}\nparameters {\n  real mu;\n}\nmodel {\n  mu ~ normal(0, 2);\n  y ~ normal(mu, sigma);\n}\ngenerated quantities {\n  real y_tilde = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\nlibrary(cmdstanr)\n\nm1 <- cmdstan_model(\"stan/normal_pred.stan\") # compile the model\ndata <- list(y = 5, sigma = 0.5)\nf1 <- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 500  # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  <chr>    <num>  <num> <num> <num> <num> <num> <num>    <num>    <num>\n1 lp__     -3.46  -3.20 0.728 0.348 -4.81 -2.94  1.00     889.    1128.\n2 mu        4.74   4.72 0.494 0.524  3.97  5.56  1.00     720.     958.\n3 y_tilde   4.72   4.74 0.712 0.715  3.51  5.88  1.00    1091.    1441."
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#extra-credit-homework",
    "href": "bayes-course/04-lecture/04-lecture.html#extra-credit-homework",
    "title": "Bayesian Inference",
    "section": "Extra Credit Homework",
    "text": "Extra Credit Homework\n\nDianconisCypher textAlgorithmDecoded text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#linear-regression-and-model-evaluation",
    "href": "bayes-course/05-lecture/05-lecture.html#linear-regression-and-model-evaluation",
    "title": "Bayesian Inference",
    "section": "Linear Regression and Model Evaluation",
    "text": "Linear Regression and Model Evaluation\n\n\n\n\nIntroducing linear regression\nPrior predictive simulations\nSampling from the posterior\nExample of linear regression in Stan\nEvaluating the quality of the draws\nPosterior predictions\nCross validation, ELPD, and LOO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "href": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\n\n\nWe borrow this example from Richard McElreath’s Statistical Rethinking\nThe data sets provided have been produced between 1969 to 2008, based on Nancy Howell’s observations of the !Kung San\nFrom Wikipedia: “The ǃKung are one of the San peoples who live mostly on the western edge of the Kalahari desert, Ovamboland (northern Namibia and southern Angola), and Botswana.”\n\n\n\n\n\n\n\n\n\n\n\nUnivercity of Toronto Data Sets"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\n\n\nData sample and summary:\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n151.765\n47.82561\n63\n1\n\n\n139.700\n36.48581\n63\n0\n\n\n136.525\n31.86484\n65\n0\n\n\n156.845\n53.04191\n41\n1\n\n\n145.415\n41.27687\n51\n0\n\n\n163.830\n62.99259\n35\n1\n\n\n\n\n\n     height           weight            age       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.00  \n 1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00  \n Median :148.59   Median :40.058   Median :27.00  \n Mean   :138.26   Mean   :35.611   Mean   :29.34  \n 3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00  \n Max.   :179.07   Max.   :62.993   Max.   :88.00  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice a non-linearity\nThinking about why should this be, can give you an insight into how to model these data"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\nFor now, we will focus on the linear subset of the data\nWe will denonstrate the non-linear model at the end\nWe will restrict our attention to adults (age > 18)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#general-aproach",
    "href": "bayes-course/05-lecture/05-lecture.html#general-aproach",
    "title": "Bayesian Inference",
    "section": "General Aproach",
    "text": "General Aproach\n\n\nAssess the scope of the inferences that you will get with this model\nIn the case of linear regression, there is likely no causal mechanism and the coeficients should be intepreted as comparisons (RAOS, Page 84)\nSet up reasonable priors and likelihood\nPerform a prior predictive simualtion\nAdjust your priors\nFit the model to data\nAssess quality of the inference and quality of the model\nAdjust your model"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe will build a predictive model for adult Weight \\(y\\) given Height \\(x\\) using the Howell dataset\nInitial stab at the model: \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x_i \\\\\n\\alpha & \\sim & \\text{Normal}(\\alpha_{l}, \\, \\alpha_s) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWe have to specify \\(\\alpha_{l}\\) and \\(\\alpha_s\\), where l and s signify location and scale, and r, the rate of the exponential\nIf we work on the original scale for \\(x\\), it is awkward to choose a prior for the intercept : it correponds to the weight of the person with zero height\nThis can be fixed by subtracting average height from \\(x\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe define a new variable, the centered version of \\(x\\): \\(x^c_i = x_i - \\bar{x}\\)\nNow \\(\\alpha\\) corresponds to the weight of an average person\nChecking Wikipedia reveals that average weight of a person in Africa is about 60 kg\nThey don’t state the standard deviation but it is unlikely that an African adult would weigh less than 30 kg and more than 120 kg and so we will set the prior sd = 10 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the slope \\(\\beta\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nIn this dataset, the units of \\(\\beta\\) are \\(\\frac{kg}{cm}\\), since the units of height are \\(cm\\)\nFirst thing, \\(\\beta\\) should be positive. Why?\nSecond, \\(beta\\) is likely less than 1. Why?\nWe can consult height-weight tables for the expected value and variance\nIn the dataset, \\(\\E(\\beta) = 0.55\\) with a standard error of 0.006, but since we are uncertain how applicable that is to !Kung, we will allow the prior to vary more \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the error term \\(\\sigma\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe know that \\(\\sigma\\) must be positive and so a possible choice for a the prior is \\(\\text{Normal}^+\\), Exponential, etc.\nAt this stage they key is rule out implausable values, not to get something precise, partcularly since we have enough data (> 340 observations)\nFrom the background data, the residual standard error was 4.6, which implies the exponential rate parameter to be 0.2 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(0.2) \\\\\n\\end{eqnarray}\n\\]\nWe are now ready to perform a prior predictive simulation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nThe simulation follows the generative process defined by the model\n\n\n\n\n\n\nd <- d |>\n  mutate(height_c = height - mean(height))\nround(mean(d$height_c), 2)\n\n[1] 0\n\nhead(d)\n\n# A tibble: 6 × 5\n  height weight   age  male height_c\n   <dbl>  <dbl> <dbl> <dbl>    <dbl>\n1   152.   47.8    63     1    -2.83\n2   140.   36.5    63     0   -14.9 \n3   137.   31.9    65     0   -18.1 \n4   157.   53.0    41     1     2.25\n5   145.   41.3    51     0    -9.18\n6   164.   63.0    35     1     9.23\n\nprior_pred <- function(data) {\n  alpha <- rnorm(1, 60, 10)\n  beta <- rnorm(1, 0.55, 0.1)\n  sigma <- rexp(1, 0.2)\n  l <- nrow(data); y <- numeric(l)\n  for (i in 1:l) {\n    mu <- alpha + beta * data$height_c[i]\n    y[i] <- rnorm(1, mu, sigma)\n  }\n  return(y)\n}\n\n\n\n\n\nn <- 100\npr_p <- replicate(n = n, prior_pred(d))\n# using library(purrr) functional primitives:\n# pr_p <- map(1:n, \\(i) prior_pred(d))\ndim(pr_p)\n\n[1] 352 100\n\nround(pr_p[1:12, 1:8], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n [1,] 50.02 49.35 67.40 50.84 40.34 47.49 67.09 39.72\n [2,] 46.98 43.39 62.40 53.24 34.19 47.66 62.79 32.45\n [3,] 45.60 41.89 62.17 53.54 34.27 47.21 61.63 30.93\n [4,] 50.96 51.11 69.22 57.28 41.94 58.24 68.66 42.54\n [5,] 48.87 45.78 65.16 52.88 36.66 51.49 64.78 36.91\n [6,] 52.35 54.13 71.59 61.06 44.36 56.78 71.32 47.25\n [7,] 48.64 47.73 66.35 56.74 38.62 44.15 66.08 37.69\n [8,] 55.56 56.36 74.16 55.90 45.27 53.42 73.19 50.21\n [9,] 50.09 47.26 65.32 51.37 39.50 52.89 65.69 39.02\n[10,] 54.01 55.10 72.17 60.16 43.42 50.05 71.78 47.44\n[11,] 52.27 49.91 67.93 61.93 41.40 48.83 67.93 41.93\n[12,] 52.74 48.63 67.90 57.07 40.23 44.49 66.86 39.32"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nTo get a sense for the possible regression lines implied by the prior, we fit a linear model to each simulation draw and plot the lines over observations\n\n\n\n\n\nintercepts <- numeric(n)\nslopes <- numeric(n)\nfor (i in 1:n) {\n  coefs <- coef(lm(pr_p[, i] ~ d$height_c))\n  intercepts[i] <- coefs[1]\n  slopes[i] <- coefs[2]\n}\n\n# using library(purrr) functional primitives:\n# df <- pr_p |> map_dfr(\\(y) coef(lm(y ~ d$height_c)))\n\np <- ggplot(aes(height_c, weight), data = d)\np + geom_point(size = 0.5) + ylim(20, 90) + \n  geom_abline(slope = slopes, \n              intercept = intercepts, \n              alpha = 1/6) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"Kalahari !Kung San people\", \n          subtitle = \"Prior predictive simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice about this prior?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "href": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving a Posterior Distribution",
    "text": "Deriving a Posterior Distribution\n\n\nWe have seen how to derive posterior and posterior predictive distribution\nThree dimentional posterior: \\(f(\\alpha, \\beta, \\sigma)\\). What happened to \\(\\mu\\)?\nWe construct the posterior from the prior and data likelihood (for each \\(y_i\\)): \\[\n\\begin{eqnarray}\n&\\text{Prior: }f(\\alpha, \\beta, \\sigma) = f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\\\\n&\\text{Likelihood: }f(y \\mid \\alpha, \\beta, \\sigma) = \\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\\\\n&\\text{Posterior: }f(\\alpha,\\beta,\\sigma \\mid y) = \\frac{f_1(\\alpha) f(_2\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right]}\n{\\int\\int\\int f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right] d\\alpha \\, d\\beta \\, d\\sigma}\n\\end{eqnarray}\n\\]\nTo be more precise, we would indicate that \\(f_1, f_2\\) and \\(f_4\\) are Normal with different parameters, and \\(f_3\\) is \\(\\text{Exp}(0.2)\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "href": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "title": "Bayesian Inference",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nEven though our prior is slighly off, 300+ observations is a lot in this case (big data!), and so we proceed to model fitting\nWe will use stan_glm() function in rstanarm\nrstanarm has default priors, but you should specify your own:\n\n\n\n\n\n\nlibrary(rstanarm)\nlibrary(bayesplot)\noptions(mc.cores = parallel::detectCores())\n\nm1 <- stan_glm(\n  weight ~ height_c,\n  data = d,\n  family = gaussian,\n  prior_intercept = normal(60, 10),\n  prior = normal(0.55, 0.1),\n  prior_aux = exponential(0.2),\n  chains = 4,\n  iter = 500,\n  seed = 1234\n)\n\n\n\nBy default, rstanarm samples from the posterior. To get back the prior predictive distribution (instead of doing it in R) use prior_PD = TRUE"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "href": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "title": "Bayesian Inference",
    "section": "Looking at the Model Summary",
    "text": "Looking at the Model Summary\n\n\nsummary(m1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ height_c\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 352\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 45.0    0.2 44.7  45.0  45.3 \nheight_c     0.6    0.0  0.6   0.6   0.7 \nsigma        4.2    0.2  4.0   4.2   4.5 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 45.0    0.3 44.6  45.0  45.4 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  824  \nheight_c      0.0  1.0  994  \nsigma         0.0  1.0  867  \nmean_PPD      0.0  1.0  970  \nlog-posterior 0.1  1.0  459  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\nIf you can examine the priors by running prior_summary(m1)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "title": "Bayesian Inference",
    "section": "Evaluting Quality of the Inferences",
    "text": "Evaluting Quality of the Inferences\n\n\nneff_ratio(m1) |> round(2)\n\n(Intercept)    height_c       sigma \n       0.82        0.99        0.87 \n\n\n\n\n\nrhat(m1) |> round(2)\n\n(Intercept)    height_c       sigma \n          1           1           1 \n\n\n\n\n\nmcmc_trace(m1, size = 0.3)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "href": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "title": "Bayesian Inference",
    "section": "Same Model in Stan",
    "text": "Same Model in Stan\n\n\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n  int<lower=0, upper=1> prior_PD;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\ntransformed parameters {\n  vector[N] mu = alpha + beta * x;\n}\nmodel {\n  alpha ~ normal(60, 10);\n  beta ~ normal(0.55, 0.1);\n  sigma ~ exponential(0.2);\n  if (!prior_PD) {\n    y ~ normal(mu, sigma);\n  }\n}\ngenerated quantities {\n  array[N] real y_tilde = normal_rng(mu, sigma);\n}\n\n\n\n\nYou can pass prior_PD as a flag to enable drawing from prior predictive distribution"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "title": "Bayesian Inference",
    "section": "Prior vs Posterior",
    "text": "Prior vs Posterior\n\n\nComparing the prior to the posterior tell us how much the model learned from data\nIt also helps us to validate if our priors were reasonable\nIn rstanarm, you can use posterior_vs_prior function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour prior should cover the plausable range of parameter values\nWhen we don’t have a lot of data and parameters are complex, setting good priors takes work, but there are guidelines"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\nlibrary(tidybayes)\n\ndraws <- spread_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(head(round(draws, 2)))\n\n\n\n\n.chain\n.iteration\n.draw\n(Intercept)\nheight_c\nsigma\n\n\n\n\n1\n1\n1\n44.97\n0.62\n4.09\n\n\n1\n2\n2\n45.06\n0.61\n4.21\n\n\n1\n3\n3\n45.02\n0.63\n4.13\n\n\n1\n4\n4\n45.07\n0.64\n4.32\n\n\n1\n5\n5\n44.66\n0.64\n4.38\n\n\n1\n6\n6\n45.31\n0.62\n4.12\n\n\n\n\n\n\n\n\nspread_draws will arrange the inferences in columns (wide format)\ngather_draws will arrange the inferences in rows (long format), which is usually more convenient for plotting and computation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\noptions(digits = 3)\ndraws <- gather_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(tail(draws, 4))\n\n\n\n\n.chain\n.iteration\n.draw\n.variable\n.value\n\n\n\n\n4\n247\n997\nsigma\n4.09\n\n\n4\n248\n998\nsigma\n4.22\n\n\n4\n249\n999\nsigma\n4.20\n\n\n4\n250\n1000\nsigma\n4.11\n\n\n\n\n\n\n\n\ndraws |> mean_qi(.width = 0.90) |> knitr::kable() # also see ?median_qi(), etc\n\n\n\n\n.variable\n.value\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\n(Intercept)\n45.000\n44.635\n45.36\n0.9\nmean\nqi\n\n\nheight_c\n0.624\n0.576\n0.67\n0.9\nmean\nqi\n\n\nsigma\n4.250\n3.996\n4.53\n0.9\nmean\nqi\n\n\n\n\n\n\n\n\nFrom the above table: \\(\\E(y|x^c) = 45 (\\text{kg}) + 0.62 (\\text{kg/cm})x^c (\\text{cm})\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "title": "Bayesian Inference",
    "section": "Variability in Parameter Inferences",
    "text": "Variability in Parameter Inferences\n\n\nThe code in section 9.4 in the book doesn’t work as the function and variable names have changed\n\n\n\n\ndpred <- d |> \n  # same as add_epred_draws for lin reg not not for other GLMs\n  add_linpred_draws(m1, ndraws = 50)\nhead(dpred, 3)\n\n# A tibble: 3 × 10\n# Groups:   height, weight, age, male, height_c, .row [1]\n  height weight   age  male height_c  .row .chain .iteration .draw .linpred\n   <dbl>  <dbl> <dbl> <dbl>    <dbl> <int>  <int>      <int> <int>    <dbl>\n1   152.   47.8    63     1    -2.83     1     NA         NA     1     43.4\n2   152.   47.8    63     1    -2.83     1     NA         NA     2     43.0\n3   152.   47.8    63     1    -2.83     1     NA         NA     3     42.9\n\n\n\n\n\n\np <- dpred |> ggplot(aes(x = height_c, y = weight)) +\n  geom_line(aes(y = .linpred, group = .draw), \n            alpha = 0.1) + \n  geom_point(data = d, size = 0.05) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"100 draws from the slope/intercept posterior\")\nprint(p)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nSuppose we are interested in predicting the weight of a person with the height of 160 cm\nThis corresponds to the centered height of 5.4: (\\(160 - \\bar{x}\\))\nWe can now compute the distribution of the mean weight of a 160 cm person (reflecting variability in the slope and intercept only):\n\n\\(\\mu = \\alpha + \\beta \\cdot 5.4\\), for each posterior draw\n\nAnd a predictive distribution:\n\n\\(y_{\\text{pred}} \\sim \\text{Normal}(\\mu, \\sigma)\\)\n\n\n\n\n\ndraws <- spread_draws(m1, `(Intercept)`, height_c, sigma)\ndraws <- draws |>\n  mutate(mu = `(Intercept)` + height_c * 5.4,\n         y_pred = rnorm(nrow(draws), mu, sigma))\ndraws[1:3, 4:8]\n\n# A tibble: 3 × 5\n  `(Intercept)` height_c sigma    mu y_pred\n          <dbl>    <dbl> <dbl> <dbl>  <dbl>\n1          45.0    0.619  4.09  48.3   50.6\n2          45.1    0.607  4.21  48.3   50.8\n3          45.0    0.628  4.13  48.4   40.5"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nWe can compare predictive and average densitives\nLeft panel showing the densities on their own\nRight panel showing the same densities in the context of raw observations\n\n\n\n\nmqi <- draws |> median_qi(.width = 0.90)\nselect(mqi, contains(c('mu', 'y_pred'))) |> round(2)\n\n# A tibble: 1 × 6\n     mu mu.lower mu.upper y_pred y_pred.lower y_pred.upper\n  <dbl>    <dbl>    <dbl>  <dbl>        <dbl>        <dbl>\n1  48.4     47.9     48.8   48.4         41.5         55.4"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "href": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "title": "Bayesian Inference",
    "section": "RStanArm Prediction Functions",
    "text": "RStanArm Prediction Functions\n\n\nposterior_linpred returns \\(D \\times N\\) matrix with D draws and N data points\n\n\\(\\eta_n = \\alpha + \\sum_{p=1}^P \\beta_p x_{np}\\), where \\(P\\) is the total number of regression inputs\n\nposterior_epred returns an \\(D \\times N\\) matrix that applies the inverse link (in GLMs) to the linear predictor \\(\\eta\\)\n\n\\(\\mu_n = \\E(y | x_n)\\); this is the same as \\(\\eta\\) in Lin Regression\n\nposterior_predict returns an \\(D \\times N\\) matrix of predictions: \\(y \\mid \\mu_n\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "href": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "title": "Bayesian Inference",
    "section": "Predictions in RStanArm",
    "text": "Predictions in RStanArm\n\n\nPosterior linear predictor\n\n\n\neta <- posterior_linpred(m1, newdata = data.frame(height_c = 5.4))\nquantile(eta, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\nglue::glue('From the R simulation, 90% interval for eta = [{mqi$mu.lower |> round(2)}, {mqi$mu.upper |> round(2)}]')\n\nFrom the R simulation, 90% interval for eta = [47.91, 48.82]\n\n\n\n\nPosterior conditional mean\n\n\n\nmu <- posterior_epred(m1, newdata = data.frame(height_c = 5.4))\nquantile(mu, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\n\n\n\nPosterior prediction\n\n\n\ny_pred <- posterior_predict(m1, newdata = data.frame(height_c = 5.4))\nquantile(y_pred, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n41.0 48.1 55.3 \n\nglue::glue('From the R simulation, 90% interval for y_pred = [{mqi$y_pred.lower |> round(2)}, {mqi$y_pred.upper |> round(2)}]')\n\nFrom the R simulation, 90% interval for y_pred = [41.52, 55.36]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-predictions",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-predictions",
    "title": "Bayesian Inference",
    "section": "Evaluting Quality of the Predictions",
    "text": "Evaluting Quality of the Predictions\n\n\n\n\nThere are at least two stages of model evaluation: 1) the quality of the draws and 2) the quality of predictions\nJust becuase the draws have good statistical properties (e.g., good mixing, low auto-correlation, etc.), it does not mean the model will perform well\nModel performance is assessed how well it can make predictions for the types of questions that you are interested in\nTherefore, there is no universal measure of “goodness of fit”"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-predictions-1",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-predictions-1",
    "title": "Bayesian Inference",
    "section": "Evaluting Quality of the Predictions",
    "text": "Evaluting Quality of the Predictions\n\n\nEat spaghetti\nDrink wine\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course.html",
    "href": "bayes-course.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "This is the home for APSTA-GE 2123: Bayesian Inference class at NYU.\n\nThis is the current version of the syllabus: syllabus.pdf\nLecture 01: Introduction and Bayesian Workflow\nLecture 02: [Conjugate Models and Beta-Binomial] (bayes-course/02-lecture/02-lecture.html)\n\nThe final project is due at the end of the semester and the presentations will take place during finals week. Take a look at these guidelines, when working on your proposal and the final report."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Bayes and Stats-Math",
    "section": "",
    "text": "This website contains teaching materials for Bayesian analysis (APSTA-GE 2123) course and stats-math bootcamp, both taught at NYU. If you find any errors, please email me at eric.novik@nyu.edu."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for stopping by."
  }
]