[
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#session-8-outline",
    "href": "stats-math/08-lecture/08-lecture-old.html#session-8-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 8 Outline",
    "text": "Session 8 Outline\n\n\nStatistical analysis workflow\nIntroduction to the rstanarm package\nSetting up a linear regression\nModel evaluation\nModel expansion\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#analysis-workflow",
    "href": "stats-math/08-lecture/08-lecture-old.html#analysis-workflow",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Workflow",
    "text": "Analysis Workflow\n\nFollowing is a high-level, end-to-end view from “R for Data Science” by Wickham and Grolemund"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#analysis-workflow-1",
    "href": "stats-math/08-lecture/08-lecture-old.html#analysis-workflow-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Workflow",
    "text": "Analysis Workflow\n\n\n\nA more comprehensive “Bayesian Workflow” by Gelman at al."
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#some-notation",
    "href": "stats-math/08-lecture/08-lecture-old.html#some-notation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Notation",
    "text": "Some Notation\n\n\nObserved data \\(y\\)\nUnobserved but observable data \\(\\widetilde{y}\\)\nUnobservable parameters \\(\\theta\\)\nCovariates \\(X\\)\nPrior distribution \\(f(\\theta)\\)\nLikelihood (as a function of \\(\\theta\\)), \\(f(y | \\theta, X)\\)\nPosterior distribution \\(f(\\theta | y, X)\\) (for Bayesian inference only)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#bayes-vs-frequentist-inference",
    "href": "stats-math/08-lecture/08-lecture-old.html#bayes-vs-frequentist-inference",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Bayes vs Frequentist Inference",
    "text": "Bayes vs Frequentist Inference\n\n\n\n\nEstimation is the process of figuring out the unknowns, i.e., unobserved quantities\nIn frequentist inference, the problem is framed in terms of the most likely value(s) of \\(\\theta\\)\nBayesians want to characterize the whole distribution, a much more ambitious goal\n\n\n\n\n\nSuppose we want to characterize the following function, which represents some distribution of the unknown parameter:"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#wine-dataset",
    "href": "stats-math/08-lecture/08-lecture-old.html#wine-dataset",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Wine Dataset",
    "text": "Wine Dataset"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#wine-dataset-1",
    "href": "stats-math/08-lecture/08-lecture-old.html#wine-dataset-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Wine Dataset",
    "text": "Wine Dataset\n\nd &lt;- read.delim(\"data/winequality-red.csv\", sep = \";\")\ndim(d)\n\n[1] 1599   12\n\nd &lt;- d[!duplicated(d), ] # remove the duplicates\ndim(d)\n\n[1] 1359   12\n\nknitr::kable(head(d))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n1\n7.4\n0.70\n0.00\n1.9\n0.076\n11\n34\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n2\n7.8\n0.88\n0.00\n2.6\n0.098\n25\n67\n0.9968\n3.20\n0.68\n9.8\n5\n\n\n3\n7.8\n0.76\n0.04\n2.3\n0.092\n15\n54\n0.9970\n3.26\n0.65\n9.8\n5\n\n\n4\n11.2\n0.28\n0.56\n1.9\n0.075\n17\n60\n0.9980\n3.16\n0.58\n9.8\n6\n\n\n6\n7.4\n0.66\n0.00\n1.8\n0.075\n13\n40\n0.9978\n3.51\n0.56\n9.4\n5\n\n\n7\n7.9\n0.60\n0.06\n1.6\n0.069\n15\n59\n0.9964\n3.30\n0.46\n9.4\n5"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#scaling-the-data",
    "href": "stats-math/08-lecture/08-lecture-old.html#scaling-the-data",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Scaling the Data",
    "text": "Scaling the Data\n\nds &lt;- scale(d) # subtract the mean and divide by sd\nknitr::kable(head(ds) %&gt;% round(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\n1\n-0.52\n0.93\n-1.39\n-0.46\n-0.25\n-0.47\n-0.38\n0.58\n1.29\n-0.58\n-0.95\n-0.76\n\n\n2\n-0.29\n1.92\n-1.39\n0.06\n0.20\n0.87\n0.60\n0.05\n-0.71\n0.12\n-0.58\n-0.76\n\n\n3\n-0.29\n1.26\n-1.19\n-0.17\n0.08\n-0.09\n0.21\n0.16\n-0.32\n-0.05\n-0.58\n-0.76\n\n\n4\n1.66\n-1.36\n1.47\n-0.46\n-0.27\n0.11\n0.39\n0.69\n-0.97\n-0.46\n-0.58\n0.46\n\n\n6\n-0.52\n0.71\n-1.39\n-0.53\n-0.27\n-0.28\n-0.20\n0.58\n1.29\n-0.58\n-0.95\n-0.76\n\n\n7\n-0.24\n0.39\n-1.09\n-0.68\n-0.39\n-0.09\n0.36\n-0.17\n-0.06\n-1.16\n-0.95\n-0.76\n\n\n\n\ncheck &lt;- apply(ds, 2, function(x) c(mean = mean(x), sd = sd(x))) %&gt;% round(2)\nknitr::kable(check)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\nmean\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nsd\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\nclass(ds); ds &lt;- as_tibble(ds)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#quality-rating-of-red-wine",
    "href": "stats-math/08-lecture/08-lecture-old.html#quality-rating-of-red-wine",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Quality Rating of Red Wine",
    "text": "Quality Rating of Red Wine\n\n\n\nUnscaled Quality\n\n\nqplot(d$quality, geom = \"histogram\") + xlab(\"Quality Rating of Red Wine\")\n\n\n\n\n\n\n\n\n\n\nScaled Quality\n\n\nqplot(ds$quality, geom = \"histogram\") + xlab(\"Quality Rating of Red Wine\")"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#alcohol",
    "href": "stats-math/08-lecture/08-lecture-old.html#alcohol",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Alcohol",
    "text": "Alcohol\n\np &lt;- ggplot(ds, aes(alcohol, quality))\np + geom_jitter(width = 0.1, height = 0.2, alpha = 1/5) + xlab(\"Alcohol\") + ylab(\"Quality (Jittered)\")"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#volatile-acidity",
    "href": "stats-math/08-lecture/08-lecture-old.html#volatile-acidity",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Volatile Acidity",
    "text": "Volatile Acidity\n\np &lt;- ggplot(ds, aes(volatile.acidity, quality))\np + geom_jitter(width = 0.1, height = 0.2, alpha = 1/5) + xlab(\"Volatile Acidity\") + ylab(\"Quality (Jittered)\")"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#correlations",
    "href": "stats-math/08-lecture/08-lecture-old.html#correlations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Correlations",
    "text": "Correlations\n\nlibrary(corrplot)\nM &lt;- cor(ds)\ncorrplot(M, method = 'ellipse', order = 'AOE', type = 'upper')"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#linear-regression",
    "href": "stats-math/08-lecture/08-lecture-old.html#linear-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\nWe will fit a linear regression to the scaled wine dataset\nThe priors come from (sensible) defaults in rstanarm\nLinear regression can be specified in the following way, where \\(X\\) is the design matrix with one or more scaled predictors and \\(y\\) is the scaled quality score\n\n\n\n\\[\n\\begin{aligned}\ny &\\sim \\mathrm{Normal}(\\alpha + X \\beta, \\ \\sigma) \\\\\n\\alpha &\\sim \\mathrm{Normal}(0, \\ 2.5) \\\\\n\\beta &\\sim \\mathrm{Normal}(0, \\ 2.5) \\\\\n\\sigma &\\sim \\mathrm{Exp}(1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#estimation-using-rs-lm",
    "href": "stats-math/08-lecture/08-lecture-old.html#estimation-using-rs-lm",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimation using R’s lm()",
    "text": "Estimation using R’s lm()\n\n\nWe will start by comparing alcohol content to quality ratings\nIn R’s lm() priors are not specified – they are uniform\n\n\n\n\nfit1_freq &lt;- lm(quality ~ alcohol, data = ds)\narm::display(fit1_freq)\n\nlm(formula = quality ~ alcohol, data = ds)\n            coef.est coef.se\n(Intercept) 0.00     0.02   \nalcohol     0.48     0.02   \n---\nn = 1359, k = 2\nresidual sd = 0.88, R-Squared = 0.23\n\n# avoid R's summary() function\nsummary(fit1_freq)\n\n\nCall:\nlm(formula = quality ~ alcohol, data = ds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4372 -0.4761 -0.2097  0.6494  3.1666 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.310e-15  2.380e-02    0.00        1    \nalcohol      4.803e-01  2.381e-02   20.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8774 on 1357 degrees of freedom\nMultiple R-squared:  0.2307,    Adjusted R-squared:  0.2302 \nF-statistic:   407 on 1 and 1357 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#running-the-regression-in-rstanarm",
    "href": "stats-math/08-lecture/08-lecture-old.html#running-the-regression-in-rstanarm",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Running the Regression in rstanarm",
    "text": "Running the Regression in rstanarm\n\nlibrary(rstanarm)\noptions(mc.cores = parallel::detectCores())\nfit1 &lt;- stan_glm(quality ~ alcohol, data = ds, refresh = 0)\nsummary(fit1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ alcohol\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.0    0.0  0.0   0.0   0.0  \nalcohol     0.5    0.0  0.4   0.5   0.5  \nsigma       0.9    0.0  0.9   0.9   0.9  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.0    0.0  0.0   0.0   0.0  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  3930 \nalcohol       0.0  1.0  4059 \nsigma         0.0  1.0  3914 \nmean_PPD      0.0  1.0  4029 \nlog-posterior 0.0  1.0  1689 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#plotting",
    "href": "stats-math/08-lecture/08-lecture-old.html#plotting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Plotting",
    "text": "Plotting\n\n\n\nplot(fit1, plotfun = \"areas\", prob = 0.9)\n\n\n\n\n\n\n\n\n\n\nposterior_vs_prior(fit1)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#evaluating-the-model",
    "href": "stats-math/08-lecture/08-lecture-old.html#evaluating-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\n\nHow good is this model?\n\n\np1 &lt;- pp_check(fit1, plotfun = \"ppc_dens_overlay\"); p2 &lt;- pp_check(fit1, plotfun = \"ppc_ecdf_overlay\")\np3 &lt;- pp_check(fit1, plotfun = \"ppc_stat\", stat =\"mean\"); p4 &lt;- pp_check(fit1, plotfun = \"ppc_stat\", stat =\"sd\")\np1 + p2 + p3 + p4"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#evaluating-the-model-1",
    "href": "stats-math/08-lecture/08-lecture-old.html#evaluating-the-model-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Model",
    "text": "Evaluating the Model\n\nHow good is this model?\n\n\nyrep1 &lt;- posterior_predict(fit1)\ndim(yrep1)\n\n[1] 4000 1359\n\ns &lt;- sample(1:nrow(ds), size = 50) # select 50 random records\np1 &lt;- ppc_intervals(ds$quality[s], yrep1[, s]); p1"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#we-can-add-other-predictors",
    "href": "stats-math/08-lecture/08-lecture-old.html#we-can-add-other-predictors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "We Can Add Other Predictors",
    "text": "We Can Add Other Predictors\n\nfit2 &lt;- stan_glm(quality ~ ., data = ds, refresh = 0)\nplot(fit2, plotfun = \"areas\", prob = 0.9, params = \"alcohol\")"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#have-we-improved-the-model",
    "href": "stats-math/08-lecture/08-lecture-old.html#have-we-improved-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Have We Improved the Model",
    "text": "Have We Improved the Model\n\nyrep2 &lt;- posterior_predict(fit2)\np1 &lt;- p1 + ggtitle(\"Model 1 predictions\")\np2 &lt;- ppc_intervals(ds$quality[s], yrep2[, s]) + ggtitle(\"Model 2 predictions\")\np1 + p2 + plot_layout(nrow = 2, byrow = FALSE)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#have-we-improved-the-model-1",
    "href": "stats-math/08-lecture/08-lecture-old.html#have-we-improved-the-model-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Have We Improved the Model",
    "text": "Have We Improved the Model\n\n# Mean Square Error for Model 1\nmean((colMeans(yrep1) - ds$quality)^2) %&gt;% round(2)\n\n[1] 0.77\n\n# Mean Square Error for Model 2\nmean((colMeans(yrep2) - ds$quality)^2) %&gt;% round(2)\n\n[1] 0.64\n\nwidth &lt;- function(yrep, q1, q2) {\n  q &lt;- apply(yrep, 2, function (x) quantile(x, probs = c(q1, q2)))\n  width &lt;- apply(q, 2, diff)\n  return(mean(width))\n}\nwidth(yrep1, 0.25, 0.75) %&gt;% round(2)\n\n[1] 1.18\n\nwidth(yrep2, 0.25, 0.75) %&gt;% round(2)\n\n[1] 1.09"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#comparing-out-of-sample-performance",
    "href": "stats-math/08-lecture/08-lecture-old.html#comparing-out-of-sample-performance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Comparing Out of Sample Performance",
    "text": "Comparing Out of Sample Performance\n\nloo1 &lt;- loo(fit1, cores = 4)\nloo2 &lt;- loo(fit2, cores = 4)\nloo_compare(loo1, loo2)\n\n     elpd_diff se_diff\nfit2    0.0       0.0 \nfit1 -117.6      17.8 \n\n\nVehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413–1432. :10.1007/s11222-016-9696-4. Links: published | arXiv preprint."
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#can-we-still-improve-the-model",
    "href": "stats-math/08-lecture/08-lecture-old.html#can-we-still-improve-the-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Can We Still Improve the Model",
    "text": "Can We Still Improve the Model\n\n\nQuality variable is an ordinal scale variable, so a more appropriate likelihood would also be ordinal\nFor more information, see ordered logistic distribution in the Stan manual\nWe are going to use the following parameterization available in Stan:\n\n\n\n\\[\n\\text{OrderedLogistic}(k~|~\\eta,c) = \\left\\{ \\begin{array}{ll} 1 -\n\\text{logit}^{-1}(\\eta - c_1)  &  \\text{if } k = 1, \\\\[4pt]\n\\text{logit}^{-1}(\\eta - c_{k-1}) - \\text{logit}^{-1}(\\eta - c_{k})  &\n\\text{if } 1 &lt; k &lt; K, \\text{and} \\\\[4pt] \\text{logit}^{-1}(\\eta -\nc_{K-1}) - 0  &  \\text{if } k = K \\end{array} \\right.\n\\]\n\n\n\nA similar model can fit with frequentist methods using MASS::polr() function"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#fitting-ordered-logistic",
    "href": "stats-math/08-lecture/08-lecture-old.html#fitting-ordered-logistic",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Fitting Ordered Logistic",
    "text": "Fitting Ordered Logistic\n\nds$quality &lt;- d$quality - 2          # quality vector is now 1:6\nds$quality &lt;- as.factor(ds$quality)  # it has to be treated as factor (category)\nfit3 &lt;- stan_polr(quality ~ ., prior = R2(0.25), # the prior is on explained variance\n                  prior_counts = dirichlet(1), \n                  data = ds, iter = 1000, refresh = 0)\nyrep3 &lt;- posterior_predict(fit3)\nyrep3[1:5, 1:5]\n\n     1   2   3   4   5  \n[1,] \"1\" \"3\" \"4\" \"4\" \"4\"\n[2,] \"4\" \"3\" \"3\" \"3\" \"3\"\n[3,] \"3\" \"3\" \"3\" \"4\" \"3\"\n[4,] \"3\" \"3\" \"3\" \"3\" \"3\"\n[5,] \"4\" \"4\" \"4\" \"3\" \"3\"\n\nyrep3 &lt;- apply(yrep3, 2, as.numeric)\nyrep3[1:5, 1:5]\n\n     1 2 3 4 5\n[1,] 1 3 4 4 4\n[2,] 4 3 3 3 3\n[3,] 3 3 3 4 3\n[4,] 3 3 3 3 3\n[5,] 4 4 4 3 3"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#evaluating-model-fit",
    "href": "stats-math/08-lecture/08-lecture-old.html#evaluating-model-fit",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\n\n\n\npp_check(fit2, plotfun = \"ppc_dens_overlay\")\n\n\n\n\n\n\n\n\n\n\npp_check(fit3, plotfun = \"ppc_dens_overlay\")"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#evaluating-model-fit-1",
    "href": "stats-math/08-lecture/08-lecture-old.html#evaluating-model-fit-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\n\nPlotsMetrics\n\n\n\np3 &lt;- ppc_intervals(as.numeric(ds$quality[s]), yrep3[, s]) + ggtitle(\"Model 3 predictions\")\np2 + p3 + plot_layout(nrow = 2, byrow = FALSE)\n\n\n\n\n\n\n\n\n\n\n\ncat(\"Mean Square Error for Model 2:\", mean((colMeans(yrep2) - scale(d$quality))^2) %&gt;% round(2))\n\nMean Square Error for Model 2: 0.64\n\ncat(\"Mean Square Error for Model 3:\", mean((colMeans(yrep3) - as.numeric(ds$quality))^2) %&gt;% round(2))\n\nMean Square Error for Model 3: 0.43\n\ncat(\"Width of the 50% interval for Model 2:\", width(yrep2, 0.25, 0.75) %&gt;% round(2))\n\nWidth of the 50% interval for Model 2: 1.09\n\ncat(\"Width of the 50% interval for Model 3:\", width(yrep3, 0.25, 0.75) %&gt;% round(2))\n\nWidth of the 50% interval for Model 3: 0.73"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#quick-tips-to-improve-your-regression-modeling",
    "href": "stats-math/08-lecture/08-lecture-old.html#quick-tips-to-improve-your-regression-modeling",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "10+ quick tips to improve your regression modeling",
    "text": "10+ quick tips to improve your regression modeling\nThis advice comes from “Regression and Other Stories” by Gelman and Hill\n\n\nThink generatively (+)\nThink about what you are measuring (+)\nThink about variation, replication\nForget about statistical significance\nGraph the relevant and not the irrelevant\nInterpret regression coefficients as comparisons\nUnderstand statistical methods using fake-data simulation\nFit many models\nSet up a computational workflow\nUse transformations\nDo causal inference in a targeted way, not as a byproduct of a large regression\nLearn methods through live examples"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture-old.html#so-long-and-thanks-for-all-the-fish",
    "href": "stats-math/08-lecture/08-lecture-old.html#so-long-and-thanks-for-all-the-fish",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "So Long (and Thanks for All the Fish)",
    "text": "So Long (and Thanks for All the Fish)\n\n\nLectures: https://ericnovik.github.io/smac.html\nKeep in touch:\n\neric.novik@nyu.edu\nTwitter: @ericnovik\nhttps://www.linkedin.com/in/enovik/\nPersonal blog: https://ericnovik.com/"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#session-6-outline",
    "href": "stats-math/06-lecture/06-lecture.html#session-6-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 6 Outline",
    "text": "Session 6 Outline\n\n\nVectors and vector arithmetic\nMatrices and matrix arithmetic\nDeterminants\nMatrix inverses\nSolving linear systems\n\n\n\nInspiration for a lot of the examples came from the Essense of Linear Algebra by 3blue1brown\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nLinear Algebra offers a language for manipulating n-dimensional objects\nVectors are the basic building block of Linear Algebra\nA straightforward way to think about them is as a column of numbers\nHow we interpret the entries is up to us\nVectors are typically written in a column form:\n\n\n\n\\[\\begin{aligned} V = \\left[\\begin{matrix}a\\\\b\\\\c\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\nIf we want the row version, we transpose it:\n\n\\[\\begin{aligned} V^T = \\left[\\begin{matrix}a & b & c\\end{matrix}\\right] \\end{aligned}\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-1",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nIn R, vectors, unfortunately, are neither row nor column vectors\nR makes some assumptions when performing vector-matrix arithmetic\nWe already saw lots of vectors whenever we used c() function or generated random numbers with, say runif() function\n\n\n\n\nset.seed(123)\n(v &lt;- c(sample(3)))\n\n[1] 3 1 2\n\nclass(v)\n\n[1] \"integer\"\n\n\n\n\n\nR reports the \\(v\\) is an integer vector\nYou can add two vectors in a usual way, elementwise:\n\n\\[\\begin{aligned} \\left[\\begin{matrix}3\\\\1\\\\2\\end{matrix}\\right]+\\left[\\begin{matrix}2\\\\1\\\\3\\end{matrix}\\right]=\\left[\\begin{matrix}5\\\\2\\\\5\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\nIn R:\n\n\nv\n\n[1] 3 1 2\n\nw\n\n[1] 2 1 3\n\nv + w\n\n[1] 5 2 5"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-2",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nYou can take linear combination \\(av + bw\\), where a and b are scalars\n\n\n2*v + 3*w # linear combination\n\n[1] 12  5 13\n\n\n\n\n\nYou can also take dot product: \\(v \\cdot w\\), which results in a scalar\n\n\\[\\begin{aligned} v \\cdot w = \\left[\\begin{matrix}3 & 1 & 2\\end{matrix}\\right]\\left[\\begin{matrix}2\\\\1\\\\3\\end{matrix}\\right]=\\left[\\begin{matrix}13\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\nIn R, we multiply vectors and Matrices with %*%, not *, which will produce a component-wise multiplication, not a dot product\n\n\nv %*% w # dot product\n\n     [,1]\n[1,]   13\n\nv * w   # component wise multiplication\n\n[1] 6 1 6\n\n\n\n\n\nWe write dot product this way:\n\n\\[\nv^T w = v_1w_1 + v_2w_2 + \\cdots + v_nw_n\n\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-3",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-3",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\n\nWhen is dot product zero?\n\n\nv &lt;- c(1, 1)\nw &lt;- c(1, -1)\nv %*% w\n\n     [,1]\n[1,]    0\n\n(3 * v) %*% (4 * w)\n\n     [,1]\n[1,]    0\n\n\n\n\n\nThese vectors are perpendicular (orthogonal)"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-4",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-vectors-4",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Vectors",
    "text": "Introduction to Vectors\n\nA length of a vector is the square root of the dot product with itself\n\n\\[\n||v|| = \\sqrt{v \\cdot v} = \\sqrt{v_1^2 + v_2^2 + \\cdots v_n^2}\n\\]\n\nv &lt;- c(1, 1)  \nsqrt(v %*% v) # this is sqrt of 2 \n\n         [,1]\n[1,] 1.414214"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Matrices",
    "text": "Introduction to Matrices\n\n\nYou can think of a matrix as a rectangular (or square) set of numbers\n\\(m{\\times}n\\) matrix has m rows and n columns\nIn statistics, it is convenient to think of a matrix as n columns where each column is a variable like the price of the house, and the rows are the observations for each house\nIn Linear Algebra books, you will often see linear equations written as \\(Ax = b\\), where we are trying to find \\(x\\)\nIn statistics, we usually write the same thing as \\(X\\beta = y\\), and we are trying to find \\(\\beta\\)\nAnother source of confusion: matrix \\(A\\) is sometimes called a coefficient matrix in Linear Algebra books. In statistics, the entries in \\(A\\) (the \\(X\\) matrix) have observations, and the \\(\\beta\\) vector contains coefficients estimated from A and b (X and y)."
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices-1",
    "href": "stats-math/06-lecture/06-lecture.html#introduction-to-matrices-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Matrices",
    "text": "Introduction to Matrices\n\nBasisPlot of v and wTransformationR Example\n\n\n\n\nA good way to think about the Matrices is that they act on vectors and transform them in some way (stretch or turn them)\nYou can think of this transformation as encoding the eventual locations of the basis vectors\nStandard basis vectors in \\(R^2\\) (two-dimensional space), are commonly called \\(\\hat{i}\\) with location \\((1, 0)\\), and \\(\\hat{j}\\) with location \\((0, 1)\\).\nNotice their dot product is zero: \\(\\hat{i} \\cdot \\hat{j} = 0\\) and so they are orthogonal\nLet’s look at one simple transformation – rotation by 90 degrees counter-clockwise\n\n\n\n\n\n\n\n\nRecall, vectors \\(v\\) and \\(w\\):\n\n\\[\nv = \\left[ \\begin{matrix}1\\\\2\\end{matrix} \\right]\nw = \\left[ \\begin{matrix}3\\\\1\\end{matrix} \\right]\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following matrix encodes a 90-degree, counterclockwise rotation. Why?\n\n\\[\nR =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\]\n\n\n\nLet’s see how this matrix will act on vectors \\(v\\) and \\(w\\). That’s where multiplication comes in.\n\n\\[\nRv =\n\\begin{bmatrix}\n0 & -1 \\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\\n2\n\\end{bmatrix}\n=\n1\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n+\n2\n\\begin{bmatrix}\n-1 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n-2 \\\\\n0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n\n\n\nWe scale the first vector, then scale the second, and then add.\n\n\n\n\n\n\n\n\nThe following shows the same operation you can do in one step in R.\n\n\nR &lt;- matrix(c(0, 1, -1, 0), ncol = 2)\nR\n\n     [,1] [,2]\n[1,]    0   -1\n[2,]    1    0\n\nv &lt;- c(1, 2)\n(v_prime &lt;- R %*% v)\n\n     [,1]\n[1,]   -2\n[2,]    1\n\n\n\n\n\n\nNow let’s plot \\(v\\) and \\(v'\\). Notice, the 90 degree rotation"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#matrix-multiplication",
    "href": "stats-math/06-lecture/06-lecture.html#matrix-multiplication",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\n\nMultiplicationPlotYour Turn\n\n\n\n\nOur matrix encodes a rotation of every single vector in \\(R^2\\).\nIn particular, it rotates every point \\((x, y)\\) in \\(R^2\\) by 90 degrees\nWe can do this transformation in one go by applying the Rotation matrix to another matrix comprising our vectors \\(v\\) and \\(w\\)\nThe number of columns in the Rotation matrix has to match the number of rows in the \\(K = [v, w]\\) matrix\n\n\n\n\n(K &lt;- matrix(c(v, w), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    1\n\n(K_prime &lt;- R %*% K)\n\n     [,1] [,2]\n[1,]   -2   -1\n[2,]    1    3\n\n\n\n\n\nThe resulting matrix \\(K\\), has the correctly rotated \\(v\\) in the first column and rotated \\(w\\) in the second column.\nAnother way to think about this operation is to encode two transformations in \\(K'\\) — the \\(K\\) transformation followed by the \\(R\\) transformation. We can now use the resulting \\(K'\\) matrix and apply these two transformations in one swoop to any vector in \\(R^2\\).\nThink about why matrix multiplication, in general, does not commute — \\(RK \\neq KR\\)\n\n\n\n\n\nRotating red (\\(v = [1 \\ 2]\\)) and blue (\\(w = [3 \\ 1]\\)) vectors by 90 degrees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn: Come up with a 90-degree clockwise rotation matrix and show that it sends \\((2, 2)\\) to \\((2, -2)\\)\nNow pick three vectors in \\(R^2\\) and rotate all three at the same time"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#linear-independence-and-determinants",
    "href": "stats-math/06-lecture/06-lecture.html#linear-independence-and-determinants",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Independence and Determinants",
    "text": "Linear Independence and Determinants\n\n\n\n\nThink of a determinant as a (signed) area scaling factor from the basis \\((\\hat{i}, \\hat{j})\\) to the transformed vectors\nThe area represented by two \\(R^2\\) basis vectors is 1\n\n\n\n\n# this matrix scales the area by 12\n(X &lt;- matrix(c(4, 0, 0, 3), ncol = 2))\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    0    3\n\n# compute the determinant\ndet(X)\n\n[1] 12\n\n\n\n\n\nWhat happens if the columns of \\(X\\) are linear combinations of each other?\nGeometrically, that means that in \\(R^2\\), they both sit on the same line\n\n\n\n\n\n# the second column is the first column * 1.5\n(X &lt;- matrix(c(2, 1, 1.5 * 2, 1.5 * 1), ncol = 2))\n\n     [,1] [,2]\n[1,]    2  3.0\n[2,]    1  1.5\n\n# compute the determinant\ndet(X)\n\n[1] 0"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#example-random-matrix",
    "href": "stats-math/06-lecture/06-lecture.html#example-random-matrix",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Random Matrix",
    "text": "Example: Random Matrix\n\n\nGenerate a random, say 5x5 matrix\nIs it likely that we will get linearly dependent columns?\n\n\n\n\nset.seed(123)\n(X &lt;- matrix(sample(25), ncol = 5))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   15   18    9   25    2\n[2,]   19   11   21   17   16\n[3,]   14    5   24    1    7\n[4,]    3   23   20   12    8\n[5,]   10    6   22   13    4\n\ndet(X)\n\n[1] 1155943\n\n\n\n\n\nThis is a full rank matrix that encodes a transformation in \\(R^5\\) — 5-dimensional space"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#computing-determinants",
    "href": "stats-math/06-lecture/06-lecture.html#computing-determinants",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Computing Determinants",
    "text": "Computing Determinants\n\nDon’t bother doing it by hand\n\n\\[\\begin{aligned} A = \\left[\\begin{matrix}a & c\\\\b & d\\end{matrix}\\right] \\end{aligned}\\]\n\n\\[\\begin{aligned} \\det(A) = a d - b c \\end{aligned}\\]\n\n\\[\\begin{aligned} B = \\left[\\begin{matrix}a & d & g\\\\b & e & h\\\\c & f & i\\end{matrix}\\right] \\end{aligned}\\]\n\n\\[\\begin{aligned} \\det(B) = a e i - a f h - b d i + b f g + c d h - c e g \\end{aligned}\\]"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#linear-systems-and-inverses",
    "href": "stats-math/06-lecture/06-lecture.html#linear-systems-and-inverses",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Systems and Inverses",
    "text": "Linear Systems and Inverses\n\nLinear SystemsNo solutionIdea of an inverseInverse in RAx = b\n\n\n\n\nOne of the key ideas from Linear Algebra that is relevant to statistics is solving linear systems\nThis is where estimating coefficients in \\(X\\beta = y\\) comes from\nIf \\(X\\) is a square matrix, the problem is not statistical but algebraic\nThe reason is that if we have the same number of equations as we have unknowns, we can solve the system exactly unless X is not full rank\nFor example, try to solve this system:\n\n\n\n\\[\n2x + y = 1 \\\\\n4x + 2y = 1\n\\]\n\n\n\nQuestion: Geometrically, what does it mean to solve a system of equations?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou should now have a deeper understanding of why there is no solution\n\n\n(A &lt;- matrix(c(2, 4, 1, 2), ncol = 2))\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    4    2\n\ndet(A)\n\n[1] 0\n\n\n\n\n\nBefore we talk about overdetermined systems (more equations than unknowns) that we see in statistics, let’s see how it works in the square matrix case\n\n\n\n\n\n\nTo see what a matrix inverse does, let’s bring back the 90-degree counterclockwise rotation matrix \\(A\\)\n\n\\[\\begin{aligned} A = \\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\nThe idea of an inverse is to reverse the action of this transformation\nIn this case, we need a clockwise 90-degree rotation matrix\nWe can find that matrix by directly tracing the basis vectors\n\n\n\n\\[\\begin{aligned} A^{-1} = \\left[\\begin{matrix}0 & 1\\\\-1 & 0\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\n\n\nLet’s check our results in R. When you multiply a matrix by its inverse, you get back the identity matrix (which is like when you divide a scalar by its reciprocal, you get 1)\n\n\n\n\n(A &lt;- matrix(c(0, 1, -1, 0), 2, 2))\n\n     [,1] [,2]\n[1,]    0   -1\n[2,]    1    0\n\n(A_inv &lt;- matrix(c(0, -1, 1, 0), 2, 2))\n\n     [,1] [,2]\n[1,]    0    1\n[2,]   -1    0\n\nA_inv %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# find an inverse using R's solve function\nsolve(A)\n\n     [,1] [,2]\n[1,]    0    1\n[2,]   -1    0\n\n\n\n\n\n\n\nWe are now ready to solve the square linear system \\(Ax = b\\), in the case when A is full rank\n\n\\[\n\\begin{eqnarray}\nAx & = & b \\\\\nA^{-1}Ax & = & A^{-1}b \\\\\nx & = & A^{-1}b\n\\end{eqnarray}\n\\]\n\n\n\nThe general solution in two-dimensional case:\n\n\\[\\begin{aligned} \\left[\\begin{matrix}a & c\\\\b & d\\end{matrix}\\right]x = \\left[\\begin{matrix}b_{1}\\\\b_{2}\\end{matrix}\\right] \\end{aligned}\\]\n\n\\[\\begin{aligned} \\det(A) = a d - b c \\end{aligned}\\]\n\n\\[\\begin{aligned} A^{-1} = \\left[\\begin{matrix}\\frac{d}{a d - b c} & - \\frac{c}{a d - b c}\\\\- \\frac{b}{a d - b c} & \\frac{a}{a d - b c}\\end{matrix}\\right] \\end{aligned}\\]\n\n\\[\\begin{aligned} x = \\left[\\begin{matrix}\\frac{b_{1} d - b_{2} c}{a d - b c}\\\\\\frac{a b_{2} - b b_{1}}{a d - b c}\\end{matrix}\\right] \\end{aligned}\\]\n\n\n\nIn R, solve(A) inverts the matrix, and solve(A, b) solves \\(Ax = b\\).\n\n\n(A &lt;- matrix(sample(9), ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]    3    1    7\n[2,]    4    9    2\n[3,]    6    5    8\n\n(b &lt;- c(1, 2, 3))\n\n[1] 1 2 3\n\nsolve(A, b) %&gt;% round(2)\n\n[1]  0.93 -0.14 -0.24\n\n# same as above\nsolve(A) %*% b %&gt;% round(2)\n\n      [,1]\n[1,]  0.93\n[2,] -0.14\n[3,] -0.24"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#your-turn-1",
    "href": "stats-math/06-lecture/06-lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nConsider the following system of equations:\n\n\\[\n\\begin{eqnarray}\n3x + 2y + 1.5z & = & 4 \\\\\n7x + y & = & 2 \\\\\n3y + 2z & = & 1\n\\end{eqnarray}\n\\]\n\nExpress it in matrix form, solve it in R using the solve() function, and validate that the results are correct"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#some-useful-rules",
    "href": "stats-math/06-lecture/06-lecture.html#some-useful-rules",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Useful Rules",
    "text": "Some Useful Rules\n\nIf you put on socks and then shoes, the first to be taken off are the ____\n\n— Gilbert Strang (discussing the order of undoing the inverse)\n\\[\n(ABC \\dots)^{-1} = \\dots C^{-1}B^{-1}A^{-1} \\\\\n(A^T)^{-1} = (A^{-1})^T \\\\\n(A + B)^T = A^T + B^T \\\\\n(ABC \\dots)^T = \\dots C^T B^T A^T\n\\]\n\nFor a complete list, see the Matrix Cookbook"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-n-p-systems",
    "href": "stats-math/06-lecture/06-lecture.html#solving-n-p-systems",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving n > p Systems",
    "text": "Solving n &gt; p Systems\n\n\nFrequentist statistical inference is concerned with “solving” overdetermined systems\nWe have lots of observations with relatively few variables. (Sometimes, we have more variables than observations, but we will not discuss it here)\nClearly, there is no exact solution\nIn fact, there are typically infinitely many ways in which you can draw a line through a cloud of points or a plane through n-dimensional space\nOptimization based (or frequentist) inference is concerned with finding the most likely values of the unknowns giving rise to that line (or plane) and approximating their variance\nIntegration based (or Bayesian) inference is concerned with finding a joint PDF of the unknowns – all plausible values weighted by their probability. Therefore, the “estimated variance” is a consequence of this PDF. (Recall our example of estimating variance from the distribution of the difference in heights.) In this sense, Bayesian inference is an uncertainty-preserving system, a very desirable quality.\nFor simple linear models, both of these methods produce similar results."
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y",
    "href": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)\n\n\nWe are switching to a more familiar (to statisticians) notation of \\(X \\beta = y\\) (instead of \\(Ax = b\\))\nWe typically augment the \\(X\\) matrix with the column of 1s on the left to model the intercept term, sometimes called \\(\\beta_0\\)\nThis should make sense when you think about \\(X \\beta\\) as the linear combination of columns of \\(X\\)\nIn this form, \\(X\\) is usually called the design matrix or the model matrix\nThe problem is that \\(X\\) is not invertible, even if columns of \\(X\\) are linearly independent\nLet’s say that \\(X\\) is a \\(3 \\times 2\\) matrix (3 rows and 2 columns), and columns are linearly independent. Note that \\(\\beta\\) is \\(2 \\times 1\\) and \\(y\\) is \\(3 \\times 1\\).\n\\(X\\) and its linear combinations \\(X\\beta\\) span a plane in three-dimensional space – there is no way for it to reach the target vector \\(y\\) in \\(R^3\\)"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#approximating-a-solution-to-overdetermined-xhatbeta-y",
    "href": "stats-math/06-lecture/06-lecture.html#approximating-a-solution-to-overdetermined-xhatbeta-y",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Approximating a Solution to Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Approximating a Solution to Overdetermined \\(X\\hat{\\beta} = y\\)\n\n\nIf we can not reach \\(y\\), we need to find a vector in the column space of \\(X\\) that is closest (in a certain sense) to \\(y\\)\nThe projection of \\(y\\) onto this plane gives us the answer\n\n\n\n\n\n\n\n\n\n\nImage from Introduction to Linear Algebra, Boyd and Vandenberghe"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y-1",
    "href": "stats-math/06-lecture/06-lecture.html#solving-an-overdetermined-xhatbeta-y-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)",
    "text": "Solving an Overdetermined \\(X\\hat{\\beta} = y\\)\n\nSolutionExample of an \\(X^TX\\)\n\n\n\n\nThis residual \\(r\\) is also called the error term\nThis error, \\(y - X\\hat{\\beta}\\), is the smallest when it’s perpendicular to the plane\nAnother way of saying that is that:\n\n\n\n\\[\nX^T(y - X\\hat{\\beta}) = 0\n\\]\n\n\n\nWe can now solve this normal equation:\n\n\\[\n\\begin{eqnarray}\nX^T(y - X\\hat{\\beta}) & = & 0 \\\\\nX^TX\\hat{\\beta} & = & X^Ty \\\\\n(X^TX)^{-1}(X^TX)\\hat{\\beta} & = & (X^TX)^{-1}X^Ty \\\\\n\\hat{\\beta} & = & (X^TX)^{-1}X^Ty\n\\end{eqnarray}\n\\]\n\n\n\n\n\nMatrix \\(X^TX\\) is square and symmetric. It is also invertible if the columns of \\(X\\) are linearly independent:\n\n\n\n\n(X &lt;- matrix(sample(6), ncol = 2))\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    5    4\n[3,]    3    6\n\n(XtX &lt;- t(X) %*% X)\n\n     [,1] [,2]\n[1,]   38   40\n[2,]   40   53\n\nsolve(XtX) %&gt;% round(2)\n\n      [,1]  [,2]\n[1,]  0.13 -0.10\n[2,] -0.10  0.09\n\n(X &lt;- matrix(c(1, 2, 3, 2, 4, 6), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n[3,]    3    6\n\n(XtX &lt;- t(X) %*% X)  # will not invert\n\n     [,1] [,2]\n[1,]   14   28\n[2,]   28   56"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line",
    "href": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Motion in the Straight Line",
    "text": "Motion in the Straight Line\n\n\nRecall, from lecture 1, our example of a car moving in a straight line where we have noisy measurements of its position over time\nAt the time, we assumed we were able somehow to find the intercept and slope of the equation\nWe are now in a position to solve the problem"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line-1",
    "href": "stats-math/06-lecture/06-lecture.html#motion-in-the-straight-line-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Motion in the Straight Line",
    "text": "Motion in the Straight Line\n\nSetupSolutionPlot the Fit\n\n\n\n\nThis is a quadratic function. How can we solve it using linear regression (the least squares method)?\nWe assumed the equation of motion was \\(x(t) = a + bt^2\\), which we will write as \\(y(t) = \\beta_0 + \\beta_1 t^2\\)\nThe unknowns are \\(\\beta_0\\) and \\(\\beta_1\\)\nLet’s express it in the matrix notation\nWhat is our design matrix \\(X\\)? We have 51 observations, so \\(X\\) will have two columns and 51 rows: a column of 1s for the intercept and a column of times squared \\(t^2\\)\nOur unknown vector \\(\\hat{\\beta}\\) has two elements \\((\\hat{\\beta_0},\\, \\hat{\\beta_1)}\\)\n\\(y\\) is our outcome vector of length 51, capturing the car’s position at each time point \\(t\\)\n\\(y = X\\hat{\\beta}\\) captures our system in matrix form\n\n\n\n\n\n\n\n\n# length of t (our time vector)\nlength(t)\n\n[1] 51\n\n# first few elements of t\nhead(t)\n\n[1] 0.0 0.1 0.2 0.3 0.4 0.5\n\n# construct the design matrix\nX &lt;- matrix(c(rep(1, length(t)), t^2), ncol = 2)\nhead(X)\n\n     [,1] [,2]\n[1,]    1 0.00\n[2,]    1 0.01\n[3,]    1 0.04\n[4,]    1 0.09\n[5,]    1 0.16\n[6,]    1 0.25\n\n# inspect the vector y\nlength(y)\n\n[1] 51\n\nhead(y)\n\n[1] -2.4417028  6.7615084 -0.7502334 -0.4900157 -3.5129263  1.9331119\n\n\n\n\n\n\nRecall, that the least squares estimate of \\(\\beta\\) is given by \\(\\hat{\\beta} =  (X^TX)^{-1}X^Ty\\)\n\n\n\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbeta_hat %&gt;% round(2)\n\n     [,1]\n[1,] 1.46\n[2,] 3.01\n\ndata &lt;- data.frame(y = y, t_squared = t^2)\n# compare to R's lm()\nfit &lt;- lm(y ~ t_squared, data = data)\narm::display(fit)\n\nlm(formula = y ~ t_squared, data = data)\n            coef.est coef.se\n(Intercept) 1.46     0.54   \nt_squared   3.01     0.05   \n---\nn = 51, k = 2\nresidual sd = 2.60, R-Squared = 0.99\n\n\n\n\n\nWarning: do not use \\((X^TX)^{-1}X^Ty\\) directly as above; it’s computationally inefficient\n\n\n\n\n\n\nd &lt;- data.frame(yhat = beta_hat[1] + beta_hat[2] * x^2)\np + geom_line(aes(y = yhat), data = d, size = 0.2, color = 'red')"
  },
  {
    "objectID": "stats-math/06-lecture/06-lecture.html#your-turn-2",
    "href": "stats-math/06-lecture/06-lecture.html#your-turn-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nLook at the dataset mtcars\nPlot mpg against wt, treating mpg as the output variable \\(y\\). (We generally don’t like to use the words dependent vs. independent in this context)\nUsing the normal equations, find the intercept and slope of the best-fit line\nValidate that it matches the output from lm\nNow plot the line over the data and see if it “fits”"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#session-5-outline",
    "href": "stats-math/05-lecture/05-lecture.html#session-5-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 5 Outline",
    "text": "Session 5 Outline\n\n\nRandom variables\nBernoulli, Binomial, Geometric\nPDF and CDF\nLOTUS\nPoisson\nExpectations, Variance\nSt. Petersburg Paradox\nNormal\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#random-variables-are-not-random",
    "href": "stats-math/05-lecture/05-lecture.html#random-variables-are-not-random",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Random Variables are Not Random",
    "text": "Random Variables are Not Random\n\nStoryPicture\n\n\n\n\nIt would be inconvenient to enumerate all possible events to describe a stochastic system\nA more general approach is to introduce a function that maps sample space \\(S\\) onto the Real line\nFor each possible outcome \\(s\\), random variable \\(X(s)\\) performs this mapping\nThis mapping is deterministic. The randomness comes from the experiment, not from the random variable (RV)\nWhile it makes sense to talk about \\(\\P(A)\\), where \\(A\\) is an event, it does not make sense to talk about \\(\\P(X)\\), but you can say \\(\\P(X(s) = x)\\), which we usually write as \\(\\P(X = x)\\)\nLet \\(X\\) be the number of Heads in two coin flips. You flip the coin twice, and you get \\(HH\\). In this case, \\(s = {HH}\\), \\(X(s) = 2\\), while \\(S = \\{TT, TH, HT, HH\\}\\)\n\n\n\n\n\nRandom variable \\(X\\) for the number of Heads in two flips"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#characterising-random-variables",
    "href": "stats-math/05-lecture/05-lecture.html#characterising-random-variables",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Characterising Random Variables",
    "text": "Characterising Random Variables\n\nIntroductionR Conventions\n\n\n\n\nTwo ways of describing an RV are CDF (Cumulative Distribution Function) and PMF (Probability Mass Function) for discrete RVs and PDF (Probability Density Function) for continuous RVs. There are other ways, but we will stick with CDF and P[D/M]F.\nCDF \\(F_X(x)\\) is a function of \\(x\\) and is bounded between 0 and 1:\n\n\n\n\\[\nF_X(x) = \\P(X \\leq x)\n\\]\n\n\n\nPMF (for discrete RVs only) \\(f_X(x)\\) is a function of \\(x\\)\n\n\\[\nf_X(x) = \\P(X = x)\n\\]\n\n\n\nYou can get from \\(f_X\\) to \\(F_X\\) by summing. Let’s say \\(x = 4\\). In that case:\n\n\\[\nF_X(4) = \\P(X \\leq 4) = \\sum_{i = 4,3,2,...}\\P(X = i)\n\\]\n\n\n\n\n\nIn R, PMFs and PDFs start with the letter d. For example dbinom() and dnormal() refer to binomial PMF and normal PDF\nCDFs start with p, so pbinom() and pnorm()\nInverse CDFs or quantile functions, start with q so qbinom() and so on\nRandom number generators start with r, so rbinom()\nA binomial RV, which we will define later, represents the number of successes in N trials. In R, the PMF is dbinom() and CDF is pbinom()\nHere is the full function signature: dbinom(x, size, prob, log = FALSE)\n\nx is the number of successes, size is the number of trials N, prob is the probability of success in each trial \\(\\theta\\), and log is a flag asking if we want the results on the log scale."
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#binomial-rv",
    "href": "stats-math/05-lecture/05-lecture.html#binomial-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial RV",
    "text": "Binomial RV\n\nBinomial PMFPMF and CDF PlotsCode to Generate the Plots\n\n\n\n\nBernoulli RV is one coin flip with a set probability of success (say Heads)\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), the PMF can be written directly as \\(\\P(X = x) = \\theta^x (1 - \\theta)^{1-x}, \\, x \\in \\{0, 1\\}\\)\nBinomial can be thought of as the sum of \\(N\\) independent Bernoulli trials. We can also write:\n\n\n\n\\[\n\\text{Bernoulli}(x~|~\\theta) = \\left\\{ \\begin{array}{ll} \\theta &\n\\text{if } x = 1, \\text{ and} \\\\ 1 - \\theta & \\text{if } x = 0\n\\end{array} \\right.\n\\]\n\n\n\nWe can write the Binomial PMF, \\(X \\sim \\text{Binomial}(N, \\theta)\\) this way:\n\n\\[\n\\text{Binomial}(x~|~N,\\theta) = \\binom{N}{x}\n\\theta^x (1 - \\theta)^{N - x}\n\\]\n\n\n\n\\(\\text{Binomial}(x~|~N=4,\\theta = 1/2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\nlibrary(MASS)\n\nN &lt;- 4 # Number of successes out of x trials\n\n# compute and plot the PMF\npmf &lt;- dbinom(x = 0:N, size = N, prob = 1/2)\nd &lt;- data.frame(x =  0:N, y = pmf)\np1 &lt;- ggplot(d, aes(x, pmf))\np1 &lt;- p1 + geom_col(width = .2) + \n  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +\n  ylab(\"P(X = x)\") + xlab(\"x = Number of Heads\") +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n# compute and plot the CDF\nx &lt;- seq(-0.5, 4.5, length = 500)\ncdf &lt;- pbinom(q = x, size = N, prob = 1/2)\nd &lt;- data.frame(q = x, y = cdf)\ndd &lt;- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)\np2 &lt;- ggplot(d, aes(x, cdf)) \np2 &lt;- p2 + geom_point(size = 0.2) + \n  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(CDF: F[X](x) == P(X &lt;= x))) +\n  ylab(expression(P(X &lt;= x))) + xlab(\"x = Number of Heads\")\n\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#binomial-in-r",
    "href": "stats-math/05-lecture/05-lecture.html#binomial-in-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial in R",
    "text": "Binomial in R\n\n# What is the probability of getting 2 Heads out of 5 fair trials?\nN &lt;- 5; x &lt;- 2\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 5/16\n\n# What is the binomial PMF: P(X = x), for N = 5, p = 0.5?\nN &lt;- 5; x &lt;- -2:7 # notice we range x over any integers\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]    0    0 1/32 5/32 5/16 5/16 5/32 1/32    0    0\n\n# Verify that the PMF sums to 1\nsum(dbinom(x = x, size = N, prob = 0.5))\n\n[1] 1\n\n# What is the probability of 3 heads or fewer\npbinom(3, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 13/16\n\n# compute the CDF: P(X &lt;= x), for N = 5, p = 0.5\npbinom(x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n# get from the PMF to CDF; cumsum() is the cumulative sum function\ndbinom(x = x, size = N, prob = 0.5) |&gt; cumsum() |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n\n\n\nYour Turn: Suppose the probability of success is 1/3, N = 10. What is the probability of 6 or more successes? Compute it with a PMF first and verify with the CDF."
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#geometric-rv",
    "href": "stats-math/05-lecture/05-lecture.html#geometric-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Geometric RV",
    "text": "Geometric RV\n\nPMFCheck ConvergenceExamplesPMFSumming the Geometric\n\n\n\n\nGeometric is a discrete waiting time distribution, and Exponential is its continuous analog\nIf \\(X\\) is the number of failures before first success \\(X \\sim \\text{Geometric}(\\theta)\\), where \\(\\theta\\) is probability of success\nExample: We keep flipping a coin until we get success, say Heads\n\nSay we flip five times, which means we get the following sequence: T T T T H\nThe probability of this sequence is: \\((\\frac{1}{2})^4 (\\frac{1}{2})^1\\)\nNotice this is the only way to get this sequence\n\nIf \\(x\\) is the number of failures, the PMF is \\(P(X = x) = (1 - \\theta)^x \\theta\\), where \\(x = 0, 1, 2, ...\\)\n\n\n\n\n\nTo check if this is a valid PMF, we need to sum over all \\(x\\):\n\n\n\\[\n\\begin{align}\n\\sum_{x = 0}^{\\infty} \\theta (1 - \\theta)^x  =\n\\theta \\sum_{x = 0}^{\\infty} (1 - \\theta)^x \\\\\n\\text{Let } u = 1 - \\theta \\\\\n\\theta \\sum_{x = 0}^{\\infty} u^x = \\theta \\frac{1}{1-u} = \\theta \\frac{1}{1-1 + \\theta} = \\frac{\\theta}{\\theta} = 1\n\\end{align}\n\\]\n\n\n\nThe last bit comes from geometric series for \\(|u| &lt; 1\\)\n\n\n\n\n\nThe probability of T T T H (x = 3 failures) when \\(\\theta = 1/2\\), has to be \\((1/2)^4\\) or \\(1/16\\)\n\n\n\nx &lt;- 3; theta &lt;- 1/2\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 1/16\n\n\n\n\n\nIf \\(\\theta = 1/3\\), the probability of the same sequence has to be \\((2/3)^3 \\cdot 1/3 = 8/81\\)\n\n\nx &lt;- 3; theta &lt;- 1/3\ndgeom(x = x, prob = theta) |&gt; fractions()\n\n[1] 8/81\n\n\n\n\n\nThe PMF is unbounded, but it converges to 1 as demonstrated before\n\n\n\n\n\n\nx &lt;-  0:15\ntheta &lt;- 1/5\ny &lt;- dgeom(x = x, prob = theta)\nd &lt;- data.frame(x, y)\np &lt;- ggplot(d, aes(x, y))\np + geom_col(width = 0.2) +\nxlab(\"x = Number of failures before first success\") +\n ylab(expression(P(X == x))) +\nggtitle(\"X ~ Geom(1/5)\", \n subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n\n\n\n\n\n\n\n\n\n\nLet’s consider the infinite geometric series:\n\\[\nS = \\sum_{x=0}^{\\infty} u^x = u^0 + u^1 + u^2 + u^3 + \\dots\n\\] This can be written explicitly as: \\[\nS = 1 + u + u^2 + u^3 + \\dots\n\\]\nNext, multiply the entire series by \\(u\\):\n\\[\nuS = u \\cdot (1 + u + u^2 + u^3 + \\dots)\n\\]\nThis results in: \\[\nuS = u + u^2 + u^3 + u^4 + \\dots\n\\]\nNow, subtract the equation for \\(uS\\) from the equation for \\(S\\):\n\\[\nS - uS = (1 + u + u^2 + u^3 + \\dots) - (u + u^2 + u^3 + u^4 + \\dots)\n\\]\nNotice that all terms on the right-hand side except the first term (which is 1) cancel out:\n\\[\nS - uS = 1\n\\]\nFactor the left-hand side:\n\\[\nS(1 - u) = 1\n\\]\nFinally, solve for \\(S\\):\n\\[\nS = \\frac{1}{1 - u}\n\\]"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#expectations",
    "href": "stats-math/05-lecture/05-lecture.html#expectations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Expectations",
    "text": "Expectations\n\nIntroductionExamplesR CodeE(X) of Bernoulli and Binomial\n\n\n\n\nAn expectation is a kind of average, typically a weighted average\nExpectation is a single number summary of the distribution\nWhen computing a weighted average, the weights must add up to one\nThat’s fortunate for us since the PMF satisfies this property\nSo, the expectation of a discrete RV is the sum of its values weighted by their respective probabilities\nFor a discrete RV, we have: \\(\\E(X) = \\sum_{x}^{} x f_X(x)\\)\nFor continuous RVs we have: \\(\\E(X) = \\int x f_X(x)\\, dx\\)\n\n\n\n\n\n\nLet’s compute the expectation of the Geometric RV, \\(X \\sim \\text{Geom}(\\theta)\\)\nWe can solve the expectation by differentiation, but we will skip the mechanics\nHere, we will just write the answer:\n\n\n\n\\[\n\\E(X) = \\sum_{x=0}^{\\infty} x \\theta (1 - \\theta)^x = \\frac{1-\\theta}{\\theta}\n\\]\n\n\n\nIn particular, for \\(X \\sim \\text{Geom}(1/5)\\), what is the expected number of failures before the first success?\nThe answer is \\(4 = \\frac{(1 - \\frac{1}{5})}{\\frac{1}{5}}\\)\n\n\n\n\n\n\nLet’s check computationally:\n\n\n\n\ntheta &lt;- 1/5\nx &lt;- 0:100\nsum(x * dgeom(x = x, prob = theta))\n\n[1] 4\n\ntheta &lt;- 1/4\nsum(x * dgeom(x = x, prob = theta))\n\n[1] 3\n\n\n\n\n\nQuestion: Geometric is unbounded, yet summed a finite number (100) and got the right answer. What’s going on?\nGeometric decays quickly, and higher terms are negligible for these choices of \\(\\theta\\)\nWithout doing any calculations at all, what is \\(\\E(X)\\), where \\(X \\sim \\text{Geom}(1/8)\\)\n\n\n\n\n\n\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), \\(\\E(X) = \\sum_{x=0}^{1} x \\theta^x (1 - \\theta)^{1-x} = 0 + 1 \\cdot \\theta \\cdot 1 = \\theta\\)\nFor the Binomial, is it easier to think about it as a sum of iid Bernoulli RVs. Each Bernoulli RV has an expectation of \\(\\theta\\), and there are \\(n\\) of them, so the expectation is \\(n \\theta\\).\nThis argument relies on the linearity of expectations: \\(\\E(X_1 + X_2 ... + X_n) = \\E(X_1) + \\E(X_2) + ... + \\E(X_n) = \\theta + ... + \\theta = n\\theta\\)"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#st.-petersburg-paradox",
    "href": "stats-math/05-lecture/05-lecture.html#st.-petersburg-paradox",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "St. Petersburg paradox",
    "text": "St. Petersburg paradox\n\nFirst Success DistributionThe ParadoxExpected Utility\n\n\n\n\nThis is a slight modification to Geometric as we are counting the number of failures, including the first success vs. Geometric, where we counted the failures\n\\(X \\sim \\text{FS}(\\theta)\\), if \\(\\P(X = x) = (1 - \\theta)^{x - 1} \\theta\\), for \\(x = 1, 2, 3, ...\\)\n\\(X - 1 \\sim \\text{Geom}(\\theta)\\)\nSay, \\(Y \\sim \\text{Geom}(\\theta)\\), then \\(\\E(X) = \\E(Y + 1) = \\E(Y) + 1 = \\frac{1 - \\theta}{\\theta} + \\frac{\\theta}{\\theta} = \\frac{1}{\\theta}\\)\n\n\n\n\n\n\nSuppose you are offered the following game: you flip a coin until heads appear. You get $2 if the game ends after round 1, $4 after two rounds, $8 after three, and so on.\n\n\n\n\\[\n\\E(X) = \\sum_{n=1}^{\\infty} \\frac{1}{2^n} \\cdot 2^n =  \\sum_{n=1}^{\\infty} 1 = \\infty\n\\]\n\n\n\nVote how much are you willing to pay to play this game? (you can’t lose)\nHow many rounds do we expect to play? If \\(N\\) is the number of rounds, then \\(N \\sim \\text{FS}(1/2)\\), and \\(\\E(N) = \\frac{1}{1/2} = 2\\)\nIn addition, the utility of money has been shown to be non-linear\n\n\n\n\n\n\nDaniel Bernoulli (1700 - 1782) proposed that \\(\\frac{dU}{dw} \\propto \\frac{1}{w}\\), where \\(U\\) is the utility function and \\(w\\) is the wealth\nThis gives rise to the following differential equation \\(\\frac{dU}{dw} = k \\frac{1}{w}\\), where \\(k\\) is the constant of proportionality\nIntegrating both sides: \\(\\int dU = k \\int \\frac{1}{w} dw\\) we get:\n\n\\(U(w) = k \\log(w) + C\\), where \\(C\\) is some initial wealth\n\nFor simplicity let’s take \\(k=1\\), and consider \\(\\log = \\log_2\\)\n\n\n\n\\[\n\\E(U) = \\sum_{n=1}^{\\infty} \\frac{1}{2^n} \\log_2(2^n) =  \\sum_{n=1}^{\\infty} \\frac{n}{2^n} = 2 \\neq \\infty\n\\]\n\n\n\nThe last equality comes from summing the geometric series \\(\\sum_{n=1}^{\\infty} \\frac{n}{2^n}\\) using the same methods we used for Geometric distribution"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#indicator-random-variable",
    "href": "stats-math/05-lecture/05-lecture.html#indicator-random-variable",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Indicator Random Variable",
    "text": "Indicator Random Variable\n\nRecall our \\(\\pi\\) SimulationFundamental Bridge\n\n\n\n\\[\n\\I_A(s) = \\I(s \\in A) = \\left\\{\\begin{matrix}\n1 & \\text{if } s \\in A \\\\\n0 & \\text{if } s \\notin A\n\\end{matrix}\\right.\n\\]\n\n\nRecall from Lecture 1:\n\\[\n\\begin{align}\nX& \\sim \\text{Uniform}(-1, 1) \\\\\nY& \\sim \\text{Uniform}(-1, 1) \\\\\n\\pi& \\approx \\frac{4 \\sum_{i=1}^{N} \\text{I}(x_i^2 + y_i^2 &lt; 1)}{N}\n\\end{align}\n\\]\n\n\n\n\n\nThere is a link between probability and (expectations of) indicator RVs\nJoe Blitzstein calls it the fundamental bridge: \\(\\P(A) = \\E(\\I_A)\\)\n\n\n\n\n# Pr of two heads in five trials\n(P2 &lt;- dbinom(2, 5, prob = 1/2))\n\n[1] 0.3125\n\n# Let's create a realization Binom(5, 1/2)\nx &lt;- rbinom(1e6, size = 5, prob = 1/2)\nx[1:30]\n\n [1] 0 2 0 0 2 1 3 4 3 2 2 1 2 0 2 2 1 0 3 0 2 3 0 3 2 0 2 2 2 4\n\n# indicator RV: if x == 2 Ix = TRUE (1), else Ix = (0)\nI2 &lt;- (x == 2)\nI2[1:30] |&gt; as.integer()\n\n [1] 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n\n# compute E(Ix) == P2\nmean(I2)\n\n[1] 0.312888"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#lotus",
    "href": "stats-math/05-lecture/05-lecture.html#lotus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "LOTUS",
    "text": "LOTUS\n\n\nLOTUS stands for the Law Of The Unconscious Statistician\nIt allows us to work with PDFs of the original RV \\(X\\), even though we want to compute an expectation of the function of that RV, say \\(g(X)\\)\n\n\n\n\\[\n\\begin{eqnarray}\nE(g(X)) &=& \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx \\\\\nE(g(X)) &=& \\sum_{x} g(x) p_X(x)\n\\end{eqnarray}\n\\]\n\n\n\nNotice, that we don’t need to compute \\(f_{g(X)}(x)\\) or \\(p_{g(X)}(x)\\)\nThe idea is that even though the \\(x\\)s change from, say \\(x\\) to \\(x^2\\), reflecting the fact the \\(g(X) = X^2\\), that did not change their probability assignments, we can still work with \\(\\P(X = x)\\) (in case of X discrete)"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#variance",
    "href": "stats-math/05-lecture/05-lecture.html#variance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Variance",
    "text": "Variance\n\nDefinitionsComputation\n\n\n\n\nVariance is a measure of the spread of the distribution \\(\\V(X) = \\E(X - E(X))^2 = \\E(X^2) - \\E(X)^2\\)\nIf we let \\(\\mu = \\E(X)\\), this becomes: \\(\\E(X - \\mu)^2 = \\E(X^2) - \\mu^2\\)\nUnlike Expectation, Variance is not a linear operator. In particular, \\(\\V(cX) = c^2 \\V(X)\\)\n\\(\\V(c + X) = \\V(X)\\): constants don’t vary\nIf \\(X\\) and \\(Y\\) are independent \\(\\V(X + Y) = \\V(X) + \\V(Y)\\), but unlike for Expectations, this is not true in general\nSquare root of Variance is called a standard deviation \\(\\text{sd} := \\sqrt{\\V}\\), which is easier to interpret as it is expressed in the same units as data \\(x\\)\n\n\n\n\n\nIn R, estimated Variance can be computed with var() and standard deviation with sd()\nFor reference, the variance of the Geometric distribution is: \\((1 - \\theta)/\\theta^2\\)\n\n\nn &lt;- 1e5; theta1 &lt;- 1/6; theta2 &lt;- 1/3\nx &lt;- rgeom(n, prob = theta1)\nvar(x) |&gt; round(2)\n\n[1] 29.83\n\ny &lt;- rgeom(n, prob = theta2)\nvar(y) |&gt; round(2)\n\n[1] 6.01\n\n(var(x) + var(y)) |&gt; round(2)\n\n[1] 35.83\n\nvar(x + y) |&gt; round(2)\n\n[1] 35.8\n\ncov(x, y) |&gt; round(2)\n\n[1] -0.01\n\n(var(x) + var(y) + 2*cov(x, y)) |&gt; round(2)\n\n[1] 35.8\n\n# compare to the analytic result\n(1 - theta1) / (theta1)^2 + (1 - theta2) / (theta2)^2 \n\n[1] 36"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#poisson-random-variable",
    "href": "stats-math/05-lecture/05-lecture.html#poisson-random-variable",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Poisson Random Variable",
    "text": "Poisson Random Variable\n\n\nPoisson distributions arise when we are modeling counts\nBut not every type of counting can be modeled with Poisson, just like not every kind of waiting can be modeled by Geometric\nWe write \\(X \\sim \\text{Poisson}(\\lambda)\\)\nThe PDF of Poisson is \\(\\P(X=x) = \\frac{{\\lambda^x e^{-\\lambda}}}{{x!}}\\), where \\(x = 0, 1, 2, ...\\) and \\(\\lambda &gt; 0\\)\n\n\n\n\\[\n\\begin{eqnarray}\n\\sum_{x=0}^{\\infty} \\frac{{\\lambda^x e^{-\\lambda}}}{{x!}} = e^{-\\lambda} \\sum_{x=0}^{\\infty} \\frac{\\lambda^x}{x!} = 1\n\\end{eqnarray}\n\\]\n\n\n\nThe last equality follows, because: \\(e^{\\lambda} = \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}\\), which is its Taylor Series expansion\nFor Poisson, \\(\\E(X) = \\lambda\\) and \\(\\V(X) = \\lambda\\), which is why most real count data do not follow this distribution"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#poisson-pmf",
    "href": "stats-math/05-lecture/05-lecture.html#poisson-pmf",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Poisson PMF",
    "text": "Poisson PMF\n\nNotice the location of \\(\\E(X) = \\lambda\\) in each plot\n\n\np1 &lt;- dot_plot(0:10, dpois(0:10, lambda = 3)) + xlab(\"x\") + \n  ylab(expression(P(X == x))) + ggtitle(\"X ~ Poisson(3)\")\np2 &lt;- dot_plot(0:22, dpois(0:22, lambda = 10)) + xlab(\"x\") + \n  ylab(expression(P(X == x))) + ggtitle(\"X ~ Poisson(10)\")\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#continuous-rvs-and-the-uniform",
    "href": "stats-math/05-lecture/05-lecture.html#continuous-rvs-and-the-uniform",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Continuous RVs and the Uniform",
    "text": "Continuous RVs and the Uniform\n\nDefinitionsUniform\n\n\n\n\nWe leave the discrete world and enter continuous RVs\nWe can no longer say, \\(\\P(X = x)\\), since for a continuous RV \\(\\P(X = x) = 0\\) for all \\(x\\)\nInstead of PMFs, we will be working with PDFs, and we get probability out of them by integrating over the region that we care about\nFor a continuous RV: \\(\\int_{-\\infty}^{\\infty} f_X(x)\\, dx = 1\\)\n\n\n\n\\[\n\\begin{eqnarray}\nP(a &lt; X &lt; b) & = & \\int_{a}^{b} f_X(x)\\, dx \\\\\nF_X(x) & = & \\int_{-\\infty}^{x} f_X(u)\\, du\n\\end{eqnarray}\n\\]\n\n\n\n\nUniform \\(X \\sim \\text{Uniform}(\\alpha, \\beta)\\) has the following PDF:\n\\[\n\\text{Uniform}(x|\\alpha,\\beta) =\n\\frac{1}{\\beta - \\alpha}, \\,  \\text{where } \\alpha \\in \\mathbb{R} \\text{ and } \\beta \\in (\\alpha,\\infty)\n\\]\n\n\n\nYour Turn: Guess the \\(\\E(X)\\)\nNow derive \\(\\E(X)\\) using the definition of the Expected Value: \\(\\E(X) = \\int x f_X(x)\\, dx\\)\n\n\n\n\nx &lt;- seq(-0.5, 1.5, length = 100)\npdf_x &lt;- dunif(x, min = 0, max = 1)\np &lt;- ggplot(data.frame(x, pdf_x), aes(x, pdf_x))\np + geom_line() + ylab(expression(p[X](x))) +\n  ggtitle(\"X ~ Unif(0, 1)\", subtitle = expression(PDF: p[X](x) == 1))"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#example-stick-breaking",
    "href": "stats-math/05-lecture/05-lecture.html#example-stick-breaking",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Stick breaking",
    "text": "Example: Stick breaking\n\nAnalysisSimulation\n\n\n\n\nYou break a unit-length stick at a random point\nLet \\(X\\) be the breaking point so \\(X \\sim \\text{Uniform}(0, 1)\\)\nLet \\(Y\\) be the larger piece. What is \\(\\E(Y)\\)?\nLOTUS, says that we do not need \\(f_Y\\), we can work with \\(f_X\\)\n\n\n\n\\[\n\\E(Y)= \\int y(x) f_X(x) dx\n\\]\n\n\n\nThis works only if \\(Y\\) is a function of \\(X\\). Here, \\(Y = \\max\\{X, 1 - X\\}\\).\nWe consider two cases: when \\(x\\) is larger than \\(1/2\\), \\(y(x)\\) is between \\(1/2\\) and \\(1\\) and when \\(1 - x\\) is larger, it is between \\(0\\) and \\(1/2\\):\n\n\n\n\\[\ny(x) = \\left\\{\n\\begin{array}{ll} x &\n\\text{if } 1/2 &lt; x &lt; 1, \\text{ and} \\\\ 1 - x & 0 &lt; x &lt; 1/2\n\\end{array}  \\right.\n\\]\n\n\n\nIn other words, \\(\\E(Y)\\) can be computed as the sum of two integrals:\n\n\\[\n\\E(Y) = \\int_{0}^{1} y(x) f_X(x)\\, dx =    \\\\\n\\int_{1/2}^{1} x \\cdot 1 \\, dx + \\int_{0}^{1/2} (1-x) \\cdot 1 \\, dx = \\\\\n\\frac{x^2}{2} \\Biggr|_{1/2}^{1} + \\frac{1}{2} - \\frac{x^2}{2} \\Biggr|_{0}^{1/2} = \\frac{3}{4}\n\\]\n\n\n\n\n\nLet’s do a quick simulation in R\n\n\nx &lt;- runif(1e4, min = 0, max = 1) # pick a random breaking point on (0, 1)\ny &lt;- ifelse(x &gt; 0.5, x, 1 - x)    # generate y = max(x, 1-x)\nmean(y) |&gt; round(3)               # estimate the expectation E(Y)\n\n[1] 0.75"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#normal-rv",
    "href": "stats-math/05-lecture/05-lecture.html#normal-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Normal RV",
    "text": "Normal RV\n\nNormalProperties of Standard NormalExample: Heights of US adults\n\n\n\n\n\\(X \\sim \\text{Normal}(\\mu, \\sigma)\\) has the following PDF:\n\n\n\n\\[\n\\text{Normal}(x \\mid \\mu,\\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{x - \\mu}{\\sigma} \\right)^2    \\right) \\!\n\\]\n\n\n\nThe bell shape comes from the \\(\\exp(-x^2)\\) part\nThe expected value is \\(\\E(X) = \\mu\\), the mode (highest peak) and median are also \\(\\mu\\).\nVariance is \\(\\V(X) = \\sigma^2\\) and standard deviation \\(\\text{sd} = \\sigma\\)\nA Normal RV can be converted to standard normal by subtracting \\(\\mu\\) and dividing by \\(\\sigma\\)\n\n\n\n\\[\n\\text{Normal}(x \\mid 0, 1) \\ = \\\n\\frac{1}{\\sqrt{2 \\pi}} \\, \\exp \\left( \\frac{-x^2}{2} \\right)\\\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSums of small contributions tend to have a normal distribution\nHeights of people stratified by gender is a good example\nThe following data come from “Teaching Statistics, A Bag of Tricks” by Gelman and Nolan\n\n\n\n\nmu_m &lt;- 69.1   # mean heights of US males in inches\nsigma_m &lt;- 2.9 # standard deviation for same\nmu_w &lt;- 63.7   # mean heights of US women in inches\nsigma_w &lt;- 2.7 # standard deviation of the same\n\nx &lt;- seq(50, 80, len = 1e3)\npdf_m &lt;- dnorm(x, mean = mu_m, sd = sigma_m)\npdf_w &lt;- dnorm(x, mean = mu_w, sd = sigma_w)\np &lt;- ggplot(data.frame(x, pdf_m, pdf_w), aes(x, pdf_m))\np &lt;- p + geom_line(size = 0.2, color = 'red') + \n  geom_line(aes(y = pdf_w), size = 0.2, color = 'blue') +\n  xlab(\"Height (in)\") + ylab(\"\") +\n  ggtitle(\"Distribution of heights of US adults\") +\n  annotate(\"text\", x = 73.5, y = 0.10, label = \"Men\", color = 'red') +\n  annotate(\"text\", x = 58, y = 0.10, label = \"Women\", color = 'blue') +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nprint(p)\n\n\n\n\n\n\n\n\n\n\n\nThe combined distribution is not Normal:\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou randomly sample a man from the population. What is \\(\\P(\\text{Height}_m &lt; 65)\\)\n\n\nintegrate(dnorm, lower = -Inf, upper = 65, \n          mean = mu_m, sd = sigma_m)$value |&gt;\n  round(2)\n\n[1] 0.08\n\n\n\n\n\nYou randomly sample a woman from the population. What is \\(\\P(60 &lt; \\text{Height}_w &lt; 70)\\)\n\n\nintegrate(dnorm, lower = 60, upper = 70, \n          mean = mu_w, sd = sigma_w)$value |&gt;\n  round(2)\n\n[1] 0.9\n\n(integrate(dnorm, lower = -Inf, upper = 70, \n          mean = mu_w, sd = sigma_w)$value - \nintegrate(dnorm, lower = -Inf, upper = 60, \n          mean = mu_w, sd = sigma_w)$value) |&gt;\n  round(2)\n\n[1] 0.9\n\n\n\n\n\nWhat is the probability that a randomly chosen man is taller than a randomly chosen woman?\nWe have two distributions \\(M \\sim \\text{Normal}(\\mu_m, \\sigma_m)\\) and \\(W \\sim \\text{Normal}(\\mu_w, \\sigma_w)\\). We want \\(\\P(M &gt; W) = P(Z &gt; 0),\\, \\text{where }Z = M - W\\)\nSum of two normals is normal where both means and variances sum:\n\n\n\n\\[\n\\begin{eqnarray}\nZ & \\sim & \\text{Normal}(\\mu_m + \\mu_w,\\, \\sigma_m^2 + \\sigma_m^2) \\\\\nE(Z) & = & E(M - W) = E(M) - E(W) = 69.1 - 63.7 = 5.4 \\\\\n\\text{Var}(Z) & = & \\text{Var}(M - W) = \\text{Var}(M) + \\text{Var}(W) =  2.9^2 + 2.7^2 = 15.7 \\\\\n\\text{sd} & = & \\sqrt{Var} = \\sqrt{15.7} =3.96 \\\\\nZ & \\sim & \\text{Normal}(5.4, 3.96)\n\\end{eqnarray}\n\\]\n\n\n\nTo figure out when \\(Z &gt; 0\\), we can integrate the PDF from 0 to Infinity:\n\n\nintegrate(dnorm, lower = 0, upper = Inf, \n          mean = 5.4, sd = sqrt(15.7))$value |&gt;\n  round(2)\n\n[1] 0.91\n\n\n\n\n\nWe don’t have to integrate. We can evaluate the CDF instead:\n\n\n1 - pnorm(0, mean = 5.4, sd = sqrt(15.7)) |&gt;\n  round(2)\n\n[1] 0.91\n\n\n\n\n\nBy symmetry, the probability that a randomly chosen woman is taller than a randomly chosen man is 0.09\nHow can we check the analytic solution? We can do a simulation!\n\n\n\n\nn &lt;- 1e5\ntaller_m &lt;- numeric(n)\nfor (i in 1:n) {\n  height_m &lt;- rnorm(1, mu_m, sigma_m)\n  height_w &lt;- rnorm(1, mu_w, sigma_w)\n  taller_m[i] &lt;- height_m &gt; height_w\n}\nmean(taller_m) |&gt; round(2)\n\n[1] 0.91\n\n\n\n\n\nSuppose you wanted to compute the variance this way. How would you do it?"
  },
  {
    "objectID": "stats-math/05-lecture/05-lecture.html#what-we-did-not-cover",
    "href": "stats-math/05-lecture/05-lecture.html#what-we-did-not-cover",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What We Did Not Cover",
    "text": "What We Did Not Cover\n\nJoint, Marginal, and Conditional P[M/D]Fs\nCovariance and correlation\nConditional Expectations"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#session-3-outline",
    "href": "stats-math/03-lecture/03-lecture.html#session-3-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 3 Outline",
    "text": "Session 3 Outline\n\n\nTransforming data for plotting\nAntiderivative\nSome rules of integration\nEvaluating integrals numerically\nThe waiting time distribution\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#data-transformations",
    "href": "stats-math/03-lecture/03-lecture.html#data-transformations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data transformations",
    "text": "Data transformations\n\n\n\n\nWe saw that continuous compounding does not make such a big difference over time\nHow would the value vary by rate?\nWe investigate with a common simulate-pivot-plot pattern"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#map-function",
    "href": "stats-math/03-lecture/03-lecture.html#map-function",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Map Function",
    "text": "Map Function\n“The purrr::map* functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input.”\n\n\nGenerate 10 vectors of 100 uniform random realizations, where the first vector has min = 1, second min = 2 … last vector has min = 10, and max = 15 for all\nNow compute the average value of each of the 10 vectors\n\n\n\n\nlibrary(purrr)\n\nx &lt;- 1:10\ny &lt;- x |&gt;\n  map(\\(x) runif(n = 100, min = x, max = 15))\n\ny &lt;- x |&gt;\n  map(\\(x) runif(n = 100, min = x, max = 15)) |&gt;\n  map_dbl(mean)\n\n\n\n\nRun the code and look inside y after then first function call and after the second. What do you expect to see?"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#pivot-functions",
    "href": "stats-math/03-lecture/03-lecture.html#pivot-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Pivot Functions",
    "text": "Pivot Functions\n“pivot_longer()”lengthens” data, increasing the number of rows and decreasing the number of columns. The inverse transformation is pivot_wider()”\n\n\nlibrary(tidyr)\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\niris_long &lt;- iris |&gt; pivot_longer(!Species, names_to = \"length_width\", values_to = \"measure\")\nhead(iris_long)\n\n# A tibble: 6 × 3\n  Species length_width measure\n  &lt;fct&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 setosa  Sepal.Length     5.1\n2 setosa  Sepal.Width      3.5\n3 setosa  Petal.Length     1.4\n4 setosa  Petal.Width      0.2\n5 setosa  Sepal.Length     4.9\n6 setosa  Sepal.Width      3"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#data-transformations-1",
    "href": "stats-math/03-lecture/03-lecture.html#data-transformations-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Transformations",
    "text": "Data Transformations\n\nSimulateLoops and MapsPivotPlot\n\n\n\n\n\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nrates &lt;- seq(0.05, 0.20, length = 10)\nP &lt;- 100 \ntime &lt;- seq(1, 50, length = 50)\n\nPe &lt;- function(A, r, t) A * exp(r * t)\nd &lt;- time |&gt;\n  map(\\(x) Pe(A = P, r = rates, t = x)) \nclass(d)\n\n[1] \"list\"\n\nd[[1]][1:4]\n\n[1] 105.1271 106.8939 108.6904 110.5171\n\nnames(d) &lt;- as.character(time)\nd &lt;- as_tibble(d)\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\n105.1271\n110.5171\n116.1834\n\n\n106.8939\n114.2631\n122.1403\n\n\n108.6904\n118.1360\n128.4025\n\n\n110.5171\n122.1403\n134.9859\n\n\n112.3745\n126.2802\n141.9068\n\n\n114.2631\n130.5605\n149.1825\n\n\n116.1834\n134.9859\n156.8312\n\n\n118.1360\n139.5612\n164.8721\n\n\n120.1215\n144.2917\n173.3253\n\n\n122.1403\n149.1825\n182.2119\n\n\n\n\n\n\n\n\n\n\n\nModern R usage offers a lot of shortcuts but those may be confusing to beginners.\nIn particular, R loops have mostly been replaced with map() functions.\nWe recommend purrr::map() functions instead of R’s *apply().\n\n\n\n\nrates &lt;- seq(0.05, 0.20, length = 10)\nP &lt;- 100 \ntime &lt;- seq(1, 5, length = 50)\n\nPe &lt;- function(A, r, t) A * exp(r * t)\ntime |&gt; map(\\(x) Pe(A = P, r = rates, t = x)) \n\n# above is a shortcut for\nmap(time, function(x) Pe(A = P, r = rates, t = x))\n\n# and the above is a shortcut for the following loop\nl &lt;- list()\nfor (i in seq_along(time)) {\n  l[[i]] &lt;- Pe(A = P, r = rates, t = time[i])\n}\n\n\n\n\n\n\n\nlibrary(tidyr)\n# add rates as a column\nd &lt;- d %&gt;% mutate(rate = \n            round(rates, 2) |&gt;\n              as.character())\n\n# convert from wide format to long\nd &lt;- d %&gt;% \n  pivot_longer(!rate, \n               names_to = \"year\", \n               values_to = \"value\") \n\nd$year &lt;- as.numeric(d$year)\n\n\nHere is a cheat sheet explaining tidyr functions.\nAnd here is much shorter version of the exercise.\n\n\n\n\n\n\n\n\nrate\nyear\nvalue\n\n\n\n\n0.05\n1\n105.1271\n\n\n0.05\n2\n110.5171\n\n\n0.05\n3\n116.1834\n\n\n0.05\n4\n122.1403\n\n\n0.05\n5\n128.4025\n\n\n0.05\n6\n134.9859\n\n\n0.05\n7\n141.9068\n\n\n0.05\n8\n149.1825\n\n\n0.05\n9\n156.8312\n\n\n0.05\n10\n164.8721\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(d, aes(year, value))\np + geom_line(aes(color = rate), linewidth = 0.2) +\n  scale_y_continuous(labels = \n          scales::dollar_format()) +\n  xlab(\"Time (years)\") + \n  ylab(\"Asset value\") +\n  ggtitle(\"Growth of $100 at different interest rates\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd |&gt; group_by(rate) |&gt; \n  summarise(y50 = last(value)) |&gt;\n  knitr::kable()\n\n\n\n\nrate\ny50\n\n\n\n\n0.05\n1218.249\n\n\n0.07\n2803.162\n\n\n0.08\n6450.009\n\n\n0.1\n14841.316\n\n\n0.12\n34149.510\n\n\n0.13\n78577.199\n\n\n0.15\n180804.241\n\n\n0.17\n416026.201\n\n\n0.18\n957266.257\n\n\n0.2\n2202646.579\n\n\n\n\n\n\n\n\n\n\n\nRStudio cheatsheats"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#your-turn",
    "href": "stats-math/03-lecture/03-lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nDataInstructionsOutput\n\n\n\n\n\n\n\n\n\n\nRun the following command: install.packages('HistData')\nFollowed by library(HistData)\nTale a look at the Arbuthnot dataset: ?Arbuthnot\n\n\n\n\n\n\nYear\nMales\nFemales\nPlague\nMortality\nRatio\nTotal\n\n\n\n\n1629\n5218\n4683\n0\n8771\n1.114243\n9.901\n\n\n1630\n4858\n4457\n1317\n10554\n1.089971\n9.315\n\n\n1631\n4422\n4102\n274\n8562\n1.078011\n8.524\n\n\n1632\n4994\n4590\n8\n9535\n1.088017\n9.584\n\n\n1633\n5158\n4839\n0\n8393\n1.065923\n9.997\n\n\n1634\n5035\n4820\n1\n10400\n1.044606\n9.855\n\n\n\n\n\n\n\n\nUse the tools to produce the plot the looks something like this.\nBonus: compute the ratio of female births and plot it."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#integral-calculus",
    "href": "stats-math/03-lecture/03-lecture.html#integral-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Integral Calculus",
    "text": "Integral Calculus\n\n\n\n\nIntegration plays a central role in Statistics\nIt is a way to compute Expectations and Event Probabilities\nIn Bayesian Statistics, we use integration to compute posterior distributions of the unknowns\nIn Frequentist Statistics, we use derivatives to find the most likely values of the unknowns\n\n\n\n\n\n\n\n\n\n\nUnknowns are sometimes called parameters, like our \\(a\\) and \\(b\\), in the \\(x(t) = a + bt^2\\) model"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#intuition-behind-integration",
    "href": "stats-math/03-lecture/03-lecture.html#intuition-behind-integration",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Intuition Behind Integration",
    "text": "Intuition Behind Integration\n\n\n\n\nIntegration is a continous analog of summation\nYou can also think of an integral as undoing a derivative\nYou can also think of it as a signed area under a (one-dimensional) function \\(f\\)\nIn modern applications, integration is almost always done numerically on the computer\nBut it helps to understand what what the computer is doing\n\n\n\n\n\n\n\n\n\n\nImage Source: Calculus Volume 1"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#from-velocity-to-postion-functions",
    "href": "stats-math/03-lecture/03-lecture.html#from-velocity-to-postion-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "From Velocity to Postion Functions",
    "text": "From Velocity to Postion Functions\n\nAntiderivativePlot\n\n\n\n\nRecal our position function \\(x(t) = 2 + 3t^2\\)\nWe found the velocity function by differentiating and we can almost get back the position function by integrating.\n\n\n\n\\[\n\\begin{eqnarray}\nv(t) & = & \\frac{d}{dt} \\left( 2 + 3t^2 \\right) = 6t \\\\\nx(t) & = & \\int{6t\\, dt} = 3t^2 + C\n\\end{eqnarray}\n\\]\n\n\n\nWhy almost? Look at the constant \\(2\\) in \\(\\frac{d}{dt}(2 + 3t^2)\\). You can replace it with any other constant and the result will still be \\(6t\\).\nTo put it another way, to characterize the position fucntion you need to know the intial position and you can’t get that from the velocity function alone.\n\n\n\n\nThe position function for different values of initial position \\(C\\). Notice that the only thing that changes is the intercept."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#some-common-integrals",
    "href": "stats-math/03-lecture/03-lecture.html#some-common-integrals",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Common Integrals",
    "text": "Some Common Integrals\n\n\nOpenStax: Here is a more complete list"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#techniques-of-intergration",
    "href": "stats-math/03-lecture/03-lecture.html#techniques-of-intergration",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Techniques of Intergration",
    "text": "Techniques of Intergration\n\n\nIntegral, like a derivative, is a linear operator:\n\n\n\n\\[\n\\begin{eqnarray}\n\\int [f(x) + g(x)] \\, dx &=& \\int f(x) \\, dx + \\int g(x) \\, dx \\\\\n\\int [c \\cdot f(x)] \\, dx &=& c \\int f(x) \\, dx\n\\end{eqnarray}\n\\]\n\n\n\nUnlike derivatives, there are generablly no rules for finding integrals\nMost integrals do not have a closed-form, analytical solutions. This is true for almost all integrals in statistics.\nIn one or two dimentions, it is easy to evaluate most integrals numerically.\nIn higher dimentions, you need very sophisticated methods that rely on Markov Chain Monte Carlo (MCMC). We will not cover MCMC in this course.\nFor simple integrals we can somtimes find a closed-form solution by relying on u-substitution and integration by parts."
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#playing-with-integrals",
    "href": "stats-math/03-lecture/03-lecture.html#playing-with-integrals",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Playing with Integrals",
    "text": "Playing with Integrals\n\n\nGiven our intuition for integrals being singed areas, let’s see how to compute them analytically and numerically.\nWarning: these techniques only work in low dimentions. For high dimentional integrals you need to use MCMC.\nSuppose we want evaluate the integral \\(\\int_{1}^{3} x \\sin(x^2)\\, dx\\)"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#constructing-a-riemann-sum",
    "href": "stats-math/03-lecture/03-lecture.html#constructing-a-riemann-sum",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Constructing a Riemann Sum",
    "text": "Constructing a Riemann Sum\n\n\n\n\n\n\n\nriemann &lt;- function(f, lower, upper, step_size) {\n  step &lt;- seq(lower, upper, by = step_size)\n  s &lt;- 0 # initialize the sum\n  for (i in 2:length(step)) {\n    # multiply base by the height of the rectangle\n    area &lt;- step_size * f(step[i]) \n    s &lt;- s + area\n  }\n  return(s)\n}\n\n\n\nNotice that the function takes a function as an argument. These are called higher order functions.\n\n\nSource: OpenStax Calculus Volume 1"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#evaluating-the-integral",
    "href": "stats-math/03-lecture/03-lecture.html#evaluating-the-integral",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Evaluating the Integral",
    "text": "Evaluating the Integral\n\nf &lt;- function(x) x * sin(x^2)\nx &lt;- seq(0, pi, len = 100)\n\nriemann(f, 1, 3, 0.01)\n\n[1] 0.7275414\n\n# compute using R's integrate function\nintegrate(f, 1, 3)\n\n0.7257163 with absolute error &lt; 1.3e-09"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#integrating-analytically",
    "href": "stats-math/03-lecture/03-lecture.html#integrating-analytically",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Integrating Analytically",
    "text": "Integrating Analytically\n\n\nMost integrals can’t be evaluated analytically but we can do \\(\\int x \\sin(x^2)\\, dx\\).\nWe make a substitution. Let \\(u = x^2\\), then \\(du/dx = 2x\\) and \\(dx = \\frac{1}{2 \\sqrt{u}}du\\)\n\n\n\n\\[\n\\begin{eqnarray}\n\\int \\sqrt{u} \\cdot \\sin(u) \\frac{1}{2\\sqrt{u}}du  & = & \\\\\n\\frac{1}{2}\\int \\sin(u)\\, du & = & \\\\\n-\\frac{1}{2} \\cos(u) & = & -\\frac{1}{2} \\cos(x^2)  \n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#comparing-the-results",
    "href": "stats-math/03-lecture/03-lecture.html#comparing-the-results",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Comparing the Results",
    "text": "Comparing the Results\n\n\nUsing R’s integrate function:\n\n\nintegrate(f, 1, 3)\n\n0.7257163 with absolute error &lt; 1.3e-09\n\n\n\n\n\nUsing the analytical solution\n\n\nf1 &lt;- function(x) -1/2 * cos(x^2)\n\nf1(3) - f1(1)\n\n[1] 0.7257163\n\n\n\n\n\nThe universe is in balance!"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#analytical-integration-on-the-computer",
    "href": "stats-math/03-lecture/03-lecture.html#analytical-integration-on-the-computer",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analytical Integration on the Computer",
    "text": "Analytical Integration on the Computer\n\n\nWhen in doubt, you can always try WolframAlpha\nPython library SymPy through R pacakge caracas\n\n\n\nlibrary(caracas); library(stringr)\nadd_align &lt;- function(latex) {\n  str_c(\"\\\\begin{align} \", latex, \" \\\\end{align}\")\n}\nadd_int &lt;- function(latex) {\n  str_c(\"\\\\int \", latex, \"\\\\, dx\")\n}\nx &lt;- symbol('x'); f &lt;- x^2 / sqrt(x^2 + 4)\ntex(f) %&gt;% add_int() %&gt;% str_c(\" =\") %&gt;% add_align() %&gt;% cat()\n\\[\\begin{align} \\int \\frac{x^{2}}{\\sqrt{x^{2} + 4}}\\, dx = \\end{align}\\]\nint(f, x) %&gt;% tex() %&gt;% add_align() %&gt;% cat()\n\\[\\begin{align} \\frac{x \\sqrt{x^{2} + 4}}{2} - 2 \\operatorname{asinh}{\\left(\\frac{x}{2} \\right)} \\end{align}\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#your-turn-waiting-time",
    "href": "stats-math/03-lecture/03-lecture.html#your-turn-waiting-time",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn: Waiting Time",
    "text": "Your Turn: Waiting Time\n\nThere is s famous distribution in statistics called Exponential distribution\nIts probability density function (PDF) is given by:\n\n\\[\nf(x) = \\lambda e^{-\\lambda x}, \\, x &gt; 0, \\text{and } \\lambda &gt; 0\n\\]\n\nThis distribution is sometimes called the waiting time (to some event) distribution, where \\(\\lambda\\) is the rate of events we expect\nOne property of this distribution is that no matter how long you wait, the probability of seeing an event remains the same.\nOne of the properties of the PDF is that it must integrate to 1\nLet’s check that it’s true\n\n\\[\n\\int_{0}^{\\infty} \\lambda e^{-\\lambda x} dx =\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#ingegration-by-parts",
    "href": "stats-math/03-lecture/03-lecture.html#ingegration-by-parts",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Ingegration by Parts",
    "text": "Ingegration by Parts\n\\[\n\\begin{eqnarray}\n(f g)' & = & f'g + g'f \\\\\n\\int (f g)' \\, dx & = & \\int f'g dx + \\int g'f \\, dx \\\\\nfg & = & \\int f'g \\, dx + \\int g'f \\, dx \\\\\n\\int f g' \\, dx & = & fg - \\int f' g \\, dx \\\\\nu & = & f(x) \\\\\nv & = & g(x) \\\\\ndu & = & f'(x) \\, dx \\\\\ndv & = & g'(x) \\, dx \\\\\n\\int u \\, dv & = & uv - \\int v \\, du\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#exponential-growth-again",
    "href": "stats-math/03-lecture/03-lecture.html#exponential-growth-again",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Exponential Growth (again)",
    "text": "Exponential Growth (again)\nRecall, at the beginning we defined an exponential growth with the following differential equation:\n\n\\[\n\\frac{\\text{d}[y(t)]}{\\text{d}t} = k \\cdot y(t)\n\\]\n\n\nWe can now solve it:\n\\[\n\\begin{align*}\n\\frac{1}{y} \\, \\text{d}y &= k \\, \\text{d}t \\\\\n\\int \\frac{1}{y} \\, \\text{d}y &= \\int k \\, \\text{d}t \\\\\n\\log(y) &= k \\cdot t + C \\\\\ny(t) &= y_0 \\cdot e^{kt}, \\, y_0 = e^C\n\\end{align*}\n\\]"
  },
  {
    "objectID": "stats-math/03-lecture/03-lecture.html#homework",
    "href": "stats-math/03-lecture/03-lecture.html#homework",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Homework",
    "text": "Homework\n\nTake a look at dataset iris (?iris)\nCompute the overall average Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\nCompute the average by each Species of flower (hint: use group_by and summarise functions from dplyr)\nProduce the plot that looks like this:\n\n\n\n\n\n\n\nProduce the plot that looks like this: (check out geom_density and facet_wrap functions)\n\n\n\n\n\n\n\nCompute the following integral and show the steps:\n\n\\[\n\\int 2x \\cos(x^2)\\, dx\n\\]\n\nEvaluate this integral (on paper) from \\(0\\) to \\(2\\pi\\) and use R’s integrate function to validate your answer."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-objectives",
    "href": "stats-math/01-lecture/01-Lecture.html#course-objectives",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Objectives",
    "text": "Course Objectives\n\n\nThis course will help you to prepare for the A3SR MS Program by covering the minimal necessary foundation in computing, math, and probability.\nAfter completing the course, you will be able to write simple R programs and perform simulations, plot and manipulate data, solve basic probability problems, and understand the concept of regression\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-format",
    "href": "stats-math/01-lecture/01-Lecture.html#course-format",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Format",
    "text": "Course Format\n\n\nThe course will run for two weeks, five days per week, 3 hours per day\nEach day will consist of:\n\n~30-minute going over the homework questions\n~1.5-hour lecture\n~1-hour hands of exercises\n\nThe course will be computationally intensive – we will write many small programs."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#course-outline",
    "href": "stats-math/01-lecture/01-Lecture.html#course-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Course Outline",
    "text": "Course Outline\n\n\n\n\nIntroduction to the R language, RStudio, and R Markdown.\nBasic differentiation. Meaning of the derivative. Numeric and symbolic differentiation and optimization.\nBasic integration. Riemann integral and basic rules of integration.\nReview of one-dimensional probability. Conditional probability, random variables, and expectations. Solving probability problems by simulation.\n\n\n\n\n\nDiscrete distributions like Bernoulli and Binomial and continuous distributions like Normal and Exponential\nIntroduction to Linear Algebra. Vectors and vector arithmetic. Matrixes and matrix operations.\nManipulating and graphing data and Exploratory Data Analysis\nProgramming basics: variables, flow control, loops, functions, and writing simulations\nIntroduction to basic statistical inference and linear regression."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#references",
    "href": "stats-math/01-lecture/01-Lecture.html#references",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\nHands-On Programming with R, Grolemund (2014)\nR for Data Science 2e, Wickham, Çetinkaya-Rundel, and Grolemund (2023)\nCalculus Made Easy, Thompson (1980)\nCalculus, Herman, Strang, and OpenStax (2016)\nYouTube: Essence of Calculus, Sanderson (2018a)\nOptional: YouTube: Essense of Linear Algebra, Sanderson (2018b)\nOptional: Introduction to Linear Algebra, Boyd and Vandenberghe (2018)\nOptional: Matrix Cookbook, Petersen and Pedersen (2012)\nIntoduction to Probability, Blitzstein and Hwang (2019)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#where-are-you-from",
    "href": "stats-math/01-lecture/01-Lecture.html#where-are-you-from",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Where are you from?",
    "text": "Where are you from?"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#session-1-outline",
    "href": "stats-math/01-lecture/01-Lecture.html#session-1-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 1 Outline",
    "text": "Session 1 Outline\n\n\nThe big picture – costs and benefits\nSetting up an analysis environment\nRStudio projects\nWorking with interpreted languages like R\nSome basic R syntax and elements of R style\nGenerating data with R\nSome basic R and ggplot graphics\nWriting your first Monte Carlo simulation"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-silly",
    "href": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-silly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Silly",
    "text": "Some Mistakes Are Silly"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-deadly",
    "href": "stats-math/01-lecture/01-Lecture.html#some-mistakes-are-deadly",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Mistakes Are Deadly",
    "text": "Some Mistakes Are Deadly\nOn January 28, 1986, shortly after launch, Shuttle Challenger exploded, killing all seven crew members.\n\nO-Ring DataBinomial ModelReferences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability of 1 or more rings being damaged at launch is about 0.99\nProbability of all 6 rings being damaged at launch is about 0.46\n\n\n\n\n\nFowlkes and Hoadley (1989): Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure\nMartz and Zimmer (1992): The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.\n\n\n\n\n\nData source: UCI Machine Learning Repository"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#allan-mcdonald-dies-at-83",
    "href": "stats-math/01-lecture/01-Lecture.html#allan-mcdonald-dies-at-83",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Allan McDonald Dies at 83",
    "text": "Allan McDonald Dies at 83\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunication is part of the job — it’s worth learning how to do it well.\n\n\nSource: The New York Times, March 9, 2021"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#smac-why-bother",
    "href": "stats-math/01-lecture/01-Lecture.html#smac-why-bother",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "SMaC: Why Bother?",
    "text": "SMaC: Why Bother?\n\n\n\n\nProgramming: a cheap way to do experiments and a lazy way to do math\nDifferential calculus: optimize functions, compute MLEs\nIntegral calculus: compute probabilities, expectations\nLinear algebra: solve many equations at once\nProbability: the language of statistics\nStatistics: quantify uncertainty"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-differential-calculus",
    "href": "stats-math/01-lecture/01-Lecture.html#example-differential-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Differential Calculus",
    "text": "Example: Differential Calculus\n\nRegression Line ViewLikelihood View\n\n\n\n\n\n\nDifferentiation comes up when you want to find the most likely values of parameters (unknowns) in optimization-based (sometimes called frequentist) inference\nImagine that we need to find the values of the slope and intercept such that the yellow line fits “nicely” through the cloud of points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have linear regression model of the form \\(y_n \\sim \\operatorname{normal}(\\alpha + \\beta x_n, \\, \\sigma)\\) and you want to learn the most likely values of \\(\\alpha\\) and \\(\\beta\\)\nThe most likely values for slope (beta) and intercept (alpha) are at the peak of this function, which can be found by using (partial) derivatives"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-integral-calculus",
    "href": "stats-math/01-lecture/01-Lecture.html#example-integral-calculus",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Integral Calculus",
    "text": "Example: Integral Calculus\n\n\n\n\nSuppose we have a probability distribution of some parameter \\(\\theta\\), which represents the differences between the treated and control units\nFurther, suppose that \\(\\theta &gt; 0\\) favors the treatment group\nWe want to know the probability that treatment is better than control\nThis probability can be written as:\n\n\n\n\\[\n    \\P(\\theta &gt; 0) = \\int_{0}^{\\infty} p_{\\theta}\\, \\text{d}\\theta\n\\]\n\n\n\n\n\n\nAssuming \\(\\theta\\) is normally distributed with \\(\\mu = 1\\) and \\(\\sigma = 1\\) we can evaluate the integral as an area under the normal curve from \\(0\\) to \\(\\infty\\)."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-linear-regression",
    "href": "stats-math/01-lecture/01-Lecture.html#example-linear-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Linear Regression",
    "text": "Example: Linear Regression\nNotice a Linear Algebra notation \\(X \\beta\\), which is matrix-vector multiplication. \n\ndata {\n  int&lt;lower=0&gt; N;   // number of data items\n  int&lt;lower=0&gt; K;   // number of predictors\n  matrix[N, K] X;   // predictor matrix\n  vector[N] y;      // outcome vector\n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real&lt;lower=0&gt; sigma;  // error scale\n}\nmodel {\n  y ~ normal(X * beta + alpha, sigma);  // likelihood\n}\n\n\nLinear Regression in Stan. Source: Stan Manual\n\n\n\nStan is a probabilistic programming language. You write your data-generating process (model), and Stan performs inference for all the unknowns."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-bionomial-regression",
    "href": "stats-math/01-lecture/01-Lecture.html#example-bionomial-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Bionomial Regression",
    "text": "Example: Bionomial Regression\nThis is the type of model we fit to the O-Rings data.\n\n\ndata {\n  int&lt;lower=0&gt; N_rows; // number of rows in data\n  int&lt;lower=0&gt; N;      // number of possible \"successes\" in Binom(N, p)\n  vector[N_rows] x;    // temperature for the O-Rings example\n  array[N_rows] int&lt;lower=0, upper=N&gt; y; // number of \"successes\" in y ~ Binom(N, p)\n}\nparameters {\n  real alpha; \n  real beta;  \n}\nmodel {\n  alpha ~ normal(0, 2.5); // we can encode what we know about plausible values\n  beta ~ normal(0, 1);    // of alpha and beta prior to conditioning on the data\n  y ~ binomial_logit(N, alpha + beta * x); // likehood (conditioned on x)\n}\n\n\\[\n\\begin{eqnarray*}\n\\text{BinomialLogit}(y~|~N,\\theta) & = &\n\\text{Binomial}(y~|~N,\\text{logit}^{-1}(\\theta)) \\\\[6pt] & = &\n\\binom{N}{y} \\left( \\text{logit}^{-1}(\\theta) \\right)^{y}  \\left( 1 -\n\\text{logit}^{-1}(\\theta) \\right)^{N - y}  \n\\end{eqnarray*}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#example-irt-model",
    "href": "stats-math/01-lecture/01-Lecture.html#example-irt-model",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: IRT Model",
    "text": "Example: IRT Model\nItem-Response Theory models are popular in education research but generalize to other applications. \n\ndata {\n  int&lt;lower=1&gt; J;                     // number of students\n  int&lt;lower=1&gt; K;                     // number of questions\n  int&lt;lower=1&gt; N;                     // number of observations\n  array[N] int&lt;lower=1, upper=J&gt; jj;  // student for observation n\n  array[N] int&lt;lower=1, upper=K&gt; kk;  // question for observation n\n  array[N] int&lt;lower=0, upper=1&gt; y;   // correctness for observation n\n}\nparameters {\n  real delta;            // mean student ability\n  array[J] real alpha;   // ability of student j - mean ability\n  array[K] real beta;    // difficulty of question k\n}\nmodel {\n  alpha ~ std_normal();         // informative true prior\n  beta ~ std_normal();          // informative true prior\n  delta ~ normal(0.75, 1);      // informative true prior\n  for (n in 1:N) {\n    y[n] ~ bernoulli_logit(alpha[jj[n]] - beta[kk[n]] + delta);\n  }\n}\n\n\n1PL item-response model. Source: Stan Manual"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#analysis-environment",
    "href": "stats-math/01-lecture/01-Lecture.html#analysis-environment",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Environment",
    "text": "Analysis Environment\n\n\n\n\nInstructions for installing R and RStudio\nInstall the latest version of R\nInstall the latest version of the RStudio Desktop\nCreate a directory on your hard drive and give it a simple name. Mine is called statsmath\nIn RStudio, go to File -&gt; New Project and select: “Existing Directory”\nHow many of you have not used RStudio?\n\n\n\n\n\n\nDownload R, Download RStudio"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#what-is-r",
    "href": "stats-math/01-lecture/01-Lecture.html#what-is-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What is R",
    "text": "What is R\n\n\n\n\nR is an open-source, interpreted, weakly typed, (somewhat) functional programming language\nR is an implementation of the S language developed at Bell Labs around 1976\nRoss Ihaka and Robert Gentleman started working on R in the early 1990s\nVersion 1.0 was released in 2000\nThere are ~20,000+ R packages available on CRAN\nR has a large and mostly friendly user community\n\n\n\n\n\n\n Source: StackOverflow Developer Survey"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-very-basics",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-very-basics",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Very Basics",
    "text": "Hands-On: Very Basics\n\n\n\n2 + 3\n\n[1] 5\n\n10^3\n\n[1] 1000\n\n10 - 5 / 2\n\n[1] 7.5\n\n(10 - 5) / 2\n\n[1] 2.5\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n1:10 + 1\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\nsin(pi/2) + cos(pi/2)\n\n[1] 1\n\n1i\n\n[1] 0+1i\n\nround(exp(1i * pi) + 1)\n\n[1] 0+0i\n\n(exp(1i * pi) + 1) |&gt; round()\n\n[1] 0+0i\n\n\n\n\n\n\ndie &lt;- 1:6\ndie &lt;- seq(1, 6, by = 1)\ndie\n\n[1] 1 2 3 4 5 6\n\nlength(die) \n\n[1] 6\n\nstr(die)     \n\n num [1:6] 1 2 3 4 5 6\n\nsum(die)\n\n[1] 21\n\nprod(die)\n\n[1] 720\n\nmean(die)\n\n[1] 3.5\n\nsum(die) / length(die)\n\n[1] 3.5\n\nmedian(die)\n\n[1] 3.5\n\nsd(die)\n\n[1] 1.870829"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#r-documentation",
    "href": "stats-math/01-lecture/01-Lecture.html#r-documentation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "R Documentation",
    "text": "R Documentation\n\n?mean # same as help(mean)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#your-turn",
    "href": "stats-math/01-lecture/01-Lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nCreate a variable called fib that contains the first 10 Fibonacci numbers\nThey are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34\nCompute the length of this vector in R\nCompute the sum, the product, and the difference (diff()) between successive numbers\nWhat do you notice about the pattern in the differences?\nNow, create a vector of 100 integers from 1 to 100\nYoung Gauss (allegedly) was asked to sum them by hand\nHe figured out that the sum has to be \\(N (N + 1) / 2\\)\nVerify that Gauss was right (just for 100)\nNow compute the sum of the first hundred squares"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-lists",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-lists",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Lists",
    "text": "Hands-On: Lists\n\n\nLists are collections of objects of different types and shapes\nContrast with a data frame, which we will discuss later, that contains objects of different types but is rectangular\n\n\n\n\nlist_x &lt;- list(A = pi, B = c(0, 1), C = 1:10, D = c(\"one\", \"two\"))\nlist_x\n\n$A\n[1] 3.141593\n\n$B\n[1] 0 1\n\n$C\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$D\n[1] \"one\" \"two\"\n\nstr(list_x)\n\nList of 4\n $ A: num 3.14\n $ B: num [1:2] 0 1\n $ C: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ D: chr [1:2] \"one\" \"two\"\n\nlist_x$A\n\n[1] 3.141593\n\nlist_x[1]\n\n$A\n[1] 3.141593\n\nstr(list_x[1])\n\nList of 1\n $ A: num 3.14\n\nlist_x[[1]] # same as x$A\n\n[1] 3.141593"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#data-frames",
    "href": "stats-math/01-lecture/01-Lecture.html#data-frames",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\nData frames are rectangular structures that are often used in data analysis\nThere is a built-in function called data.frame, but we recommend tibble, which is part of the dplyr package\nYou can look up the documentation of any R function this way: ?dplyr::tibble. If the package is loaded by using library(dplyr) you can omit dplyr:: prefix\nJohn F. W. Herschel’s data on the orbit of the Twin Stars \\(\\gamma\\) Virginis"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#data-frames-1",
    "href": "stats-math/01-lecture/01-Lecture.html#data-frames-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\nlibrary(HistData)\ndata(\"Virginis.interp\")\nclass(Virginis.interp)\n\n[1] \"data.frame\"\n\nVirginis.interp |&gt; dplyr::as_tibble()\n\n# A tibble: 14 × 4\n    year posangle distance velocity\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1  1720    160      17.2    -0.32 \n 2  1730    157.     16.8    -0.354\n 3  1740    153      16.3    -0.376\n 4  1750    149.     15.5    -0.416\n 5  1760    144.     14.5    -0.478\n 6  1770    140.     13.7    -0.533\n 7  1780    134.     13.5    -0.547\n 8  1790    129.     12.9    -0.597\n 9  1800    122.     12.6    -0.632\n10  1810    116.     11.2    -0.8  \n11  1815    111.     10.4    -0.929\n12  1820    106.      9.57   -1.09 \n13  1825     98.3     7.09   -1.99 \n14  1830     84.3     4.9    -4.16 \n\nVirginis.interp |&gt; dplyr::as_tibble() |&gt; class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n Code for generated the above plot can be found here.\n\n\n\nThe Origin and Development of the Scatterplot by M Friendly and H Wainer"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#your-turn-1",
    "href": "stats-math/01-lecture/01-Lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nSave Virginis.interp in a new tibble called virginis using as_tibble() function\nCompute average velocity in virginis\nHint: you can access columns of tibbles and data frames with an $ like this: dataframe_name$variable_name\nWe will do a lot more work with tibbles and data frames in later sessions"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#basic-plotting",
    "href": "stats-math/01-lecture/01-Lecture.html#basic-plotting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Basic Plotting",
    "text": "Basic Plotting\n\nJohn SnowDeaths Over TimeMap of Cholera in London\n\n\n\n\n\n\n\n\n\n\n\n\nR base plotting system is great for making quick graphs with relatively little typing\nIn base plot, you add elements to the plot directly, as opposed to describing how the graph is constructed\nWe will demonstrate with John Snow’s data from the 1854 cholera outbreak\n\n\n\n\n\n\n\nlibrary(HistData)\nlibrary(lubridate)\n\n# set up the plotting area so it looks nice\npar(mar = c(3, 3, 2, 1), mgp = c(2, .7, 0), \n    tck = -.01, bg = \"#f0f1eb\")\nclr &lt;- ifelse(Snow.dates$date &lt; mdy(\"09/08/1854\"), \n              \"red\", \"darkgreen\")\nplot(deaths ~ date, data = Snow.dates, \n     type = \"h\", lwd = 2, col = clr, xlab = \"\")\npoints(deaths ~ date, data = Snow.dates, \n       cex = 0.5, pch = 16, col = clr)\ntext(mdy(\"09/08/1854\"), 40, \n     \"Pump handle\\nremoved Sept. 8\", pos = 4)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#hands-on-simulating-coin-flips",
    "href": "stats-math/01-lecture/01-Lecture.html#hands-on-simulating-coin-flips",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Hands-On: Simulating Coin Flips",
    "text": "Hands-On: Simulating Coin Flips\nWe can learn a lot through simulation. We will start with the sample() function.\nsample(x, size, replace = FALSE, prob = NULL)\n\n# simulating coin flips\ncoin &lt;- c(\"H\", \"T\")\n\n# flip a fair coin 10 times; try it with replace = FALSE\nsample(coin, 10, replace = TRUE)\n\n [1] \"H\" \"T\" \"T\" \"H\" \"T\" \"T\" \"H\" \"H\" \"H\" \"T\"\n\n# flip a coin that has P(H = 0.6) 10 times\nsample(coin, 10, replace = TRUE, prob = c(0.6, 0.4))\n\n [1] \"T\" \"H\" \"H\" \"T\" \"T\" \"H\" \"H\" \"T\" \"H\" \"H\"\n\n# it's more convenient to make H = 1 and T = 0\ncoin &lt;- c(1, 0)\nsample(coin, 10, replace = TRUE)\n\n [1] 1 0 1 1 1 0 0 1 1 1\n\n\n\n\nYour turn: flip a coin 1000 times and compute the proportion of heads"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#for-loops",
    "href": "stats-math/01-lecture/01-Lecture.html#for-loops",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "For Loops",
    "text": "For Loops\n\nFor LoopsSolution\n\n\n\n\nSuppose you want to add the first 100 integers as before but without using the sum() function or the formula\nIn math notation: \\(\\sum_{i = 1}^{100}x_i\\), \\(\\mathbf{x} = (1, 2, 3, \\dots, 100)\\)\n\n\n\n\nn &lt;- 100\nx &lt;- 1:n\ns &lt;- 0\nfor (i in 1:n) {\n  s &lt;- s + x[i] \n}\n# test\ns == sum(x)\n\n[1] TRUE\n\n\n\n\n\nYour turn: modify the loop to add only even numbers in 1:100. Look up help(if) statement and modulo operator help(%%); write a test to check your work\n\n\n\n\n\n\ns &lt;- 0\nfor (i in 1:n) {\n  if (x[i] %% 2 == 0) {\n    s &lt;- s + x[i]\n  }\n}\ncat(s)\n\n2550\n\n# test \ns == sum(seq(2, 100, by = 2))\n\n[1] TRUE\n\n\n\n\n\nCongratulations, you are now Turing Complete!"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#simulate-coin-flips",
    "href": "stats-math/01-lecture/01-Lecture.html#simulate-coin-flips",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Simulate Coin Flips",
    "text": "Simulate Coin Flips\n\n\nIf we flip a coin more and more times, would the estimate of the proportion become better?\nIf so, what is the rate of convergence?\n\n\n\n\nset.seed(1) # why do we do this?\nn &lt;- 1e4\nest_prop &lt;- numeric(n) # allocate a vector of size n\nfor (i in 1:n) {\n  x &lt;- sample(coin, i, replace = TRUE)\n  est_prop[i] &lt;- mean(x)\n}\n\n\n\n\nYour turn: write down in plain English what the above code is doing\nAt the end of the loop, what does the variable est_prop contain?"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#introduction-to-ggplot",
    "href": "stats-math/01-lecture/01-Lecture.html#introduction-to-ggplot",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to ggplot",
    "text": "Introduction to ggplot\n\n\nOur task is to visualize the estimated proportion as a function of the number of coin flips\nThis can be done in base plot(), but we will do it with ggplot\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nx &lt;- seq(0, 100, by = 5)\ny &lt;- x^2\n\nquadratic &lt;- tibble(x = x, y = y)\np1 &lt;- ggplot(data = quadratic, \n             mapping = aes(x = x, y = y))\np2 &lt;- p1 + geom_point(size = 0.5)\np3 &lt;- p1 + geom_line(linewidth = 0.2, \n                     color = 'red')\np4 &lt;- p1 + geom_point(size = 0.5) + \n  geom_line(linewidth = 0.2, color = 'red')\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#law-of-large-numbers",
    "href": "stats-math/01-lecture/01-Lecture.html#law-of-large-numbers",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\nCodePlot\n\n\n\nset.seed(1) \nn &lt;- 1e4\nest_prop &lt;- numeric(n) \nfor (i in 1:n) {\n  x &lt;- sample(coin, i, replace = TRUE)\n  est_prop[i] &lt;- mean(x)\n}\n\nlibrary(scales)\ndata &lt;- tibble(num_flips = 1:n, est_prop = est_prop)\np &lt;- ggplot(data = data, mapping = aes(x = num_flips, y = est_prop))\np + geom_line(size = 0.1) + \n  geom_hline(yintercept = 0.5, size = 0.2, color = 'red') +\n  scale_x_continuous(trans = 'log10', label = comma) +\n  xlab(\"Number of flips on Log10 scale\") +\n  ylab(\"Estimated proportion of Heads\") +\n  ggtitle(\"Error decreases with the size of the sample\")\n\n\n\nWe can see some evidence for the Law of Large Numbers.\n\n\n\n\n\n\n\n\n\nWLLN: \\(\\lim_{n \\to \\infty} \\mathbb{P}\\left( \\left| \\overline{X}_n - \\mu \\right| \\geq \\epsilon \\right) = 0\\)"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#functions",
    "href": "stats-math/01-lecture/01-Lecture.html#functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Functions",
    "text": "Functions\n\nStructureWriting Your Own FunctionCoin Flip Example\n\n\n\n\nFunctions help you break up the code into self-contained, understandable pieces.\nFunctions take in arguments and return results. You saw functions like sum() and mean() before. Here, you will learn how to write your own.\n\n\n\n\n\n\n\n\n\n\nSource: Hands-On Programming with R\n\n\n\n\n\nWe will write a function that produces one estimate of the proportion given a fixed sample size n.\n\n\nestimate_proportion &lt;- function(n) {\n  coin &lt;- c(1, 0)\n  x &lt;- sample(coin, n, replace = TRUE)\n  est &lt;- mean(x)\n  return(est)\n}\nestimate_proportion(10)\n\n[1] 0.4\n\nx &lt;- replicate(1e3, estimate_proportion(10))\nhead(x)\n\n[1] 0.3 0.7 0.5 0.8 0.5 0.6\n\n\n\n\n\nhist(x, \n     xlab = \"\", \n     main = \"Distribution of Proportions of Heads\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo reproduce our earlier example, generating estimates for increasing sample sizes, use map_dbl() function from purrr package. More on that here.\n\n\n\nlibrary(purrr)\npar(mar = c(3, 3, 2, 1), mgp = c(2, .7, 0), tck = -.01, bg = \"#f0f1eb\")\ny &lt;- map_dbl(2:500, estimate_proportion)\n# above is the same as sapply(2:500, estimate_proportion)\nplot(2:500, y, xlab = \"\", ylab = \"\", type = 'l')"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#generating-continuous-uniform-draws",
    "href": "stats-math/01-lecture/01-Lecture.html#generating-continuous-uniform-draws",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Generating Continuous Uniform Draws",
    "text": "Generating Continuous Uniform Draws\n\n\nHere, we will examine a continuous version of the sample() function: runif(n, min = 0, max = 1)\nrunif generates realizations of a random variable uniformly distributed between min and max.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour turn: what is the approximate value of this line of code: mean(runif(1e3, min = -1, max = 0))? Guess before running it."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\nThe idea is that we can approximate the ratio of the area of an inscribed circle, \\(A_c\\), to the area of the square, \\(A_s\\), by uniformly “throwing darts” at the square with the side \\(2r\\) and counting how many darts land inside the circle versus inside the square.\n\n\\[\n\\begin{align}\nA_{c}& = \\pi r^2 \\\\\nA_{s}& = (2r)^2 = 4r^2 \\\\\n\\frac{A_{c}}{A_{s}}& = \\frac{\\pi r^2}{4r^2} = \\frac{\\pi}{4} \\implies \\pi = \\frac{4A_{c}}{A_{s}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-1",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\nTo estimate \\(\\pi\\), we perform the following simulation:\n\n\\[\n\\begin{align}\nX& \\sim \\text{Uniform}(-1, 1) \\\\\nY& \\sim \\text{Uniform}(-1, 1) \\\\\n\\pi& \\approx \\frac{4 \\sum_{i=1}^{N} \\I(x_i^2 + y_i^2 &lt; 1)}{N}\n\\end{align}\n\\]\n\n\nThe numerator is a sum over an indicator function \\(\\I\\), which evaluates to \\(1\\) if the inequality holds and \\(0\\) otherwise."
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-2",
    "href": "stats-math/01-lecture/01-Lecture.html#estimating-pi-by-simulation-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Estimating \\(\\pi\\) by Simulation",
    "text": "Estimating \\(\\pi\\) by Simulation\n\nCode and PlotEstimate\n\n\n\n\nn &lt;- 1e3\nx &lt;- runif(n, -1, 1); y &lt;- runif(n, -1, 1)\ninside &lt;- x^2 + y^2 &lt; 1\ndata &lt;- tibble(x, y, inside)\np &lt;- ggplot(aes(x = x, y = y), data = data)\np + geom_point(aes(color = inside)) + theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\ncat(\"Estimated value of pi =\", 4*sum(inside) / n)\n\nEstimated value of pi = 3.112\n\n\n\n\n\n\n\n\nx\ny\ninside\n\n\n\n\n-0.8287282\n-0.2198830\nTRUE\n\n\n-0.9964402\n0.4200821\nFALSE\n\n\n-0.1473951\n-0.1864385\nTRUE\n\n\n0.6455366\n0.6547743\nTRUE\n\n\n0.7709646\n-0.9771841\nFALSE"
  },
  {
    "objectID": "stats-math/01-lecture/01-Lecture.html#references-2",
    "href": "stats-math/01-lecture/01-Lecture.html#references-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "References",
    "text": "References\n\n\n\n\nBlitzstein, Joseph K., and Jessica Hwang. 2019. Introduction to Probability. Second edition. Boca Raton: crc Press/Taylor & Francis Group.\n\n\nBoyd, Stephen P., and Lieven Vandenberghe. 2018. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares. Cambridge, UK ; New York, NY: Cambridge University Press.\n\n\nFowlkes, Edward B., and Bruce Hoadley. 1989. “Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure AU - Dalal, Siddhartha r.” Journal of the American Statistical Association 84 (408): 945–57. https://doi.org/10.1080/01621459.1989.10478858.\n\n\nGrolemund, Garrett. 2014. Hands-on Programming with r. First edition. Sebastopol, CA: O’Reilly. https://rstudio-education.github.io/hopr/.\n\n\nHerman, Edwin, Gilbert Strang, and OpenStax. 2016. Calculus Volume 1. https://d3bxy9euw4e147.cloudfront.net/oscms-prodcms/media/documents/CalculusVolume1-OP.pdf.\n\n\nMartz, H. F., and W. J. Zimmer. 1992. “The Risk of Catastrophic Failure of the Solid Rocket Boosters on the Space Shuttle.” The American Statistician 46 (1): 42–47. https://doi.org/10.1080/00031305.1992.10475846.\n\n\nPetersen, Kaare Brandt, and Michael Syskind Pedersen. 2012. “The Matrix Cookbook.” https://www.freetechbooks.com/the-matrix-cookbook-t435.html.\n\n\nSanderson, Grant. 2018a. “Essence of Calculus,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr.\n\n\n———. 2018b. “Essence of Linear Algebra,” November. https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab.\n\n\nThompson, Silvanus P. 1980. Calculus Made Easy: Being a Very-Simplest Introduction to Those Beautiful Methods of Reckoning Which Are Generally Called by the Terrifying Names of the Differential Calculus and the Integral Calculus. 3d ed. New York: St. Martin’s Press.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. O’Reilly Media."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math, Bayes, and R/Stan Programming",
    "section": "",
    "text": "This website contains teaching materials for Bayesian Inference course (APSTA-GE 2123) and Math, Statistics and R Programming Bootcamp (APSTA-GE 2006), both taught at NYU. If you find any errors, please email me at eric.novik@nyu.edu."
  },
  {
    "objectID": "bayes-course.html",
    "href": "bayes-course.html",
    "title": "APSTA-GE 2123: Bayesian Inference",
    "section": "",
    "text": "This is the home of the Spring 2025 Bayesian Inference class at NYU Steinhardt. If you are enrolled in the course, the assignments will be posted on Brightspace.\nThis is the current version of the syllabus. Please check the date, as it is subject to change."
  },
  {
    "objectID": "bayes-course.html#footnotes",
    "href": "bayes-course.html#footnotes",
    "title": "APSTA-GE 2123: Bayesian Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo my frequentist friends: don’t take this the wrong way, I still love you! Also, see this claim from a Bayesian that Bayesians are frequentists.↩︎"
  },
  {
    "objectID": "smac.html",
    "href": "smac.html",
    "title": "APSTA-GE 2006: Statistics, Math, and Computing Bootcamp",
    "section": "",
    "text": "This is the home for Math, Statistics and R Programming bootcamp offered by NYU PRIISM, the Center for Practice and Research at the Intersection of Information, Society, and Methodology. Registered students will be given access to Brightspace, where we will host an online forum.\nThe bootcamp aims to prepare students for the Applied Statistics for Social Science Research program at NYU. We will cover basic programming using the R language, including data manipulation and graphical displays; some key ideas from Calculus, including differentiation and integration; basic matrix algebra, including vector and matrix arithmetic; some core concepts in Probability, including random variables, discrete and continuous distributions, and expectations; and a few simple regression examples.\nThis is the current version of the syllabus, which is subject to change, so please check the date to make sure you have the most recent version."
  },
  {
    "objectID": "smac.html#course-materials-and-references",
    "href": "smac.html#course-materials-and-references",
    "title": "APSTA-GE 2006: Statistics, Math, and Computing Bootcamp",
    "section": "Course materials and references",
    "text": "Course materials and references\n\nProgramming and Data Visualization\n\nHands-On Programming with R, Grolemund, 2014\nR for Data Science, Wickham et al., 2023\nData Visualization, A practical introduction, Healy, 2018\n\n\n\nCalculus\n\nYouTube: Essence of Calculus, Sanderson, 2018\nCalculus Made Easy, Thompson, 1980\nCalculus, Herman et al., 2016\n\n\n\nProbability\n\nYouTube: Probability Animations Blitzstein\nYouTube: Statistics 110 @ Harvard Blitzstein\nIntroduction to Probability, Blitzstein et al., 2019\nIntroduction to Probability Cheat sheet v2, Chen, 2015\n\n\n\nStatistics\n\nRegression and Other Stories, Gelman et al., 2020\n\n\n\nLinear Algebra\n\nYouTube: Essense of Linear Algebra, Sanderson, 2018\nIntroduction to Linear Algebra, Boyd, 2018\nMatrix Cookbook, Petersen, 2012"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#session-2-outline",
    "href": "stats-math/02-lecture/02-Lecture.html#session-2-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 2 Outline",
    "text": "Session 2 Outline\n\n\n\n\nLinear, exponential, and logarithmic functions\nLimits\nDefinition of the derivative\nRules of differentiation\nThe chain rule and product rules\n\n\n\n\n\n\n\n\n\n\nImage source: Wikipedia"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#lines",
    "href": "stats-math/02-lecture/02-Lecture.html#lines",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Lines",
    "text": "Lines\n\nLinesExamplesCode\n\n\n\n\nWe typically use the slope-intercept form of the line: \\(y = a + bx\\), where \\(a\\) is the intercept, and \\(b\\) is the slope (rise over run).\nFor example: \\(y = 1.5 + 0.5x\\)\n\\(1.5\\) is the value of the function when either \\(x = 0\\) or \\(b = 0\\).\nIn practice, \\(b\\) is almost never zero.\n\n\n\n\n\n\nA graph of a line with the equation \\(y = 1 + 2x\\).\n\n\n\n\n\n\n\n\n\n\nA graph of a line with the equation \\(y = 7.5 - 2x\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n# y = 1 + 2x\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = 2, intercept = 1, size = 0.2)\n\n# y = 7.5 + -2x\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = -2, intercept = 7.5, size = 0.2)\n\n# to see some other ways of plotting lines\n?geom_abline"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nTurn the following code into a function called plot_line(slope, intercept)\nTest it for different values of slope and intercept\n\n\np &lt;- ggplot() + xlim(0, 5) + ylim(0, 10)\np + geom_abline(slope = 2, intercept = 1, size = 0.2)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#exponential-functions",
    "href": "stats-math/02-lecture/02-Lecture.html#exponential-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Exponential Functions",
    "text": "Exponential Functions\n\n\nThe idea of an exponential function is that the rate of change of the function at time \\(t\\) is proportional to the value of the function at time \\(t\\). In other words:\n\n\n\n\\[\n\\frac{\\text{d}[y(t)]}{\\text{d}t} = k \\cdot y(t)\n\\]\n\n\n\nThe solution to this differential equation is the exponential function.\nThink of population growth or growth of an interest-bearing asset."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-compound-interest",
    "href": "stats-math/02-lecture/02-Lecture.html#example-compound-interest",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Compound Interest",
    "text": "Example: Compound Interest\n\nAn asset that has a value \\(A\\) is invested with an annual interest rate \\(r\\). What is the balance in the account, \\(P_1\\), at the end of the first year?\n\n\n\\[\nP_1 = A + rA = A(1 + r)\n\\]\n\n\nAfter year two, the value is:\n\\[\nP_2 = P_1 + rP_1 = P_1(1 + r) = \\\\\nA(1 + r)(1 + r) = A(1 + r)^2\n\\]\n\n\nAnd after year \\(t\\), the value is:\n\\[\nP_n = A(1 + r)^t\n\\]\n\n\nWhat if we compound the interest twice per year? In that case:\n\\[\nP_1 = A \\left (1 + \\frac{r}{2} \\right)^2\n\\]\n\n\nIf we compound \\(n\\) times per year, the principal would be:\n\\[\nP_1 = A \\left (1 + \\frac{r}{n} \\right)^n\n\\]\n\n\nCombining these ideas, if we compound \\(n\\) times per year, for \\(t\\) years, we get:\n\\[\nP = A \\left (1 + \\frac{r}{n} \\right)^{nt}\n\\]\nIf we let \\(n \\to \\infty\\), we call this process exponential growth."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-compound-interest-1",
    "href": "stats-math/02-lecture/02-Lecture.html#example-compound-interest-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Compound Interest",
    "text": "Example: Compound Interest\n\nCodeCompoundingGrowth over time\n\n\n\nrate &lt;- 0.05    # interest rate r\nP &lt;- 100        # pricical P\nn_comp &lt;- 1:100 # number of compoundings n\n\nPn &lt;- function(n, A, r, t) A * (1 + r/n)^(t*n)\nPe &lt;- function(A, r, t) A * exp(r * t)\n\npn &lt;- Pn(n = n_comp, A = P, r = rate, t = 50)\npe &lt;- Pe(A = P, r = rate, t = 50)\n\nd &lt;- data.frame(n_comp, pn)\np &lt;- ggplot(d, aes(n_comp, pn))\np + geom_line(size = 0.2) +\n  geom_hline(yintercept = pe, color = 'red', size = 0.2) +\n  xlab(\"Number of times the interest is compounded\") +\n  ylab(\"Value of an asset\") +\n  ggtitle(\"Value of $100 at r = 0.05 after 50 years\")"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-1",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\nSuppose a particular population of bacteria is known to double in size every 4 hours. If a culture starts with 1000 bacteria, what is the population after 10 hours and 24 hours?"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-number-e",
    "href": "stats-math/02-lecture/02-Lecture.html#the-number-e",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Number \\(e\\)",
    "text": "The Number \\(e\\)\n\n\nRecall the equation for asset value after \\(t\\) years compounded \\(n\\) times:\n\n\\[\nP = A \\left (1 + \\frac{r}{n} \\right)^n = A \\left (1 + \\frac{r}{n} \\right)^{nt}\n\\]\n\n\n\nLet’s make a substituion \\(m = n/r\\) and take limit and \\(m \\to \\infty\\).\n\n\\[\nP = \\lim_{m \\to \\infty}A \\left ( 1 + \\frac{1}{m} \\right)^{m(rt)}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-number-e-1",
    "href": "stats-math/02-lecture/02-Lecture.html#the-number-e-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Number \\(e\\)",
    "text": "The Number \\(e\\)\n\nPlotCode\n\n\n\n\n\n\nWe can compute the value of \\(\\left ( 1 + \\frac{1}{m} \\right)^m\\) as m increases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number that this limit approaches is called \\(e\\).\n\n\\[\nP = \\lim_{ m \\to \\infty}A \\left ( 1 + \\frac{1}{m} \\right)^{m(rt)} =\nA e^{rt}\n\\]\n\n\n\n\n\nf &lt;- function(m) (1 + 1/m)^m\nx &lt;- 0:100\ny &lt;- f(x)\np &lt;- ggplot(data.frame(x, y), aes(x, y))\np + geom_line(size = 0.2) +\n  geom_hline(yintercept = exp(1), color = 'red', size = 0.2)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#properties-of-exponents",
    "href": "stats-math/02-lecture/02-Lecture.html#properties-of-exponents",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Properties of Exponents",
    "text": "Properties of Exponents\n\n\n\\(a^x a^y = a^{x + y}\\)\n\n\n2^7 * 2^8 == 2^(7 + 8) \n\n[1] TRUE\n\n\n\n\n\n\\(\\frac{a^x}{a^y} = a^{x-y}\\)\n\n\n2^7 / 2^8 == 2^(7 - 8) \n\n[1] TRUE\n\n\n\n\n\n\\((a^x)^y = a^{xy}\\)\n\n\n(2^7)^8 == 2^(7*8)\n\n[1] TRUE\n\n\n\n\n\n\\((ab)^x = a^x b^x\\)\n\n\n(2*3)^7 == 2^7 * 3^7\n\n[1] TRUE\n\n\n\n\n\n\\(\\frac{a^x}{b^x} = \\left( \\frac{a}{b} \\right)^x\\)\n\n\n2^4 / 3^4 == (2/3)^4\n\n[1] FALSE\n\nall.equal(2^4 / 3^4, (2/3)^4)\n\n[1] TRUE"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-2",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\nSuppose $750 is invested in an account at an annual interest rate of 5.5%, compounded continuously. Let \\(t\\) denote the number of years after the initial investment and \\(A(t)\\) denote the amount of money in the account at time \\(t\\). Find a formula for \\(A(t)\\). Find the amount of money in the account after 5 years, 10 years, and 50 years."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#logarithmic-functions",
    "href": "stats-math/02-lecture/02-Lecture.html#logarithmic-functions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Logarithmic functions",
    "text": "Logarithmic functions\n\nIntroductionExample: Log-Sum-Exp\n\n\n\n\nExponential function of the form \\(f(x) = a^x\\) is one-to-one, so it has an inverse called a logarithmic function.\nIn Statistics, when we write \\(\\log\\) we mean natural log, base \\(e\\). This makes it easier to interpret the log regression coefficients as percentage changes. For example, \\(\\log(1.04) \\approx 4\\%\\).\nThe following relationship always holds: \\(a^t = e^{\\log(a)t}\\) since \\(e^{\\log(a)} = a\\)\n\\(\\log_a(uv) = \\log_a(u) + \\log_a(v)\\)\n\\(\\log_a(u/v) = \\log_a(u) - \\log_a(v)\\)\n\\(\\log_a u^n = n \\log_a u\\)\n\n\n\n\nIn statistics and Machine Learning, we often want to normalize a vector so that it adds to one. One such function is called a softmax:\n\n\\[\n\\text{softmax}(x)  = \\frac{\\exp(x)} {\\sum_{n=1}^N \\exp(x_n)}\n\\]\n\nIf \\(x\\) is a vector of size 3, the numerator is component-wise \\(\\exp\\) of size 3, the denominator is a scalar, and the function value is a vector of size 3 that adds to 1\n\n\n\n\ny &lt;- 1:3\n(numer &lt;- exp(y))\n\n[1]  2.718282  7.389056 20.085537\n\n(denom &lt;- sum(exp(y)))\n\n[1] 30.19287\n\n(softmax &lt;- numer / denom)\n\n[1] 0.09003057 0.24472847 0.66524096\n\ny &lt;- c(1e3, 1e3 + 1, 1e3 + 2)\nexp(y)\n\n[1] Inf Inf Inf\n\n\n\nThis produces overflow – the numbers are too big for the computer. Let’s compute the \\(\\log \\text{softmax}(x)\\).\n\n\n\n\\[\n    \\log\\text{softmax}(x) = x - \\log \\sum_{n=1}^N \\exp(x_n)\n\\]\n\nThe second term is the log-sum-exp or LSE for short. It has the same problem, as it will overflow or underflow.\nThe idea is that we need to reduce the magnitude of \\(x_n\\) while preserving the integrity of the LSE function.\n\n\n\n\\[\n    \\begin{eqnarray}\n    \\text{Let } c & = & \\max(x_1, x_2, x_3, ..., x_N) \\\\\n    y & = & \\log \\sum_{n=1}^N \\exp(x_n) \\\\\n    \\exp(y) & = & \\sum_{n=1}^N \\exp(x_n) \\\\\n    \\exp(y) & = & \\exp(c) \\sum_{n=1}^N \\exp(x_n - c) \\\\\n    y & = & c + \\log \\sum_{n=1}^N \\exp(x_n - c)\n    \\end{eqnarray}\n\\]\n\nBecause \\(c = \\max(x)\\), the largest exponent is zero.\n\n\n\n\nYour Turn: write a function called LSE that takes in vector \\(x\\) and implements the last equation and tests it on small and large values of \\(x\\). Now implement the softmax_log function using the LSE function. Now compute the same version of the softmax function and check that it sums to 1.\n\n\n\n\nLSE &lt;- function(x) {\n  c &lt;- max(x)\n  y &lt;- c + log(sum(exp(x - c)))\n  return(y)\n}\nsoftmax_log &lt;- function(x) {\n  x - LSE(x)\n}\nsoftmax_unsafe &lt;- function(x) exp(x) / sum(exp(x))\n\nsoftmax_unsafe(1:3)\n\n[1] 0.09003057 0.24472847 0.66524096\n\nsoftmax_log(1:3)\n\n[1] -2.407606 -1.407606 -0.407606\n\nexp(softmax_log(1:3))\n\n[1] 0.09003057 0.24472847 0.66524096\n\ny\n\n[1] 1000 1001 1002\n\nsoftmax_unsafe(y)\n\n[1] NaN NaN NaN\n\nexp(softmax_log(y))\n\n[1] 0.09003057 0.24472847 0.66524096\n\nsum(exp(softmax_log(y)))\n\n[1] 1"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#limits",
    "href": "stats-math/02-lecture/02-Lecture.html#limits",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Limits",
    "text": "Limits\nThe idea of the limit is to evaluate what the function approaches as we increase or decrease the inputs.\n\nThis demo shows what happens to the secant line as it approaches the tangent.\n\n\nSource: Calculus Volume 1"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-motion-in-a-straight-line",
    "href": "stats-math/02-lecture/02-Lecture.html#example-motion-in-a-straight-line",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Motion in a Straight Line",
    "text": "Example: Motion in a Straight Line\nSuppose you have an experiment where you can measure the position of an object moving in a straight line every \\(1/10\\) of a second for 5 seconds. When you look at the graph of time versus position, it looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\nYou guess that the function is quadratic in \\(t\\). If \\(x\\) is measured in meters, and \\(t\\) is in seconds, then \\(a\\) must have units of \\(m\\) and \\(b\\) must have units \\(m/s^2\\).\n\\[\nx(t) = a + bt^2\n\\]\n\n\nThe statistical inference problem is to find plausible values of \\(a\\) and \\(b\\), given our noisy measurements and the assumption that the position function is quadratic. We will come back to how to do it later, but for now, assume that the most likely values were found to be \\(a = 2\\) and \\(b = 3\\).\n\n\n\n\n\n\n\n\n\n\n\nWe can now ask, what was the average velocity between \\(1\\) and \\(4\\) seconds? This is the same as the slope of the secant line:\n\\[\n\\bar{v} =\n\\frac{\\text{displacment} (m)}{\\text{elapsed time} (s)} =\n\\frac{\\Delta x}{\\Delta t} =  \n\\frac{x(4) - x(1)}{4-1} = \\frac{50-5}{3} = 15 \\text{ m/s} = 54 \\text{ km/h}  \n\\]\n\n\nSince we know that this is a line of the form \\(x(t) = a + 15t\\), that goes through the point \\((t_1, x_1) = (1, 5)\\), \\(5 = a + 15\\cdot1\\) or \\(a = -10\\). The equation of the secant line is, therefore:\n\\[\nx(t) = -10 + 15t\n\\]\n\n\n\np &lt;- ggplot(data.frame(t, x), aes(t, x))\np + geom_line(size = 0.2) +\n  geom_abline(slope = 15, intercept = -10, size = 0.2, color = 'red') +\n  geom_point(x = 1, y = 5, color = 'blue') +\n  geom_point(x = 4, y = 50, color = 'blue') +\n  xlab(\"time t (s)\") + ylab(\"position x (m)\") +\n  annotate(\"text\", 3.7, 55, label = \"(4, 50)\") +\n  annotate(\"text\", 1, 12, label = \"(1, 5)\") +\n  ggtitle(\"object's position function and the secant line\")\n\n\n\n\n\n\n\n\n\n\nWhat if we wanted to know the speedometer reading at \\(4\\) seconds? In other words, we want to know the speed at that instant. This is where the limit comes in.\n\\[\nv = \\lim_{\\Delta t \\to 0} \\frac{\\Delta x}{\\Delta t} = \\frac{dx}{dt}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\n\nHow do we compute this limit, which we wrote as \\(dx/dt\\)?\n\n\n\n\n\n\n\n\n\nWe re-write the secant or average velocity equation in the following way:\n\n\\[\n\\begin{eqnarray}\n\\bar v & = & \\frac{x(a + h) - x(a)}{a + h - a} = \\frac{x(a + h) - x(a)}{h} \\\\\nv & = & \\lim_{\\Delta h \\to 0} \\frac{x(a + h) - x(a)}{h} \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative-1",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\n\nOur original problem was to find the velocity at \\(t = 4\\) given our position function \\(x(t) = 2 + 3t^2\\).\n\n\\[\n\\begin{eqnarray}\nv & = & \\lim_{\\Delta h \\to 0} \\frac{x(a + h) - x(a)}{h} =\n\\lim_{\\Delta h \\to 0} \\frac{2 + 3(t + h)^2 - 2 - 3t^2}{h} = \\\\\n& \\lim_{\\Delta h \\to 0} & \\frac{6ht + 3h^2}{h} = \\lim_{\\Delta h \\to 0} (6t + 3h) = 6t\n\\end{eqnarray}\n\\]\n\n\n\nAt \\(t = 4\\), the speedometer was reading \\(6 \\cdot 4 = 24 \\text{ m/s } = 86.4 \\text{ km/h}\\).\nThe velocity is the derivative of position with respect to time, and we can write:\n\n\\[\nv(t) = \\frac{dx}{dt} = 6t\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#the-derivative-2",
    "href": "stats-math/02-lecture/02-Lecture.html#the-derivative-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "The Derivative",
    "text": "The Derivative\n\nBut what about acceleration?\n\n\n\nAcceleration is a change in velocity with respect to time or the second derivative of position:\n\n\\[\na(t) = \\frac{dv}{dt} = \\frac{d}{dt} \\left( \\frac{d x}{d t} \\right) = \\frac{d^2 x}{d t^2}\n\\]\n\n\n\nTo compute acceleration at \\(t = 4\\), we take the limit again, only this time with respect to the velocity function:\n\n\\[\na = \\frac{dv}{dt} = \\lim_{\\Delta h \\to 0} \\frac{6(t + h) - 6t}{h} = \\\\\n\\lim_{\\Delta h \\to 0} \\frac{6t + 6h - 6t}{h} = \\lim_{\\Delta h \\to 0} 6 = 6\n\\]\n\n\n\nThis function \\(a(t) = 6\\) does not depend on \\(t\\), and so acceleration is a constant \\(6 \\text{ m}/\\text{s}^2\\) at all values of \\(t\\) including \\(t = 4\\)\n\n\n\n\nIt would be cumbersome to compute derivatives this way. Fortunately, we have shortcuts."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#rules-of-differentiation",
    "href": "stats-math/02-lecture/02-Lecture.html#rules-of-differentiation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Rules of Differentiation",
    "text": "Rules of Differentiation\n\nWe will state a few rules of differentiation without deriving them\n\n\\[\n\\begin{eqnarray}\n\\frac{d}{dx}(c) & = & 0 \\\\\n\\frac{d}{dx}(x) & = & 1 \\\\\n\\frac{d}{dx}(x^n) & = & n x^{n-1} \\\\\n\\frac{d}{dx}[c f(x)] & = & c \\frac{d}{dx}[f(x)] \\\\\n\\frac{d}{dx}[f(x) + g(x)] & = & \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x) \\\\\n\\frac{d}{dx}(e^x) & = & e^x \\\\                                                            \\end{eqnarray}                                                               \n\\]\n\n\nIf you are interested in why the last statement is true, see this.\nThe derivatives of trigonometric functions can be found here."
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#your-turn-3",
    "href": "stats-math/02-lecture/02-Lecture.html#your-turn-3",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\n\nSuppose we have the following equations of motion:\n\n\\[\nx(t) = 4 + 3t + 5t^2\n\\]\n\nWhat is the object’s position at time zero and time 1 second?\nWhat is the velocity at time zero?\nWhat is the velocity and acceleration at 5 seconds?"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#product-rule-and-chain-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#product-rule-and-chain-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Product Rule and Chain Rule",
    "text": "Product Rule and Chain Rule\n\nProduct and chain rules help us differentiate products and compositions of functions, respectively.\nIntuition for those can be found in the Essense of Calculus video.\nWe will state them here without derivation. Product rule: Right d left, Left d right.\n\n\\[\n\\frac{d}{dx}[f(x) g(x)] = f(x) \\frac{d}{dx}[g(x)] + g(x)\\frac{d}{dx}[f(x)]\n\\]\n\nIf \\(F = f \\circ g\\), in that \\(F(x)= f(g(x))\\):\n\n\\[\nF'(x) = f'(g(x))g'(x)\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#example-of-a-product-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#example-of-a-product-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example of a Product Rule",
    "text": "Example of a Product Rule\n\nSuppose we wanted to compute the derivative of \\(x(t) = \\sin(t) \\cos(t)\\)\n\n\\[\n\\begin{eqnarray}\n\\frac{d}{dt}(\\sin(t) \\cos(t)) & = & \\\\\n\\sin(t)\\frac{d}{dt}[\\cos(t)] + \\cos(t)\\frac{d}{dt}[\\sin(t)] & = & \\\\\n\\sin(t) (-\\sin(t)) + \\cos(t)\\cos(t) & = & \\\\\n\\cos^2(t) - \\sin^2(t)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#approximating-derivatives",
    "href": "stats-math/02-lecture/02-Lecture.html#approximating-derivatives",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Approximating Derivatives",
    "text": "Approximating Derivatives\n\nlibrary(ggplot2)\nlibrary(dplyr)\nn &lt;- 100\nt &lt;- seq(0, pi, length = n)\nx &lt;- sin(t) * cos(t)\ndxdt &lt;- cos(t)^2 - sin(t)^2\n\nd &lt;- tibble(t, x, dxdt)\np &lt;- ggplot(d, aes(t, x))\np + geom_line() + geom_line(aes(y = dxdt), col = 'red')\n\n\n\n\n\n\n\n# approximate the derivative\ns &lt;- pi / (n - 1)        # choose the same step s as the increment in the t sequence\nall.equal(s, diff(t)[1]) # check that the above statement is true\n\n[1] TRUE\n\nappr_dxdt &lt;- diff(x)/s   # derivative ≈ rise / run\nd &lt;- d |&gt;\n  mutate(appr_dxdt = c(NA, appr_dxdt))\np &lt;- ggplot(d, aes(t, x))\np + geom_line() + geom_line(aes(y = dxdt), col = 'red') +\n  geom_line(aes(y = appr_dxdt), col = 'blue', linewidth = 5, alpha = 1/5)"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#examples-of-the-chain-rule",
    "href": "stats-math/02-lecture/02-Lecture.html#examples-of-the-chain-rule",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Examples of the Chain Rule",
    "text": "Examples of the Chain Rule\n\nWhat is the derivative of \\(\\sin(x^2)\\)? It is \\(2x \\cos(x^2)\\)\nYour turn: \\[\n\\frac{d}{dx} \\left( e^{\\cos(x) x^2} \\right ) =\n\\]"
  },
  {
    "objectID": "stats-math/02-lecture/02-Lecture.html#homework",
    "href": "stats-math/02-lecture/02-Lecture.html#homework",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Homework",
    "text": "Homework\n\nA particle is moving in a counter-clockwise uniform circular motion according to the following equations:\n\n\\[\nx(t) = R \\cos(\\omega t) \\\\\ny(t) = R \\sin(\\omega t)\n\\]\n\nWhere \\(\\omega\\) is called angular frequency (radians per unit of time) and \\(R\\) is the radius of rotation. The full cycle is achieved when \\(T = 2 \\pi/ \\omega\\)\nCreate two functions that take \\(R\\), \\(t\\), and \\(\\omega\\) as parameters\nCreate a vector \\(t\\) of length 100 that achieves one full rotation\nPlot the \\(x\\) position against \\(t\\) and the \\(y\\) position against \\(t\\) on the same graph\nNow plot \\(x\\) against \\(y\\). Do the plots make sense?\nCreate 4 more functions: 2 for velocity (in x and y direction) and 2 for acceleration\nPlot the velocity against the position (say in the x direction) and the velocity against acceleration\nWhat have you learned?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#session-4-outline",
    "href": "stats-math/04-lecture/04-lecture.html#session-4-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 4 Outline",
    "text": "Session 4 Outline\n\n\nWhat is probability?\nHow does it relate to statistics?\nSample spaces and arithmetic of sets\nHow to count without counting\nConditional probability\nIndependence and Bayes rule\nExamples\n\nCOVID testing\nBirthday problem\nLeibniz’s error\nMonte Hall"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#what-is-probability",
    "href": "stats-math/04-lecture/04-lecture.html#what-is-probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "What is Probability",
    "text": "What is Probability\n\n\n\n\nStatistics is the art of quantifying uncertainty, and probability is the language of statistics\nProbability is a mathematical object\nPeople argue over the interpretation of probability\nPeople don’t argue about the mathematical definition of probability\n\n\n\n\n\n\n\n\nAndrei Kolmogorov (1903 — 1987)\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#laplaces-demon",
    "href": "stats-math/04-lecture/04-lecture.html#laplaces-demon",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world”"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#sample-spaces",
    "href": "stats-math/04-lecture/04-lecture.html#sample-spaces",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Sample Spaces",
    "text": "Sample Spaces\n\n\nA sample space which we will call \\(S\\) is a set of all possible outcomes \\(s\\) of an experiment.\nIf we flip a coin twice, our sample space has \\(4\\) elements: \\(\\{TT, TH, HT, HH\\}\\).\nAn event \\(A\\) is a subset of a sample space. We say that \\(A \\subseteq S\\)\nPossible events: 1) At least one \\(T\\); 2) The first one is \\(H\\); 3) Both are \\(T\\); 4) Both are neither \\(H\\) nor \\(T\\); etc"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#sample-spaces-and-de-morgan",
    "href": "stats-math/04-lecture/04-lecture.html#sample-spaces-and-de-morgan",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Sample Spaces and De Morgan",
    "text": "Sample Spaces and De Morgan\n\n\nA compliment of an event \\(A\\) is \\(A^c\\). Formally, this is \\(A^c = \\{s \\in S: s \\notin A\\}\\)\nSuppose, \\(S = \\{1, 2, 3, 4\\}\\), \\(A = \\{1, 2\\}\\), \\(B = \\{2, 3\\}\\), and \\(C = \\{3, 4\\}\\)\nThen, the union of A and B is \\(A \\cup B = \\{1, 2, 3\\}\\), \\(A \\cap B = \\{2\\}\\), \\(A^c = \\{3, 4\\}\\), and \\(B^c = \\{1, 4\\}\\). And \\(A \\cap C = \\{\\emptyset \\}\\)\nDe Morgan (1806 — 1871) said that:\n\n\\((A \\cup B)^c = A^c \\cap B^c\\) and\n\\((A \\cap B)^c = A^c \\cup B^c\\)\n\nIn our case, \\((A \\cup B)^c = (\\{1, 2, 3\\})^c = \\{4\\} = (\\{3, 4\\} \\cap \\{1, 4\\})\\)\nYour Turn: Compute \\((A \\cap B)^c = A^c \\cup B^c\\)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#naive-definition",
    "href": "stats-math/04-lecture/04-lecture.html#naive-definition",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Naive Definition",
    "text": "Naive Definition\n\n\nNaive definition of probability is an assumption that every outcome is equally likely\nIn that case, \\(\\P(A) = \\frac{\\text{number of times A occurs}}{\\text{number of total outcomes}}\\)\nWhat is the probability of rolling an even number on a six-sided die?\n\n\\(\\P(\\text{Even Number}) = \\frac{3}{6} = \\frac{1}{2}\\)\n\nWhat is the probability of rolling a prime number?\n\n\\(\\P(\\text{Prime Number}) =\\)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#counting",
    "href": "stats-math/04-lecture/04-lecture.html#counting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Counting",
    "text": "Counting\n\n\nYour mom feels probabilistic, so she asks you to flip a coin. If it lands heads, you get pizza for dinner, and if lands tails, you get broccoli. You flip again for dessert. If it lands heads, you get ice cream, and if it lands tails, you get a cricket cookie. Assume you are not a fan of veggies and insects. What is the probability of having a completely disappointing meal? What is the probability of being partly disappointed?\nFrom Blitzstein and Chen (2015): Suppose that 10 people are running a race. Assume that ties are not possible and that all 10 will complete the race, so there will be well-defined first place, second place, and third place winners. How many possibilities are there for the first, second, and third place winners?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#basic-counting",
    "href": "stats-math/04-lecture/04-lecture.html#basic-counting",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Basic Counting",
    "text": "Basic Counting\n\n\nHow many permutations of \\(n\\) object are there?\nSampling with replacement (order matters): you select one object from \\(n\\) total objects, \\(k\\) times (and put it back each time).\nSampling without replacement (order matters): same as above, but you don’t put them back"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#binomial-coefficient",
    "href": "stats-math/04-lecture/04-lecture.html#binomial-coefficient",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial coefficient",
    "text": "Binomial coefficient\n\n\nHow many ways are there to choose a number of subsets from a set; e.g., how many two-person teams can be formed from five people? (here, John and Jane is the same set as Jane and John)\n\n\n\n\\[\n{n \\choose k} = \\frac{n!}{(n - k)! k!} = {n \\choose n-k}\n\\]\n\n\n\nWhat is the probability of the full house in poker? (3 cards of the same rank and 2 cards of the other rank)\nHow many unique shuffles are there in one deck of 52 cards?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#birthday-problem",
    "href": "stats-math/04-lecture/04-lecture.html#birthday-problem",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Birthday Problem",
    "text": "Birthday Problem\n\nProblem DescriptionSimulationPlot\n\n\nThere are \\(j\\) people in a room. Assume each person’s birthday is equally likely to be any of the 365 days of the year (excluding February 29) and that people’s birthdays are independent. What is the probability that at least one pair of group members has the same birthday?\n\n\n\nn &lt;- 1e5\nn_people &lt;- 80 \nj &lt;- 2:n_people\ndays &lt;- 1:365 \n\nfind_match &lt;- function(x, d) {\n  y &lt;- sample(d, x, replace = TRUE)\n  match_found &lt;- length(unique(y)) &lt; length(y)\n}\n\nprop &lt;- numeric(n_people - 1)\nfor (i in seq_along(j)) {\n  matches &lt;- replicate(n, find_match(i, days))\n  prop[i] &lt;- mean(matches)\n}"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#birthday-analysis",
    "href": "stats-math/04-lecture/04-lecture.html#birthday-analysis",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Birthday Analysis",
    "text": "Birthday Analysis\n\n\nLet’s say we have \\(j\\) people. There are 365 ways in which the first person can get a birthday, 365 for the second, and so on, assuming no twins. So the denominator is \\(365^j\\)\nFor the numerator, it is easier to compute a probability of no match. Let’s say we have three people in the room. The numerator would be \\(365 \\cdot 364 \\cdot 363\\), the last term being \\(365 - j + 1\\)\n\n\n\n\\[\n\\P(\\text{At least one match}) = 1 - P(\\text{No match}) = \\\\\n1 - \\frac{\\prod_{i = 0}^{j-1} (365-i)}{365^j} = 1 -\n\\frac{365 \\cdot 364 \\cdot 363 \\cdot \\,... \\, \\cdot (365 - j + 1)}{365^j}\n\\]\n\n\n\n(Number of People, Probability of a match):\n\n\n\n\n\n(2, 0); (3, 0.01); (4, 0.02); (5, 0.03); (6, 0.04); (7, 0.06); (8, 0.07); (9, 0.09); (10, 0.12); (11, 0.14); (12, 0.17); (13, 0.19); (14, 0.22); (15, 0.25); (16, 0.28); (17, 0.32); (18, 0.35); (19, 0.38); (20, 0.41); (21, 0.44); (22, 0.48); (23, 0.51)"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#probability",
    "href": "stats-math/04-lecture/04-lecture.html#probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Probability",
    "text": "Probability\n\nProbability \\(\\P\\) assigns a real number to each event \\(A\\) and satisfies the following axioms:\n\n\n\\[\n\\begin{eqnarray}\n\\P(A) & \\geq & 0 \\text{ for all } A \\\\\n\\P(S) & = & 1 \\\\\n\\text{If } \\bigcap_{i=1}^{\\infty}A_i = \\{\\emptyset \\} & \\implies &\n\\P\\left( \\bigcup_{i=1}^{\\infty}A_i  \\right) = \\sum_{i=1}^{\\infty}\\P(A_i)\n\\end{eqnarray}\n\\]\n\n\n\nFor \\(S = \\{1, 2, 3, 4\\}\\), where \\(s_i = 1/4\\), and \\(A = \\{A_1, A_2\\}\\), where \\(A_1 = \\{1\\}\\) and \\(A_2 = \\{2\\}\\). Verify that all 3 axioms hold."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#some-consequences",
    "href": "stats-math/04-lecture/04-lecture.html#some-consequences",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Consequences",
    "text": "Some Consequences\n\n\\[\n\\begin{eqnarray}\n\\P\\{\\emptyset\\} & = & 0 \\\\\nA \\subseteq B  & \\implies & \\P(A) \\leq \\P(B) \\\\\n\\P(A^c) & = & 1 - P(A) \\\\\nA \\cap B = {\\emptyset} & \\implies & \\P(A \\cup B) = \\P(A) + \\P(B) \\\\\n\\P(A \\cup B) & = & \\P(A) + \\P(B) - \\P(A \\cap B)\n\\end{eqnarray}\n\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n\\]\n\n\n\nThe last one is called inclusion-exclusion\nTwo events are independent \\(A \\indep B\\) if:\n\n\n\n\\[\n\\P(A \\cap B) = \\P(A) \\P(B)\n\\]"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#some-examples",
    "href": "stats-math/04-lecture/04-lecture.html#some-examples",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Examples",
    "text": "Some Examples\n\nSimulationAnalysisYour Turn\n\n\nYou flip a fair coin four times. What is the probability that you get one or more heads?\n\n\none_or_more &lt;- function() {\n  flips &lt;- sample(c(1, 0), size = 4, replace = TRUE)\n  if (sum(flips) &gt;= 1) {\n    return(TRUE)\n  } else {\n    return(FALSE)\n  }\n}\n# flip the coin 10,000 times\nx &lt;- replicate(1e4, one_or_more())\nmean(x)\n\n[1] 0.9366\n\n\n\n\n\nLet \\(A\\) be the probability of at least one Head. Then \\(A^c\\) is the probability of all Tails. Let \\(B_i\\) be the event of Tails on the \\(i\\)th trial.\n\n\\[\n\\begin{eqnarray}\n\\P(A)  = 1 - \\P(A^c) & = & \\\\\n1 - \\P(B_1 \\cap B_2 \\cap B_3 \\cap B_4) & = & \\\\\n1 - \\P(B_1)\\P(B_2)\\P(B_3)\\P(B_4) & = & \\\\\n1 - \\left( \\frac{1}{2} \\right)^4 =  1 - \\frac{1}{16} = 0.9375\n\\end{eqnarray}\n\\]\n\n\n\n\nModify the function so that instead of 1 or more from 4 trials, it computes 1 or more from \\(n\\) trials.\nModify it so it computes \\(x\\) or more from \\(n\\) trials and simulate at least 2 Heads out of 5 trials.\nCan you solve it analytically to validate your simulation results?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#conditional-probability",
    "href": "stats-math/04-lecture/04-lecture.html#conditional-probability",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditioning is the soul of statistics — Joe Blitzstein\n\n\n\nThink of conditioning as the probability in reduced sample spaces since when we condition, we are looking at the subset of \\(S\\) where some events already occurred.\nNote: In some texts, \\(AB\\) is used as a shortcut for \\(A \\cap B\\). You can’t multiply events, but you can multiply their probabilities.\n\n\n\n\\[\n\\P(A | B) = \\frac{\\P(A \\cap B)}{\\P(B)}\n\\]\n\n\n\nNote that \\(\\P(A|B)\\) and \\(\\P(B|A)\\) are different things. Doctors and lawyers confuse those all the time.\nTwo events are independent iff \\(\\P(A | B) = \\P(A)\\). In other words, learning \\(B\\) does not improve our estimate of \\(\\P(A)\\)."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#law-of-total-probability-and-bayes",
    "href": "stats-math/04-lecture/04-lecture.html#law-of-total-probability-and-bayes",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Law of Total Probability and Bayes",
    "text": "Law of Total Probability and Bayes\n\nLOTPBayes\n\n\nIn the following, A partitions the entire sample space S. \\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B | A_i) \\P(A_i)\n\\]\n\n\n\n\n\n\n\n\n\nWe take the definition of conditional probability and expand the numerator and denominator:\n\n\n\n\\[\n\\P(A|B) = \\frac{\\P(B \\cap A)}{\\P(B)} = \\frac{\\P(B|A) \\P(A)}{\\sum_{i=1}^{n} \\P(B | A_i) \\P(A_i)}\n\\]\n\n\n\nWe call \\(\\P(A)\\), prior probability of \\(A\\) and \\(\\P(A|B)\\) a posterior probability of \\(A\\) after we learned \\(B\\).\n\n\n\n\n\n\nImage Source: Introduction to Probability, Blitzstein et al."
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-medical-testing",
    "href": "stats-math/04-lecture/04-lecture.html#example-medical-testing",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nDerivationDiscussion\n\n\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(P(D^+ | T^+)\\)\n\\(\\text{Specificity } := P(T^- | D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - P(T^- | D^-) = P(T^+ | D^-) = 0.002\\)\n\\(\\text{Sensitivity } := P(T^+ | D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - P(T^+ | D^+) = P(T^- | D^+) = 0.546\\)\nPrevalence: \\(P(D^+) = 0.001\\)\n\n\\[\n\\begin{eqnarray}\nP(D^+ | T^+) = \\frac{P(T^+ | D^+) P(D^+)}{P(T^+)} & = & \\\\\n\\frac{P(T^+ | D^+) P(D^+)}{\\sum_{i=1}^{n}P(T^+ | D^i) P(D^i) } & = & \\\\\n\\frac{P(T^+ | D^+) P(D^+)}{P(T^+ | D^+) P(D^+) + P(T^+ | D^-) P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]\n\n\n\nThe answer, 18%, is very sensitive to the prevalence of disease, or in our language, to the prior probability of an infection\nAt the time of the test, the actual prevalence was estimated at 4.8%, not 0.1%, which would change our answer by a lot: \\(\\P(D^+ | T^+) \\approx 0.92\\)\nLesson: don’t rely on intuition and check the prior"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-leibnizs-error",
    "href": "stats-math/04-lecture/04-lecture.html#example-leibnizs-error",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Leibniz’s Error",
    "text": "Example: Leibniz’s Error\n\nYou have two 6-sided fair dice. You roll the dice and compute the sum. Which one is more likely 11 or 12?"
  },
  {
    "objectID": "stats-math/04-lecture/04-lecture.html#example-monte-hall",
    "href": "stats-math/04-lecture/04-lecture.html#example-monte-hall",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Example: Monte Hall",
    "text": "Example: Monte Hall"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#session-5-outline",
    "href": "stats-math/05-lecture/05-partial.html#session-5-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 5 Outline",
    "text": "Session 5 Outline\n\n\nRandom variables\nBernoulli, Binomial, Geometric\nPDF and CDF\nLOTUS\nPoisson\nExpectations, Variance\nSt. Petersburg Paradox\nNormal\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#random-variables-are-not-random",
    "href": "stats-math/05-lecture/05-partial.html#random-variables-are-not-random",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Random Variables are Not Random",
    "text": "Random Variables are Not Random\n\nStoryPicture\n\n\n\n\nIt would be inconvenient to enumerate all possible events to describe a stochastic system\nA more general approach is to introduce a function that maps sample space \\(S\\) onto the Real line\nFor each possible outcome \\(s\\), random variable \\(X(s)\\) performs this mapping\nThis mapping is deterministic. The randomness comes from the experiment, not from the random variable (RV)\nWhile it makes sense to talk about \\(\\P(A)\\), where \\(A\\) is an event, it does not make sense to talk about \\(\\P(X)\\), but you can say \\(\\P(X(s) = x)\\), which we usually write as \\(\\P(X = x)\\)\nLet \\(X\\) be the number of Heads in two coin flips. You flip the coin twice, and you get \\(HH\\). In this case, \\(s = {HH}\\), \\(X(s) = 2\\), while \\(S = \\{TT, TH, HT, HH\\}\\)\n\n\n\n\n\nRandom variable \\(X\\) for the number of Heads in two flips"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#characterising-random-variables",
    "href": "stats-math/05-lecture/05-partial.html#characterising-random-variables",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Characterising Random Variables",
    "text": "Characterising Random Variables\n\nIntroductionR Conventions\n\n\n\n\nTwo ways of describing an RV are CDF (Cumulative Distribution Function) and PMF (Probability Mass Function) for discrete RVs and PDF (Probability Density Function) for continuous RVs. There are other ways, but we will stick with CDF and P[D/M]F.\nCDF \\(F_X(x)\\) is a function of \\(x\\) and is bounded between 0 and 1:\n\n\n\n\\[\nF_X(x) = \\P(X \\leq x)\n\\]\n\n\n\nPMF (for discrete RVs only) \\(f_X(x)\\) is a function of \\(x\\)\n\n\\[\nf_X(x) = \\P(X = x)\n\\]\n\n\n\nYou can get from \\(f_X\\) to \\(F_X\\) by summing. Let’s say \\(x = 4\\). In that case:\n\n\\[\nF_X(4) = \\P(X \\leq 4) = \\sum_{i = 4,3,2,...}\\P(X = i)\n\\]\n\n\n\n\n\nIn R, PMFs and PDFs start with the letter d. For example dbinom() and dnormal() refer to binomial PMF and normal PDF\nCDFs start with p, so pbinom() and pnorm()\nInverse CDFs or quantile functions, start with q so qbinom() and so on\nRandom number generators start with r, so rbinom()\nA binomial RV, which we will define later, represents the number of successes in N trials. In R, the PMF is dbinom() and CDF is pbinom()\nHere is the full function signature: dbinom(x, size, prob, log = FALSE)\n\nx is the number of successes, size is the number of trials N, prob is the probability of success in each trial \\(\\theta\\), and log is a flag asking if we want the results on the log scale."
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#binomial-rv",
    "href": "stats-math/05-lecture/05-partial.html#binomial-rv",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial RV",
    "text": "Binomial RV\n\nBinomial PMFPMF and CDF PlotsCode to Generate the Plots\n\n\n\n\nBernoulli RV is one coin flip with a set probability of success (say Heads)\nIf \\(X \\sim \\text{Bernoulli}(\\theta)\\), the PMF can be written directly as \\(\\P(X = x) = \\theta^x (1 - \\theta)^{1-x}, \\, x \\in \\{0, 1\\}\\)\nBinomial can be thought of as the sum of \\(N\\) independent Bernoulli trials. We can also write:\n\n\n\n\\[\n\\text{Bernoulli}(x~|~\\theta) = \\left\\{ \\begin{array}{ll} \\theta &\n\\text{if } x = 1, \\text{ and} \\\\ 1 - \\theta & \\text{if } x = 0\n\\end{array} \\right.\n\\]\n\n\n\nWe can write the Binomial PMF, \\(X \\sim \\text{Binomial}(N, \\theta)\\) this way:\n\n\\[\n\\text{Binomial}(x~|~N,\\theta) = \\binom{N}{x}\n\\theta^x (1 - \\theta)^{N - x}\n\\]\n\n\n\n\\(\\text{Binomial}(x~|~N=4,\\theta = 1/2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(patchwork)\nlibrary(MASS)\n\nN &lt;- 4 # Number of successes out of x trials\n\n# compute and plot the PMF\npmf &lt;- dbinom(x = 0:N, size = N, prob = 1/2)\nd &lt;- data.frame(x =  0:N, y = pmf)\np1 &lt;- ggplot(d, aes(x, pmf))\np1 &lt;- p1 + geom_col(width = .2) + \n  geom_text(aes(label = fractions(pmf)), nudge_y = 0.02) +\n  ylab(\"P(X = x)\") + xlab(\"x = Number of Heads\") +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(PDF: p[X](x) == P(X == x)))\n\n# compute and plot the CDF\nx &lt;- seq(-0.5, 4.5, length = 500)\ncdf &lt;- pbinom(q = x, size = N, prob = 1/2)\nd &lt;- data.frame(q = x, y = cdf)\ndd &lt;- data.frame(x = seq(-0.5, 4.5, by = 1), cdf = unique(cdf), x_empty = 0:5)\np2 &lt;- ggplot(d, aes(x, cdf)) \np2 &lt;- p2 + geom_point(size = 0.2) + \n  geom_text(aes(x, cdf, label = fractions(cdf)), data = dd, nudge_y = 0.05) +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, color = 'white') +\n  geom_point(aes(x_empty, cdf), data = dd[-6, ], size = 2, shape = 1) +\n  ggtitle(\"X ~ Binomial(4, 1/2)\",\n          subtitle = expression(CDF: F[X](x) == P(X &lt;= x))) +\n  ylab(expression(P(X &lt;= x))) + xlab(\"x = Number of Heads\")\n\np1 + p2"
  },
  {
    "objectID": "stats-math/05-lecture/05-partial.html#binomial-in-r",
    "href": "stats-math/05-lecture/05-partial.html#binomial-in-r",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Binomial in R",
    "text": "Binomial in R\n\n# What is the probability of getting 2 Heads out of 5 fair trials?\nN &lt;- 5; x &lt;- 2\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 5/16\n\n# What is the binomial PMF: P(X = x), for N = 5, p = 0.5?\nN &lt;- 5; x &lt;- -2:7 # notice we range x over any integers\ndbinom(x = x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]    0    0 1/32 5/32 5/16 5/16 5/32 1/32    0    0\n\n# Verify that the PMF sums to 1\nsum(dbinom(x = x, size = N, prob = 0.5))\n\n[1] 1\n\n# What is the probability of 3 heads or fewer\npbinom(3, size = N, prob = 0.5) |&gt; fractions()\n\n[1] 13/16\n\n# compute the CDF: P(X &lt;= x), for N = 5, p = 0.5\npbinom(x, size = N, prob = 0.5) |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n# get from the PMF to CDF; cumsum() is the cumulative sum function\ndbinom(x = x, size = N, prob = 0.5) |&gt; cumsum() |&gt; fractions()\n\n [1]     0     0  1/32  3/16   1/2 13/16 31/32     1     1     1\n\n\n\n\nYour Turn: Suppose the probability of success is 1/3, N = 10. What is the probability of 6 or more successes? Compute it with a PMF first and verify with the CDF."
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#session-7-outline",
    "href": "stats-math/07-lecture/07-lecture.html#session-7-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 7 Outline",
    "text": "Session 7 Outline\n\n\nStatistical inference\nSampling distribution\nStandard errors\nConfidence intervals\nDegrees of freedom and t distribution\nBias and uncertainty\nStatistical significance\n\n\n\nThe material for this session is based on Chapter 4 of the Regression and Other Stories by Gelman et al.\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#introduction-to-statistical-inference",
    "href": "stats-math/07-lecture/07-lecture.html#introduction-to-statistical-inference",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Introduction to Statistical Inference",
    "text": "Introduction to Statistical Inference\n\n\n\nStatistical Inference: A process of learning from noisy measurements\nKey Challenges:\n\nGeneralizing from a sample to the population of interest\nLearning what would have happened under different treatment\nUnderstanding the relationship between the measurement and the estimand"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#measurement-error-models",
    "href": "stats-math/07-lecture/07-lecture.html#measurement-error-models",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Measurement Error Models",
    "text": "Measurement Error Models\n\n\nWe are trying to estimate the parameters of some data-generating process\nFor our car example, we assumed that the car position data are coming from the following model:\n\n\n\\[\nx_i(t) = a + bt_i^2 + \\epsilon_i\n\\]\n\n\nWe also assumed that time \\(t\\) was observed precisely, but we can also have an error in \\(t\\)\nErrors may be multiplicative, not just additive"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#sampling-distribution",
    "href": "stats-math/07-lecture/07-lecture.html#sampling-distribution",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\n\nGenerative model for our data: given the generative model, we can create replicas of the dataset\nSampling distribution is typically unknown and depends on the data collection, how subjects are assigned to treatment conditions, sampling process\nExamples\n\nSimple random sample of size \\(n\\) from the population of size \\(N\\). Here, each person has the same probability of being selected\nOur constant acceleration model \\(x_i(t) = a + bt_i^2 + \\epsilon_i\\), where we fix \\(a\\) and \\(b\\), and \\(t\\), and draw \\(\\epsilon\\) from an error distribition, such as \\(\\text{Normal}(0, 1)\\)\nWe can write some code to generate observations according to this process (which is what I did for the car example)"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#standard-errors",
    "href": "stats-math/07-lecture/07-lecture.html#standard-errors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Standard Errors",
    "text": "Standard Errors\n\n\nStandard error is the estimated standard deviation of the estimate\nIt provides a measure of uncertainty around the estimate\nStandard error decreases as the sample size increases\nTo estimate the SE of the mean from a large population with sd = \\(\\sigma\\): \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\nset.seed(1)\nn &lt;- 100; mu &lt;- 5; sigma &lt;- 1.5\nx &lt;- rnorm(n, mean = mu, sd = sigma)\nx_bar &lt;- mean(x); round(x_bar, 2)\n\n[1] 5.16\n\nse &lt;- sigma/sqrt(n)\nprint(se)\n\n[1] 0.15\n\n\n\n\nIf \\(\\sigma\\) is unknown, we can estimate it from the sample: \\(s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\)\n\n\n\n\nsd(x) |&gt; round(2)\n\n[1] 1.35\n\nsd_s &lt;- sqrt(sum((x - x_bar)^2) / (n - 1))\nprint(sd_s |&gt; round(2))\n\n[1] 1.35\n\nse_s &lt;- sd_s/sqrt(n)\nprint(se_s |&gt; round(2))\n\n[1] 0.13"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#confidence-intervals",
    "href": "stats-math/07-lecture/07-lecture.html#confidence-intervals",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nFor a given sampling distribution, confidence interval provides a range of parameter values consistent with the data\nFor example, for a normal sampling distribution, an estimate \\(\\hat{\\theta}\\) will fall in \\(\\hat{\\theta} \\pm 2\\cdot\\text{se}\\) about 95% of the time"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#confidence-intervals-for-proportions",
    "href": "stats-math/07-lecture/07-lecture.html#confidence-intervals-for-proportions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\n\n\n\nIn surveys, we are often interested in estimating the standard error of the proportion\nSuppose we want to estimate the proportion of the US population that supports same-sex marriage\nSay you randomly survey \\(n = 500\\) from the population, and \\(y = 355\\) people respond yes1\nThe estimate of the proportion is \\(\\hat{\\theta} = y/n\\) with the \\(\\text{se} = \\sqrt{\\hat{\\theta}(1 -\\hat{\\theta})/n }\\)\n\n\n\n\n\nn &lt;- 500\ny &lt;- 355\ntheta_hat &lt;- y/n\ntheta_hat |&gt; round(2)\n\n[1] 0.71\n\nse &lt;- sqrt(theta_hat * (1 - theta_hat) / n)\nse |&gt; round(2)\n\n[1] 0.02\n\ntheta_hat + 2*se |&gt; round(2)\n\n[1] 0.75\n\ntheta_hat - 2*se |&gt; round(2)\n\n[1] 0.67\n\nci_95 &lt;- theta_hat + qnorm(c(0.025, 0.975)) * se\nci_95 |&gt; round(2)\n\n[1] 0.67 0.75\n\n\n\n\nThe most recent Gallup poll from May 2023 found that 71% of Americans think same-sex marriage should be legally recognized"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#your-turn",
    "href": "stats-math/07-lecture/07-lecture.html#your-turn",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Your Turn",
    "text": "Your Turn\nIn a national survey of \\(n\\) people, how large does \\(n\\) have to be so that you can estimate presidential approval to within a standard error of ±3 percentage points, ±1 percentage points?"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#standard-error-for-differences",
    "href": "stats-math/07-lecture/07-lecture.html#standard-error-for-differences",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Standard Error for Differences",
    "text": "Standard Error for Differences\n\n\nWe already saw an example of how to add variances from two independent RVs\n\n\n\\[\n\\text{se}_{\\text{diff}} = \\sqrt{\\text{se}_1^2 + \\text{se}_2^2}\n\\]\n\n\nYour turn: How large does \\(n\\) have to be so that you can estimate the gender gap in approval to within a standard error of ±3 percentage points?"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#computer-demo-computing-cis",
    "href": "stats-math/07-lecture/07-lecture.html#computer-demo-computing-cis",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Computer Demo: Computing CIs",
    "text": "Computer Demo: Computing CIs\n\nThese demos are taken from the book Active Statistics by Gelman and Vehtari\n\n\n# Generate fake data\np &lt;- 0.3\nn &lt;- 20\ndata &lt;- rbinom(1, n, p)\nprint(data)\n\n# Estimate proportion and calculate confidence interval\np_hat &lt;- data / n\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\nci &lt;- p_hat + c(-2, 2) * se\nprint(ci) \n\n# Put it in a loop\nreps &lt;- 100\nfor (i in 1:reps) {\n  data &lt;- rbinom(1, n, p)\n  p_hat &lt;- data / n\n  se &lt;- sqrt(p_hat * (1 - p_hat) / n)\n  ci &lt;- p_hat + c(-2, 2) * se\n  print(ci) \n}"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#computer-demo-proportions-means-and-differences-of-means",
    "href": "stats-math/07-lecture/07-lecture.html#computer-demo-proportions-means-and-differences-of-means",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Computer Demo: Proportions, Means, and Differences of Means",
    "text": "Computer Demo: Proportions, Means, and Differences of Means\n\n# Read data from here:  https://github.com/avehtari/ROS-Examples\nlibrary(\"foreign\")\nlibrary(\"dplyr\")\npew_pre &lt;- read.dta(\n  paste0(\n    \"https://raw.githubusercontent.com/avehtari/\",\n    \"ROS-Examples/master/Pew/data/\",\n    \"pew_research_center_june_elect_wknd_data.dta\"\n  )\n)\npew_pre &lt;- pew_pre |&gt; select(c(\"age\", \"regicert\")) %&gt;%\n  na.omit() |&gt; filter(age != 99)\nn &lt;- nrow(pew_pre)\n\n# Estimate a proportion (certain to have registered for voting?)\nregistered &lt;- ifelse(pew_pre$regicert == \"absolutely certain\", 1, 0)\np_hat &lt;- mean(registered)\nse_hat &lt;- sqrt((p_hat * (1 - p_hat)) / n)\nround(p_hat + c(-2, 2) * se_hat, 4) # ci\n\n# Estimate an average (mean age)\nage &lt;- pew_pre$age\ny_hat &lt;- mean(age)\nse_hat &lt;- sd(age) / sqrt(n)\nround(y_hat + c(-2, 2) * se_hat, 4) # ci\n\n# Estimate a difference of means\nage2 &lt;- age[registered == 1]\nage1 &lt;- age[registered == 0]\ny_2_hat &lt;- mean(age2)\nse_2_hat &lt;- sd(age2) / sqrt(length(age2))\ny_1_hat &lt;- mean(age1)\nse_1_hat &lt;- sd(age1) / sqrt(length(age1))\ndiff_hat &lt;- y_2_hat - y_1_hat\nse_diff_hat &lt;- sqrt(se_1_hat ^ 2 + se_2_hat ^ 2)\nround(diff_hat + c(-2, 2) * se_diff_hat, 4) # ci"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#degrees-of-freedom",
    "href": "stats-math/07-lecture/07-lecture.html#degrees-of-freedom",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\n\nSome distributions rely on the concept of degrees of freedom\nDegrees of freedom balance the need for accurate estimation against the amount of data available; it’s a way to correct for overfitting\nThe more parameters you estimate, the fewer degrees of freedom you have left for other calculations\nThe data will give us \\(n\\) (sample size) degrees of freedom, and \\(p\\) number of parameters will be used up during the estimation"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#normal-and-t-distribution",
    "href": "stats-math/07-lecture/07-lecture.html#normal-and-t-distribution",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Normal and t Distribution",
    "text": "Normal and t Distribution\n\nt distribution has a degrees freedom parameter, and with a low number of degrees of freedom, it has heavier tails than the Normal"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#confidence-intervals-from-the-t-distribution",
    "href": "stats-math/07-lecture/07-lecture.html#confidence-intervals-from-the-t-distribution",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Confidence Intervals from the t Distribution",
    "text": "Confidence Intervals from the t Distribution\n\n\n\n\nt distribution has location and scale parameters and is symmetric like the Normal\nStandard error would have \\(n-1\\) degrees of freedom as one will be estimated to compute the mean\nSuppose you threw 5 darts and the distance from bull’s eye was as follows (standard dart board radius is about 23 cm)\n\n\n\n\n\n# Distance from the bull's eye in cm\ndata &lt;- c(8, 6, 10, 5, 18)\n\n# Calculate the sample mean\nmean_data &lt;- mean(data)\n\n# Calculate the standard error of the mean\nse_mean &lt;- sd(data) / sqrt(length(data))\n\n# Degrees of freedom\ndf &lt;- length(data) - 1\n\n# Calculate the 95% and 50% confidence interval \nci_95 &lt;- mean_data + qt(c(0.025, 0.975), df) * se_mean\nci_50 &lt;- mean_data + qt(c(0.25, 0.75), df) * se_mean\n\n# Output the results\nmean_data |&gt; round(2)\n\n[1] 9.4\n\nse_mean |&gt; round(2)\n\n[1] 2.32\n\nci_95 |&gt; round(2)\n\n[1]  2.97 15.83\n\nci_50 |&gt; round(2)\n\n[1]  7.69 11.11"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#bias-and-uncertainty",
    "href": "stats-math/07-lecture/07-lecture.html#bias-and-uncertainty",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Bias and Uncertainty",
    "text": "Bias and Uncertainty\n\nThere is a lot more to it than what is in following standard picture\nDiscuss among yourselves what are the potential sources of bias"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#statistical-significance",
    "href": "stats-math/07-lecture/07-lecture.html#statistical-significance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Statistical Significance",
    "text": "Statistical Significance\n\n\nComes up in NHST: Null Hypothesis Significance Testing\nBad decision filter: if p-value is less than 0.05 (relative to some Null), the results can be trusted, otherwise they are likely noise\n\n\n\n\\[\n\\text{p-valu}e(y) = \\P(T(y_\\text{rep}) &gt;= T(y) \\mid H)\n\\]\n\n\n\nOften, estimates (coefficients) are labeled as not significant if they are within 2 SEs: selecting models this way is also problematic"
  },
  {
    "objectID": "stats-math/07-lecture/07-lecture.html#some-problems-with-statistical-significance",
    "href": "stats-math/07-lecture/07-lecture.html#some-problems-with-statistical-significance",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Problems with Statistical Significance",
    "text": "Some Problems with Statistical Significance\n\n\nStatistical significance does not mean practical (or in the case of Biostats clinical) significance\nNo significance does not mean there is no effect\nThe difference between “significant” and “not significant” is not itself statistically significant\nResearcher degrees of freedom, p-hacking"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#session-8-outline",
    "href": "stats-math/08-lecture/08-lecture.html#session-8-outline",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Session 8 Outline",
    "text": "Session 8 Outline\n\n\nStatistical analysis workflow\nSimulating data for simple regression\nBinary predictor in a regression\nAdding continuous predictor\nCombining binary and continuous predictors\nInterpreting coefficients\nAdding interactions\nUncertainty and predictions\nAssumptions of the regression analysis\nRegression modeling advice\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#analysis-workflow",
    "href": "stats-math/08-lecture/08-lecture.html#analysis-workflow",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Analysis Workflow",
    "text": "Analysis Workflow\n\nFollowing is a high-level, end-to-end view from “R for Data Science” by Wickham and Grolemund"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#some-notation",
    "href": "stats-math/08-lecture/08-lecture.html#some-notation",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Some Notation",
    "text": "Some Notation\n\n\nObserved data \\(y\\)\nUnobserved but observable data \\(\\widetilde{y}\\)\nUnobservable parameters \\(\\theta\\)\nCovariates \\(X\\)\nPrior distribution \\(f(\\theta)\\) (for Bayesian inference)\nData model or sampling distribution (as a function of y) \\(f(y \\mid \\theta, X)\\)\nLikelihood (as a function of \\(\\theta\\)), \\(f(y \\mid \\theta, X)\\), sometimes written as \\(\\mathcal{L}(\\theta \\mid y, X)\\)\nMaximum likelihood estimate (MLE): \\(\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} \\, \\mathcal{L}(\\theta \\mid y, X)\\) (for Frequentist inference)\nPosterior distribution \\(f(\\theta \\mid y, X)\\) (for Bayesian inference)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#bayes-vs-frequentist-inference",
    "href": "stats-math/08-lecture/08-lecture.html#bayes-vs-frequentist-inference",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Bayes vs Frequentist Inference",
    "text": "Bayes vs Frequentist Inference\n\n\n\n\nEstimation is the process of figuring out the unknowns, i.e., unobserved quantities\nIn frequentist inference, the problem is framed in terms of the most likely value(s) of \\(\\theta\\)\nBayesians want to characterize the whole distribution, a much more ambitious goal\n\n\n\n\n\nSuppose we want to characterize the following function, which represents some distribution of the unknown parameter:"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#basic-regression-setup",
    "href": "stats-math/08-lecture/08-lecture.html#basic-regression-setup",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Basic Regression Setup",
    "text": "Basic Regression Setup\n\n\nThere are lots of possible ways to set up a regression\nOne predictor: \\(y = a + bx + \\epsilon\\)\nMultiple predictors: \\(y = b_0 + b_1x_1 + b_2x_2 + \\cdots + b_nx_n + \\epsilon\\), which we can write as \\(y = X\\beta + \\epsilon\\)\nModels with interactions: \\(y = b_0 + b_1x_1 + b_2x_2 +  b_3x_1x_2 + \\epsilon\\)\nNon-linear models like: \\(\\log(y) = a + b\\log(x) + \\epsilon\\)\nGeneralized Linear Models (GLMs), which can, for example, fit binary data, categorical data, and so on\nNonparametric models that are popular in Machine Learning that can learn flexible, functional forms between \\(y\\) and \\(X\\)\nThe latter is not a panacea — if you have a good or good enough functional form, predictions will generally be better and the model will generalize better\n\n\n\nMost of the material in this section comes from Regression and Other Stories by Gelman et al."
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#simulating-a-simple-regression",
    "href": "stats-math/08-lecture/08-lecture.html#simulating-a-simple-regression",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Simulating a Simple Regression",
    "text": "Simulating a Simple Regression\n\nFitting simulated data is a good start for an analysis\n\n\nlibrary(rstanarm)\nn &lt;- 15; x &lt;- 1:n\na &lt;- 0.5\nb &lt;- 2\nsigma &lt;- 3\ny &lt;- a + b*x + rnorm(n, mean = 0, sd = sigma) # or sigma * rnorm(n)\ndata &lt;- data.frame(x, y)\nfit1 &lt;- stan_glm(y ~ x, data = data, refresh = 0) \nprint(fit1)\n\nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 15\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.1    1.4   \nx           2.1    0.2   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.6    0.5   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#simulating-a-simple-regression-1",
    "href": "stats-math/08-lecture/08-lecture.html#simulating-a-simple-regression-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Simulating a Simple Regression",
    "text": "Simulating a Simple Regression\n\nWe extract our betas using the coef() R function\n\n\na_hat &lt;- coef(fit1)[1]\nb_hat &lt;- coef(fit1)[2]\np &lt;- ggplot(aes(x, y), data = data)\np + geom_point(size = 0.2) + geom_abline(intercept = a_hat, slope = b_hat, linewidth = 0.1)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#kidiq-dataset",
    "href": "stats-math/08-lecture/08-lecture.html#kidiq-dataset",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "KidIQ Dataset",
    "text": "KidIQ Dataset\n\nData from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).\n\n\nknitr::kable(rstanarm::kidiq[1:8, ])\n\n\n\n\nkid_score\nmom_hs\nmom_iq\nmom_age\n\n\n\n\n65\n1\n121.11753\n27\n\n\n98\n1\n89.36188\n25\n\n\n85\n1\n115.44316\n27\n\n\n83\n1\n99.44964\n25\n\n\n115\n1\n92.74571\n27\n\n\n98\n0\n107.90184\n18\n\n\n69\n1\n138.89311\n20\n\n\n106\n1\n125.14512\n23"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#single-binary-predictor",
    "href": "stats-math/08-lecture/08-lecture.html#single-binary-predictor",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Single Binary Predictor",
    "text": "Single Binary Predictor\n\nfit2 &lt;- stan_glm(kid_score ~ mom_hs, data = kidiq, refresh = 0) \nprint(fit2)\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 77.6    2.0  \nmom_hs      11.8    2.4  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 19.9    0.7  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nOur model is \\(\\text{kid_score} = 78 + 12 \\cdot \\text{mom_hs} + \\epsilon\\)\nWhat is the average IQ of kids whose mothers did not complete high school? (in this dataset)\nWhat is the average IQ of kids whose mothers completed high school?\nDid high school cause the IQ change?"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#single-binary-predictor-1",
    "href": "stats-math/08-lecture/08-lecture.html#single-binary-predictor-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Single Binary Predictor",
    "text": "Single Binary Predictor\n\np &lt;- ggplot(aes(mom_hs, kid_score), data = kidiq)\np + geom_jitter(height = 0, width = 0.1, size = 0.2) +\n  geom_abline(intercept = coef(fit2)[1], slope = coef(fit2)[2], linewidth = 0.1) +\n  scale_x_continuous(breaks = c(0, 1))"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#single-continous-predictor",
    "href": "stats-math/08-lecture/08-lecture.html#single-continous-predictor",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Single Continous Predictor",
    "text": "Single Continous Predictor\n\nfit3 &lt;- stan_glm(kid_score ~ mom_iq, data = kidiq, refresh = 0) \nprint(fit3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 26.0    6.1  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.3    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nOur model is \\(\\text{kid_score} = 26 + 0.6 \\cdot \\text{mom_iq}+ \\epsilon\\)\nHow much do kids’ scores improve when maternal IQ differs by 10 points?\nWhat is the meaning of the intercepts = 26 here?"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#simple-transformations",
    "href": "stats-math/08-lecture/08-lecture.html#simple-transformations",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Simple Transformations",
    "text": "Simple Transformations\n\nWe will create a new variable called mom_iq_c, which stands for centered\nThe transformation: \\(\\text{mom_iq_c}_i = \\text{mom_iq}_i - \\overline{\\text{mom_iq}}\\)\n\n\nkidiq &lt;- kidiq |&gt;\n  mutate(mom_iq_c = mom_iq - mean(mom_iq))\nfit4 &lt;- stan_glm(kid_score ~ mom_iq_c, data = kidiq, refresh = 0) \nprint(fit4)\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_iq_c\n observations: 434\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 86.8    0.9  \nmom_iq_c     0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.3    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nOur model is \\(\\text{kid_score} = 87 + 0.6 \\cdot \\text{mom_iq_c}+ \\epsilon\\)\nWhat is the meaning of the intercepts in this model?"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#single-continous-predictor-1",
    "href": "stats-math/08-lecture/08-lecture.html#single-continous-predictor-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Single Continous Predictor",
    "text": "Single Continous Predictor\n\np &lt;- ggplot(aes(mom_iq, kid_score), data = kidiq)\np + geom_point(size = 0.2) +\n  geom_abline(intercept = coef(fit3)[1], slope = coef(fit3)[2], linewidth = 0.1)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#combining-the-predictors",
    "href": "stats-math/08-lecture/08-lecture.html#combining-the-predictors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Combining the Predictors",
    "text": "Combining the Predictors\n\nfit5 &lt;- stan_glm(kid_score ~ mom_hs + mom_iq_c, data = kidiq, refresh = 0)\nprint(fit5)\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq_c\n observations: 434\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 82.2    1.9  \nmom_hs       6.0    2.2  \nmom_iq_c     0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.2    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nOur model is now: \\(\\text{kid_score} = 82 + 6 \\cdot \\text{mom_hs} + 0.6 \\cdot \\text{mom_iq_c}+ \\epsilon\\)\nWrite down the interpretation of each coefficient"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#combining-the-predictors-1",
    "href": "stats-math/08-lecture/08-lecture.html#combining-the-predictors-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Combining the Predictors",
    "text": "Combining the Predictors\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq\n observations: 434\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) 25.8    5.9  \nmom_hs       6.0    2.3  \nmom_iq       0.6    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.2    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nOur model is: \\(\\text{kid_score} = 26 + 6 \\cdot \\text{mom_hs} + 0.6 \\cdot \\text{mom_iq}+ \\epsilon\\)\nFor moms that did not complete high school, the line \\(y = 26 + 0.6 \\cdot \\text{mom_iq}\\)\nFor moms that did: \\(\\text{kid_score} = 26 + 6 \\cdot 1 + 0.6 \\cdot \\text{mom_iq} = 32 + 0.6 \\cdot \\text{mom_iq}\\)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#visualizing-the-fit-with-two-predictors",
    "href": "stats-math/08-lecture/08-lecture.html#visualizing-the-fit-with-two-predictors",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Visualizing the Fit With Two Predictors",
    "text": "Visualizing the Fit With Two Predictors\n\np &lt;- ggplot(aes(mom_iq, kid_score), data = kidiq)\np + geom_point(aes(color = as.factor(mom_hs)), size = 0.2) +\n  labs(color = \"Mom HS\") +\n  geom_abline(intercept = coef(fit6)[1], slope = coef(fit6)[3], linewidth = 0.1, color = 'red') +\n  geom_abline(intercept = coef(fit6)[1] + coef(fit6)[2], slope = coef(fit6)[3], linewidth = 0.1, color = 'blue')"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#adding-interactions",
    "href": "stats-math/08-lecture/08-lecture.html#adding-interactions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Adding Interactions",
    "text": "Adding Interactions\n\nIn the previous model, we forced the slopes of mothers who completed and did not complete high school to be the same — the only difference was the intercept\nTo allow the slopes to vary, we include an interaction term\n\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq\n observations: 434\n predictors:   4\n------\n              Median MAD_SD\n(Intercept)   -10.6   13.7 \nmom_hs         50.2   15.1 \nmom_iq          1.0    0.1 \nmom_hs:mom_iq  -0.5    0.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 18.0    0.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nOur model is: \\(\\text{kid_score} = -10 + 49 \\cdot \\text{mom_hs} + 1 \\cdot \\text{mom_iq} - 0.5 \\cdot \\text{mom_hs} \\cdot \\text{mom_iq} + \\epsilon\\)\nTo figure out the slopes, we consider two cases\nFor \\(\\text{mom_hs} = 0\\), the line is: \\(\\text{kid_score} = -10 + 1 \\cdot \\text{mom_iq}\\)\nFor \\(\\text{mom_hs} = 1\\), the line is: \\(\\text{kid_score} = -10 + 49 \\cdot 1 + 1 \\cdot \\text{mom_iq} - 0.5 \\cdot 1 \\cdot \\text{mom_iq} = 39 + 0.5\\cdot \\text{mom_iq}\\)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#visualizing-interactions",
    "href": "stats-math/08-lecture/08-lecture.html#visualizing-interactions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\np &lt;- ggplot(aes(mom_iq, kid_score), data = kidiq)\np + geom_point(aes(color = as.factor(mom_hs)), size = 0.2) +\n  labs(color = \"Mom HS\") +\n  geom_abline(intercept = coef(fit7)[1], slope = coef(fit7)[3], linewidth = 0.1, color = 'red') +\n  geom_abline(intercept = coef(fit7)[1] + coef(fit7)[2], slope = coef(fit7)[3] + coef(fit7)[4], linewidth = 0.1, color = 'blue')"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#interpreting-coefficients-in-complex-non-linear-models",
    "href": "stats-math/08-lecture/08-lecture.html#interpreting-coefficients-in-complex-non-linear-models",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Interpreting Coefficients in Complex Non-Linear Models",
    "text": "Interpreting Coefficients in Complex Non-Linear Models\n\n\nModels that have many parameters, many interactions, and are non-linear are difficult to interpret by looking at parameters marginally (i.e., one at a time)\nFor this reason, we typically rely on assessing how changes to model inputs affect model predictions, not the impact of each parameter\nPredictions take all the parameter values and their interactions into account"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction",
    "href": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Uncertainty and Prediction",
    "text": "Uncertainty and Prediction\n\n\nWe return to our simple linear regression model: \\(\\text{kid_score} = 26 + 0.6 \\cdot \\text{mom_iq}+ \\epsilon\\)\nWe can extract the simulations of all plausible parameter values (called a posterior distribution in Bayesian analysis)\n\n\n\n\npost &lt;- as.matrix(fit3)\ndim(post)\n\n[1] 4000    3\n\nknitr::kable(post[1:5, ])\n\n\n\n\n(Intercept)\nmom_iq\nsigma\n\n\n\n\n31.77474\n0.5396861\n18.28899\n\n\n27.21061\n0.5917098\n18.20332\n\n\n27.27470\n0.5942835\n18.29135\n\n\n26.54087\n0.6114706\n18.79476\n\n\n25.46339\n0.6054314\n17.86559"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction-1",
    "href": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction-1",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Uncertainty and Prediction",
    "text": "Uncertainty and Prediction\n\n\nUsing these simulations we display 50 plausible regression lines\n\n\n\n\npost &lt;- as.data.frame(fit3)\npost_sub &lt;- post[sample(1:nrow(post), 50), ]\np &lt;- ggplot(aes(mom_iq, kid_score), data = kidiq)\np + geom_point(size = 0.2) + \n  geom_abline(aes(intercept = `(Intercept)`, slope = mom_iq), \n              linewidth = 0.1,\n              data = post_sub)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction-2",
    "href": "stats-math/08-lecture/08-lecture.html#uncertainty-and-prediction-2",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Uncertainty and Prediction",
    "text": "Uncertainty and Prediction\n\n\nWe can compute inferential uncertainty in all the parameters directly\nWe can also compute event probabilities\n\n\n\n\npost &lt;- as.matrix(fit3)\n# median\napply(post, 2, median)\n\n(Intercept)      mom_iq       sigma \n 26.0236767   0.6078977  18.2714868 \n\napply(post, 2, mad)\n\n(Intercept)      mom_iq       sigma \n 6.10031655  0.06062729  0.61543505 \n\n# 50% uncertainty estimate\napply(post, 2, quantile, probs = c(0.25, 0.75))\n\n     parameters\n      (Intercept)    mom_iq    sigma\n  25%    21.75756 0.5681001 17.86710\n  75%    30.06156 0.6504483 18.69458\n\n# 90% uncertainty estimate\napply(post, 2, quantile, probs = c(0.05, 0.95))\n\n     parameters\n      (Intercept)    mom_iq    sigma\n  5%     15.72707 0.5106528 17.32535\n  95%    35.77134 0.7095889 19.33193\n\n# What is the probability that the coefficient of mom_iq is &gt; 0.6\nmean(post[, \"mom_iq\"] &gt; 0.6) |&gt; round(2)\n\n[1] 0.55"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#types-of-prediction",
    "href": "stats-math/08-lecture/08-lecture.html#types-of-prediction",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Types of Prediction",
    "text": "Types of Prediction\n\n\nPoint prediction: \\(\\hat{a} + \\hat{b}x^{\\text{new}}\\) — predicting expected average \\(y\\) — don’t recommend\nLinear predictor with uncertainty in \\(a\\) and \\(b\\): \\(a + bx^{\\text{new}}\\) — predicting uncertainty around the expexted average value of \\(y\\) — typically used to assess treatment effect\nPredictive distribution: \\(a + bx^{\\text{new}} + \\epsilon\\) — predictive uncertainty around a new \\(y\\) — used to predict for a new individual (not ATT)"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#prediction-in-rstanarm",
    "href": "stats-math/08-lecture/08-lecture.html#prediction-in-rstanarm",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Prediction in RStanArm",
    "text": "Prediction in RStanArm\n\nWe observe a new kid whose mom’s IQ = 120\nPoint prediction is shown in Red, 90% uncertainty around the average kid score in Blue, and 90% predictive uncertainty for a new kid is in Yellow.\n\n\n\n\nmom120 &lt;- data.frame(mom_iq = 120)\n(point_pred &lt;- predict(fit3, newdata = mom120))\n\n      1 \n98.9816 \n\nlin_pred &lt;- posterior_linpred(fit3, \n                              newdata = mom120)\npost_pred &lt;- posterior_predict(fit3, \n                               newdata = mom120)\n(lp_range &lt;- quantile(lin_pred, \n                      probs = c(0.05, 0.95)))\n\n      5%      95% \n 96.5370 101.4359 \n\n(pp_range &lt;- quantile(post_pred, \n                      probs = c(0.05, 0.95)))\n\n       5%       95% \n 68.88551 128.75195"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#linear-regression-assumptions",
    "href": "stats-math/08-lecture/08-lecture.html#linear-regression-assumptions",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Linear Regression Assumptions",
    "text": "Linear Regression Assumptions\nPresented in the order of importance\n\n\nValidity of the model for the question at hand\nRepresentativeness and valid scope of inference\nAdditivity and linearity\nIndependence of errors\nEqual variance of errors\nNormality of errors"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#quick-tips-to-improve-your-regression-modeling",
    "href": "stats-math/08-lecture/08-lecture.html#quick-tips-to-improve-your-regression-modeling",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "10+ quick tips to improve your regression modeling",
    "text": "10+ quick tips to improve your regression modeling\nThis advice comes from “Regression and Other Stories” by Gelman and Hill\n\n\nThink generatively\nThink about what you are measuring\nThink about variation, replication\nForget about statistical significance\nGraph the relevant and not the irrelevant\nInterpret regression coefficients as comparisons\nUnderstand statistical methods using fake-data simulation\nFit many models\nSet up a computational workflow\nUse transformations\nDo causal inference in a targeted way, not as a byproduct of a large regression\nLearn methods through live examples"
  },
  {
    "objectID": "stats-math/08-lecture/08-lecture.html#thanks-for-being-part-of-my-class",
    "href": "stats-math/08-lecture/08-lecture.html#thanks-for-being-part-of-my-class",
    "title": "SMaC: Statistics, Math, and Computing",
    "section": "Thanks for being part of my class!",
    "text": "Thanks for being part of my class!\n\n\nLectures: https://ericnovik.github.io/smac.html\nKeep in touch:\n\neric.novik@nyu.edu\nhttps://www.linkedin.com/in/enovik/\nTwitter: @ericnovik\nPersonal blog: https://ericnovik.com/"
  }
]