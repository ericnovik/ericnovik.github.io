[
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Lecture 1: Bayesian Workflow",
    "text": "Lecture 1: Bayesian Workflow\n\n\nOverview of the Course\nStatistics vs AI/ML\nBrief history of Bayesian inference\nReview of basic probability\nIntroduction to Bayesian workflow\nBayes’s rule for events\nBinomial model and the Bayesian Crank\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "href": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "title": "Bayesian Inference",
    "section": "Overview of the class",
    "text": "Overview of the class\n\nSyllabus"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "href": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "title": "Bayesian Inference",
    "section": "Statistics vs. AI/ML",
    "text": "Statistics vs. AI/ML\n⚠️ What follows is an oversimplified opinion.\n\n\n\n\nAI is great for automating tasks that humans find easy\n\nRecognizing faces, cats, and other objects\nIdentifying tumors on a radiology scan\nPlaying Chess and Go\nDriving a car\n\n\n\n\nStatistics is great at answering questions that humans find hard\n\nHow fast does a drug clear from the body?\nWhat is the expected tumor size 2 months after treatment?\nHow would patients respond under a different treatment?\nShould I take this drug?\n\n\n\n\n\n“Machine learning excels when you have lots of training data that can be reasonably modeled as exchangeable with your test data; Bayesian inference excels when your data are sparse and your model is dense.” — Andrew Gelman"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "href": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "title": "Bayesian Inference",
    "section": "Brief History",
    "text": "Brief History\nSummary of the book The Theory That Would Not Die\n\n\n\n\nThomas Bayes (1702(?) — 1761) is credited with the discovery of the “Bayes’s Rule”\nHis paper was published posthumously by Richard Price in 1763\nLaplace (1749 — 1827) independently discovered the rule and published it in 1774\nScientific context: Newton’s Principia was published in 1687\nBayesian wins: German Enigma cipher, search for a missing H-bomb, Federalist papers, Moneyball, political forecasting, …\n\n\n\n\n\n\n\n\n\n\n\nStephen Stigler gives 3:1 in favor of Nicholas Saunderson for the discovery of Bayes’s [sic] rule."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "href": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "title": "Bayesian Inference",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world.”"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "title": "Bayesian Inference",
    "section": "Review of Probability",
    "text": "Review of Probability\n\n\nA set of all possible outcomes is called a sample space and denoted by \\(\\Omega\\)\nAn outcome of an experiment is denoted by \\(\\omega \\in \\Omega\\)\nWe typically denote events by capital letters, say \\(A \\subseteq \\Omega\\)\nAxioms of probability:\n\n\\(\\P(A) \\geq 0, \\, \\text{for all } A\\)\n\\(\\P(\\Omega) = 1\\)\nIf \\(A_1, A_2, A_3, \\ldots\\) are disjoint: \\(\\P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} \\P(A_i)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example",
    "href": "bayes-course/01-lecture/01-lecture.html#example",
    "title": "Bayesian Inference",
    "section": "Example",
    "text": "Example\n\nRolling diceOmega\n\n\n\n\nYou roll a fair six-sided die twice\nGive an example of an \\(\\omega \\in \\Omega\\)\nHow many elements are in \\(\\Omega\\)? What is \\(\\Omega\\)?\nDefine an event \\(A\\) as the sum of the two rolls less than 11\nHow many elements are in \\(A\\)?\nWhat is \\(\\P(A)\\)?\n\n\n\n\n\n\n\nouter(1:6, 1:6, FUN = \"+\")\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    2    3    4    5    6    7\n[2,]    3    4    5    6    7    8\n[3,]    4    5    6    7    8    9\n[4,]    5    6    7    8    9   10\n[5,]    6    7    8    9   10   11\n[6,]    7    8    9   10   11   12\n\n\n\n\n\n\\(\\Omega\\) consists of all pairs \\(\\{(1, 1), (1, 2), ... (2, 1), (2, 2), ...\\}\\)\nThere are 36 such pairs\n33 of those pairs result in a sum of less than 11\nTherefore, \\(\\P(A) = \\frac{33}{36} = \\frac{11}{12}\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#random-variables",
    "href": "bayes-course/01-lecture/01-lecture.html#random-variables",
    "title": "Bayesian Inference",
    "section": "Random Variables",
    "text": "Random Variables\n\nRandom variable is not random – it is a deterministic mapping from the sample space onto the real line; randomness comes from the experiment\nInclude a picture of the mapping\nPMF, PDF, CDF\nExpectations\nConditional expectations E(X | Y)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "href": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "title": "Bayesian Inference",
    "section": "Example Simulation",
    "text": "Example Simulation\n\nWe will use R’s sample() function to simulate rolls of a die and replicate() function to repeat the rolling process many times\n\n\n\n\ndie <- 1:6\nroll <- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\nroll(die, 2) # roll the die twice\n\n[1] 5 2\n\nrolls <- replicate(1e4, roll(die, 2))\nrolls[, 1:6]\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    3    3    4    1    6    3\n[2,]    2    4    1    2    4    6\n\nroll_sums <- colSums(rolls)\nhead(roll_sums)\n\n[1]  5  7  5  3 10  9\n\nmean(roll_sums < 11) \n\n[1] 0.9173\n\n\n\n\n\nGiven a Random Variable \\(Y\\), \\(y^{(1)}, y^{(2)}, y^{(3)}, \\ldots, y^N\\) are simulations or draws from \\(Y\\)\nFundamental bridge (Blitzstein & Hwang p. 164): \\(\\P(A) = \\E(\\I_A)\\)\nComputationally: \\(\\P(Y < 11) \\approx \\frac{1}{N} \\sum^{N}_{n=1} \\I(y^n < 11)\\)\nIn R, roll_sums < 11 creates an indicator variable\nAnd mean() does the average"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "title": "Bayesian Inference",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nFor arbitrary \\(A\\) and \\(B\\), if \\(\\P(B) > 0\\):\n\nConditional probability: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)}\\)\n\n\\(A\\) and \\(B\\) are independent if observing \\(B\\) does not give you any more information about \\(A\\): \\(\\P(A \\mid B) = \\P(A)\\)\nThis argument is symmetric: \\(\\P(B \\mid A) = \\P(B)\\)\nTherefore, \\(\\P(AB) = \\P(A \\mid B) \\; \\text{and} \\; \\P(B) = \\P(A) \\P(B)\\)\nBayes’s rule: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)} = \\frac{\\P(B \\mid A) \\P(A)}{\\P(B)}\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\nTitanic carried approximately 2,200 passengers and sank on 15 April 1912\nLet \\(G: \\{m, f\\}\\) represent Gender and \\(S: \\{n, y\\}\\) represent Survival\n\n\n\nsurv <- apply(Titanic, c(2, 4), sum) |> \n  as_tibble()\nsurv_prop <- round(surv / sum(surv), 3)\nbind_cols(Sex = c(\"Male\", \"Female\"), \n          surv_prop) |> \n  adorn_totals(c(\"row\", \"col\")) |>\n  knitr::kable(caption = \n               \"Titanic survival proportions\")\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n\n\n\n\\(\\P(G = m \\cap S = y) =\\) 0.167\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) =\\) ??\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) = 0.323\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\n\n\nWhat is \\(\\P(G = m \\mid S = y)\\), probability of being male given survival?\nTo compute that, we only consider the column  where S = Yes\n\\(\\P(G = m \\mid S = y) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(S = y)} = \\frac{0.167}{0.323} \\\\ \\approx 0.52\\)\nYou want \\(\\P(S = y \\mid G = m)\\), comparing it to \\(\\P(S = y \\mid G = f)\\)\n\\(\\P(S = y \\mid G = m) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(G = m)} = \\frac{0.167}{0.787} \\\\ \\approx 0.21\\)\n\\(\\P(S = y \\mid G = f) = \\frac{\\P(G = f \\, \\cap \\, S = y)}{\\P(G = f)} = \\frac{0.156}{0.213} \\\\ \\approx 0.73\\)\nHow would you calculate \\(\\P(S = n \\mid G = m)\\)?\n\\(\\P(S = n \\mid G = m) = 1 - \\P(S = y \\mid G = m)\\)\n\n\n\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n “Untergang der Titanic”, as conceived by Willy Stöwer, 1912"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "href": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "title": "Bayesian Inference",
    "section": "Law of Total Probability (LOTP)",
    "text": "Law of Total Probability (LOTP)\n\\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B \\cap A_i) = \\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)\n\\]\n\n\nImage from Blitzstein and Hwang (2019), Page 55"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "href": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "title": "Bayesian Inference",
    "section": "Bayes’s Rule for Events",
    "text": "Bayes’s Rule for Events\n\n\nWe can combine the definition of conditional probability with the LOTP to come up with a Bayes’s rule for events, assuming \\(\\P(B) \\neq 0\\)\n\n\\[\n\\P(A \\mid B) = \\frac{\\P(B \\cap A)}{\\P(B)} =\n               \\frac{\\P(B \\mid A) \\P(A)}{\\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)}\n\\]\n\nWe typically think of \\(A\\) is some unknown we wish to learn (e.g., the status of a disease) and \\(B\\) as the data we observe (e.g., the result of a diagnostic test)\nWe call \\(\\P(A)\\) prior probability of A (e.g., how prevalent is the disease in the population)\nWe call \\(\\P(A \\mid B)\\), the posterior probability of the unknown \\(A\\) given data \\(B\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "href": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "title": "Bayesian Inference",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(\\P(D^+ \\mid T^+)\\)\n\\(\\text{Specificity } := \\P(T^- \\mid D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - \\P(T^- \\mid D^-) = \\P(T^+ \\mid D^-) = 0.002\\)\n\\(\\text{Sensitivity } := \\P(T^+ \\mid D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - \\P(T^+ \\mid D^+) = \\P(T^- \\mid D^+) = 0.546\\)\nPrevalence: \\(\\P(D^+) = 0.001\\)\n\n\\[\n\\begin{eqnarray}\n\\P(D^+ \\mid T^+) = \\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+)} & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\sum_{i=1}^{n}\\P(T^+ \\mid D^i) \\P(D^i) } & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+ \\mid D^+) \\P(D^+) + \\P(T^+ \\mid D^-) \\P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "title": "Bayesian Inference",
    "section": "Bayesian Analysis",
    "text": "Bayesian Analysis\n\n\n\n\nSuppose, we enrolled 5 people in an early cancer clinical trial\nEach person was given an active treatment\nFrom previous trials, we have some idea of historical response rates (proportion of people responding)\nAt the end of the trial, \\(Y = y \\in \\{0,1,...,5 \\}\\) people will have responded1 to the treatment\nWe are interested in estimating the probability that the response rate is greater great or equal to 50%\n\n\n\n Image from Fokko Smits, Martijn Dirksen, and Ivo Schoots: RECIST 1.1 - and more\n\n\nPartial response is a decrease in the sum of the longest diameters of target lesions of at least 30% compared to the baseline measurement."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "href": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "title": "Bayesian Inference",
    "section": "Notation Confusion",
    "text": "Notation Confusion\n\n\n\\(\\theta\\): unknowns or parameters to be estimated could be multivariate, discrete, and continuous (your book uses \\(\\pi\\))\n\\(y\\): observations or measurements to be modelled (\\(y_1, y_2, ...\\))\n\\(\\tilde{y}\\): unobserved but observable quantities (in your book \\(y'\\))\n\\(x\\): covariates\n\\(f( \\theta )\\): a prior model, P[DM]F of \\(\\theta\\)\n\\(f(y \\mid \\theta, x)\\): an observational model, P[DM]F when it is a function of \\(y\\) (in your book: \\(f(y \\mid \\pi)\\)); we typically drop the \\(x\\) to simplify the notation.\n\\(f(y \\mid \\theta)\\): is a likelihood function when it is a function of \\(\\theta\\) (in your book: \\(\\L(\\pi \\mid y)\\))\nSome people write \\(\\L(\\theta; y)\\) or simply \\(\\L(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "href": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "title": "Bayesian Inference",
    "section": "General Approach",
    "text": "General Approach\n\n\nBefore observing the data, we need to specify a prior model \\(f(\\theta)\\) on all unknowns \\(\\theta\\)1\nPick a data model \\(f_{Y}(Y \\mid \\Theta = \\theta)\\) — this is typically more important than a prior\nFor more complex models, we construct a prior predictive distribution, \\(f(y)\\)\n\nWe will define this quantity later — it will help us assess if our choice of priors makes sense on the observational scale: \\(f(\\theta) \\rightarrow f(y)\\)\n\nAfter we observe data \\(y\\), we treat \\(f_{\\Theta}(Y = y \\mid \\Theta)\\) as the likelihood of observing \\(y\\) under all plausible values of \\(\\theta\\)\nDerive a posterior model for \\(\\theta\\), \\(\\, f_{\\Theta}(\\Theta \\mid Y = y)\\) using Bayes’s rule\nCompute all the quantities of interest from the posterior, such as event probabilities, e.g., \\(\\P(\\theta > 0.5)\\), posterior predictive distribution \\(f(\\tilde{Y} \\mid Y = y)\\), decision functions, etc.\n\n\nThere is always a prior, even in frequentist inference."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#prior-model",
    "href": "bayes-course/01-lecture/01-lecture.html#prior-model",
    "title": "Bayesian Inference",
    "section": "Prior Model",
    "text": "Prior Model\n\nWe will construct a prior model for our clinical trial\nFrom previous trials, we construct a discretized version of the prior distribution of the resposne rate\nThe most likely value for response rate is 30%\n\n\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\ndot_plot(theta, prior) +\n  ggtitle(\"Prior probability of response rates\")\n\n\n\nEven though \\(\\theta\\) is a continuous parameter, we can still specify a discrete prior\nThe posterior will also be discrete and of the same cardinality"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#data-model",
    "href": "bayes-course/01-lecture/01-lecture.html#data-model",
    "title": "Bayesian Inference",
    "section": "Data Model",
    "text": "Data Model\n\nWe consider each person’s response rate to be independent, given the treatment\nWe have a fixed number of people in the trial, \\(N = 5\\), and \\(0\\) to \\(5\\) successes\nWe will therefore consider: \\(Y | \\theta \\sim \\text{Bin}(N,\\theta) = \\text{Bin}(5,\\theta)\\)\n\\(f(y \\mid \\theta) = \\text{Bin} (y \\mid 5,\\theta) = \\binom{5}{y} \\theta^y (1 - \\theta)^{5 - y}\\) for \\(y \\in \\{0,1,\\ldots,5\\}\\)\nIs this a valid probability distribution as a function of \\(y\\)?\n\n\nN = 5; y <- 0:N; theta <- 0.5\n(f_y <- dbinom(x = y, size = N, prob = theta) |>\n    fractions())\n\n[1] 1/32 5/32 5/16 5/16 5/32 1/32\n\nsum(dbinom(x = y, size = N, prob = theta))\n\n[1] 1"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\nWe ran the trial and observed 3 out of 5 responders\nWe can now construct a likelihood function for \\(y = 3\\) as a function of \\(\\theta\\)\nLet’s check if this function is a probability distribution: \\[\nf (y) = \\int_{0}^{1} \\binom{N}{y} \\theta^y (1 - \\theta)^{N - y}\\, d\\theta = \\frac{1}{N + 1}\n\\]\n\n\ndbinom_theta <- function(theta, N, y) {\n  choose(N, y) * theta^y * (1 - theta)^(N - y) \n}\nintegrate(dbinom_theta, lower = 0, upper = 1, \n          N = 5, y = 3)[[1]] |> fractions()\n\n[1] 1/6\n\n\n\n\\(f(y)\\) is called marginal distribution of the data or, more aptly, prior predictive distribution\nIt tells us that prior to observing \\(y\\), all values of \\(y\\) are equally likely"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\n\n\n\n\n\n\n\nCompare with the data model"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "href": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "title": "Bayesian Inference",
    "section": "Compute the Posterior",
    "text": "Compute the Posterior\n\n\n\n\nFirst, we compute the likelihood\n\n\\(\\binom{5}{3} \\theta^3 (1 - \\theta)^{2}\\) for \\(\\theta \\in \\{0.10, 0.30, 0.50, 0.70, 0.90\\}\\)\n\nNext, we multiply the likelihood by the prior, to get the numerator\n\n\\(f(y = 3 \\mid \\theta) f(\\theta)\\)\n\nSum the numerator to get the marginal likelihood\n\n\\(f(y = 3) = \\sum_{\\theta} f(y = 3 | \\theta) f(\\theta) \\approx 0.2\\)\n\nFinally, compute the posterior\n\n\\(f(\\theta \\mid y = 3) = \\frac{f(y = 3 \\mid \\theta) f(\\theta)}{\\sum_{\\theta} f(y = 3 | \\theta) f(\\theta)}\\)\n\n\n\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "title": "Bayesian Inference",
    "section": "Computing Event Probability",
    "text": "Computing Event Probability\n\n\n\nTo compute event probabilities, we integrate (or sum) the relevant regions of the parameter space\n\n\\[\n\\P(\\theta \\geq 0.5) = \\int_{0.5}^{1} f(\\theta) \\, d\\theta\n\\]\n\nIn this case, we only have discrete quantities, so we sum:\n\n\nprobs <- d |>\n  filter(theta >= 0.50) |>\n  dplyr::select(prior, post) |>\n  colSums() |>\n  round(2)\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00 \n  \n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.5 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.71"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "href": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "title": "Bayesian Inference",
    "section": "What If We Used a Flat Prior?",
    "text": "What If We Used a Flat Prior?\n\nFlat or uniform prior means that we consider all values of \\(\\theta\\) equality likely\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.20, 0.20, 0.20, 0.20, 0.20)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.2 \n    0.01 \n    0.00 \n    0.01 \n  \n  \n    0.3 \n    0.2 \n    0.13 \n    0.03 \n    0.16 \n  \n  \n    0.5 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.7 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.9 \n    0.2 \n    0.07 \n    0.01 \n    0.09 \n  \n  \n    Total \n    1.0 \n    0.83 \n    0.17 \n    1.00 \n  \n\n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.6 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.83"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\n\nGelman, A. et al. (2020). Bayesian Workflow. ArXiv:2011.01808 [Stat]. http://arxiv.org/abs/2011.01808\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#other-conjugate-models-and-posterior-sampling",
    "href": "bayes-course/03-lecture/03-lecture.html#other-conjugate-models-and-posterior-sampling",
    "title": "Bayesian Inference",
    "section": "Other Conjugate Models and Posterior Sampling",
    "text": "Other Conjugate Models and Posterior Sampling\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#introduction",
    "href": "bayes-course/03-lecture/03-lecture.html#introduction",
    "title": "Bayesian Inference",
    "section": "Introduction",
    "text": "Introduction\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "href": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "title": "Bayesian Inference",
    "section": "Conjugate models and Beta-Binomial",
    "text": "Conjugate models and Beta-Binomial\n\n\nBeta distribution\nGreat expectations\nTuning the prior model for the clinical trial analysis\nBinomial likelihood with continuous \\(\\theta\\)\nDeriving the conjugate posterior\nAnalysis of the sex ratio\nCompromise between priors and data model\nCoherence of Bayesian inference\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introduction",
    "href": "bayes-course/02-lecture/02-lecture.html#introduction",
    "title": "Bayesian Inference",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\nLast time, we presented the discretized version of the prior\nIn practice, we are typically working in continuous space\nIn general, the prior model can be arbitrarily complex\nThere is a family of distributions called the exponential family, for which the prior has the same kernel as the posterior\nThis is called conjugacy\nIn practice, we seldom rely on conjugate relationships\nBeta distribution is conjugate to Binomial"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "href": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "title": "Bayesian Inference",
    "section": "Continous Probability Distributions",
    "text": "Continous Probability Distributions\n\nRecall that for the continuous RVs we have, the CDF is defined as\n\n\\[\n\\begin{eqnarray}\nF(x) & = & \\int_{-\\infty}^{x} f(t) \\, dt, \\, \\text{and} \\\\\nf(x) & = & F'(x), \\, \\text{where } F \\text{ is differentiable}\n\\end{eqnarray}\n\\]\n\nTo compute event probabilities:\n\n\\[\n\\begin{eqnarray}\n\\P(a \\le X \\leq b) & = & F(b) − F(a) = \\int_{a}^{b} f(x) \\, dx \\\\\n\\P(X \\in A) & = & \\int_{A} f(x) \\, dx\n\\end{eqnarray}\n\\]\n\nAs in the case of PMFs:\n\n\\[\n\\begin{eqnarray}\nf(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "title": "Bayesian Inference",
    "section": "Introducing Beta",
    "text": "Introducing Beta\n\n\n\nBeta distribution has the following functional form for \\(\\alpha > 0, \\, \\beta > 0, \\, \\theta \\in (0, 1)\\)\n\n\\[\n\\begin{eqnarray}\n\\text{Beta}(\\theta \\mid \\alpha,\\beta) & = &\n\\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta^{\\alpha - 1} \\, (1 -\n\\theta)^{\\beta - 1} \\\\\n\\text{B}(a,b) \\ & = & \\ \\int_0^1 u^{a - 1} (1 - u)^{b - 1} \\,\ndu \\ = \\ \\frac{\\Gamma(a) \\, \\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(x) & = &\n\\int_0^{\\infty} u^{x - 1} \\exp(-u) \\, du\n\\end{eqnarray}\n\\]\n\n\n\n\n\n\n\n\n\nFor positive integrers \\(n\\): \\(\\, \\Gamma(n+1) = n!\\) and in general \\(\\Gamma(z + 1) = z \\Gamma(z)\\)\n\\(\\text{Beta}(1, 1)\\) is equivalent to \\(\\text{Unif(0, 1)}\\)\nThe above makes Beta a natural choice for priors on the probability scale"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "title": "Bayesian Inference",
    "section": "Visualizing Beta",
    "text": "Visualizing Beta\n\nThe mode of Beta, \\(\\text{Mode}(\\theta) = \\argmax_\\theta \\text{Beta}(\\theta \\mid \\alpha, \\beta)\\) is shown in red and the function maximum in blue"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#expectations",
    "title": "Bayesian Inference",
    "section": "Expectations",
    "text": "Expectations\n\nRecall the definition of expectations for continuous random variables\nWe often write \\(\\mu\\) or \\(\\mu_X\\) for expected value of \\(X\\)\n\n\\[\n\\mu_X = \\E(X) = \\int x \\cdot f(x) \\, dx\n\\]\n\nVariance is a type of expectation, which we often denote by \\(\\sigma^2\\)\n\n\\[\n\\sigma_X = \\V(X) = \\E(X - \\mu)^2 = \\int (X - \\mu)^2 f(x) \\, dx\n\\]\n\nIt is often more convenient to write the variance as\n\n\\[\n\\V(X) = \\E(X^2) - \\mu^2\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\nKeeping in mind that a Gamma function is a continuous analog of the factorial:\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta) &=& \\int_{0}^{1} \\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta \\\\\n&=& \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_{0}^{1}  \\color{red}{\\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1}} \\, d\\theta \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\cdot\n\\color{red}{\\frac{\\Gamma(1 + \\alpha) \\Gamma(\\beta)}{\\Gamma(1 + a + b)}} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{\\Gamma(1 + \\alpha)}{\\Gamma(1 + \\alpha + \\beta)} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{a \\Gamma(\\alpha)}{(\\alpha + \\beta) \\Gamma(\\alpha + \\beta)} \\\\\n&=& \\frac{\\alpha}{\\alpha + \\beta}.\n\\end{eqnarray}\n\\]\n\nCheck the integral \\(\\int_{0}^{1} \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\nWe can find the mode of this distribution by taking the log, differentiating with respect to \\(\\theta\\), and setting the derivative function to zero\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta) & = & \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Mode}(\\theta) & = & \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\;\\; \\text{ when } \\; \\alpha, \\beta > 1. \\\\\n\\end{eqnarray}\n\\]\n\nThe variance of \\(\\theta\\) can be derived using the definition of the variance operator\n\n\\[\n\\begin{eqnarray}\n\\V(\\theta) & = & \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\n\n\nWe borrow an example from BDA3: probability of a girl birth given placenta previa (PP)\nPlacenta previa is a condition when the placenta completely or partially covers the opening of the uterus\nA PP study in Germany found that out of 980 births, 437 or \\(\\approx 45\\%\\) were female\nSex ratio in the population has been extremely stable over space and time at 48.5% females with deviations within no more than 1%1\nOur task is to assess the evidence for \\(\\P(\\text{ratio} < .485 \\mid \\text{PP})\\)\n\n\n\n\n\n\n\n\n\n\nGelman, A., & Weakliem, D. (2009). Of Beauty, Sex and Power. American Scientist, 97(4), 310. https://doi.org/10.1511/2009.79.310"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "href": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "title": "Bayesian Inference",
    "section": "Biomomial Likelihood",
    "text": "Biomomial Likelihood\n\nRecall that Likelihood is the function of the paramter \\(\\theta\\), assuming \\(\\theta \\in [0,1]\\)\n\n\\[\n\\text{Bin}(y \\mid \\theta) = \\binom{N}{y}\n\\theta^y (1 - \\theta)^{N - y} \\propto \\theta^y (1 - \\theta)^{N - y}\n\\]\n\nAssuming \\(N = 10\\), here is the likelihood for \\(\\theta\\), given a few possible values of \\(y\\) successes"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "href": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving the Posterior Distribution",
    "text": "Deriving the Posterior Distribution\n\nThe denominator is constant in \\(\\theta\\); we will derive the posterior up to a proportion\n\n\\[\n\\begin{eqnarray}\nf(y \\mid \\theta) & \\propto & \\theta^y (1 - \\theta)^{N - y} \\\\\nf(\\theta) & \\propto & \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\nf(\\theta \\mid y) & \\propto & f(y \\mid \\theta) f(\\theta) \\\\\n& = & \\theta^y (1 - \\theta)^{N - y} \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\n& = & \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\\\\nf(\\theta \\mid y) & = & \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\n\\end{eqnarray}\n\\]\n\nThe last equality comes from matching the kernel \\(\\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1}\\) to the normalized Beta PDF which has a normalizing constant \\(\\frac{\\Gamma (\\alpha +\\beta + N)}{\\Gamma (\\alpha + y) \\Gamma (\\beta + N - y)}\\)\nSince the posterior is in the same family as the prior, we say that Beta is a conjugate to Binomial\n\n\nCheck the kernel integral, \\(\\int_{0}^{1} \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "title": "Bayesian Inference",
    "section": "Posterior Expectations",
    "text": "Posterior Expectations\n\\[\n\\begin{eqnarray}\n\\E(\\theta \\mid Y=y)  & = & \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\cdot \\E(\\theta) + \\frac{n}{\\alpha + \\beta + n}\\cdot\\frac{y}{n} \\\\\n\\V(\\theta \\mid Y=y)  & = & \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)} \\\\\n\\text{Mode}(\\theta \\mid Y=y) & = & \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} = \\frac{\\alpha + \\beta - 2}{\\alpha + \\beta + n - 2} \\cdot\\text{Mode}(\\theta) + \\frac{n}{\\alpha + \\beta + n - 2} \\cdot\\frac{y}{n}\n\\end{eqnarray}\n\\]\n\n\nNote tha the posterior expectation is between \\(y/n\\) and \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\nAlso note that as sample becomes very large \\(\\E(\\theta \\mid Y=y) \\rightarrow \\frac{y}{n}\\) and \\(\\V(\\theta \\mid Y=y) \\rightarrow 0\\)\nAs before, \\(\\text{Mode}(\\theta \\mid Y=y) = \\E(\\theta \\mid Y=y)\\), when \\(\\alpha = \\beta\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\nLet’s derive the analytical posterior and compare it with the samples from the posterior distribution\nFirst, we will consider the uniform \\(\\text{Beta}(1, 1)\\) prior\nWe are told that data are \\(N = 980\\) and \\(y = 437\\) female births\nThe posterior is \\(\\text{Beta}(1 + 437, 1 + 980 - 437) = \\text{Beta}(438, 544)\\)\n\\(\\E(\\theta \\mid Y=437) = \\frac{\\alpha + 437}{\\alpha + \\beta + 980} = \\frac{438}{982} \\approx 0.446\\)\n\\(\\sqrt{\\V(\\theta \\mid Y=y)} \\approx\\) 0.016\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 438, 544) |> round(3)\nlower <- qbeta(l, 438, 544) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.415, 0.477]\n\nevent_prob <- integrate(dbeta, lower = 0, upper = 0.485, shape1 = 438, shape2 = 544)[[1]]\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "href": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "title": "Bayesian Inference",
    "section": "Obtaining Quantiles by Sampling",
    "text": "Obtaining Quantiles by Sampling\n\nWe can use R’s rbeta() RNG to generate draws from the posterion\n\n\n\ndraws <- rbeta(n = 1e5, 438, 544)\np <- ggplot(aes(draws), data = data.frame(draws))\np + geom_histogram(bins = 30) + ylab(\"\") +\n  geom_vline(xintercept = 0.485, \n             colour = \"red\", size = 0.3) +\n  ggtitle(\"Draws from Beta(438, 544)\") + \n  theme(axis.text.y = element_blank()) + \n  xlab(expression(theta))\n\n\n\n\n\n\n\n\n\n\n\nWe can use the quantile() function to get the posterior interval and compute the event probability by evaluating the expectation of the indicator function as before\n\n\nquantile(draws, probs = c(0.025, 0.5, 0.975)) |> round(3)\n\n 2.5%   50% 97.5% \n0.415 0.446 0.477 \n\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", mean(draws < 0.485) |> round(3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "title": "Bayesian Inference",
    "section": "Population Priors",
    "text": "Population Priors\n\n\n\n\nWhat priors should we use if we think the sample is drawn from the sex ratio “hyper-population”?\nWe know that the population mean is 0.485 and the standard deviation is about 0.01\nBack out the parameters of the population Beta distribution\n\n\n\\[\n\\begin{eqnarray}\n\\begin{cases}\n\\frac{\\alpha}{\\alpha + \\beta} & = & 0.485 \\\\\n\\sqrt{\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}} & = & 0.01\n\\end{cases}\n\\end{eqnarray}\n\\]\n\n\n\\(\\alpha \\approx 1211\\) and \\(\\beta \\approx 1286\\)\n\n\n\n\n\nCheck the result with the simulation\n\n\n\nx <- rbeta(1e4, 1211, 1286)\nmean(x) |> round(3)\n\n[1] 0.485\n\nsd(x) |> round(3)\n\n[1] 0.01\n\n\n\n\nWe can let Mathematica do the algebra"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "title": "Bayesian Inference",
    "section": "Posterior with Population Priors",
    "text": "Posterior with Population Priors\n\n\n\\(f(\\theta | y) = \\text{Beta}(1211 + 437, 1286 + 543) = \\text{Beta}(1648,1829)\\)\nWe can compare the prior and posterior using summarize_beta_binomial() in the bayesrules package\n\n\nlibrary(bayesrules)\nsummarize_beta_binomial(alpha = 1211, beta = 1286, y = 437, n = 980)\n\n      model alpha beta      mean      mode          var          sd\n1     prior  1211 1286 0.4849820 0.4849699 9.998978e-05 0.009999489\n2 posterior  1648 1829 0.4739718 0.4739568 7.168560e-05 0.008466735\n\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 1648, 1829) |> round(3)\nlower <- qbeta(l, 1648, 1829) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.457, 0.491]\n\nevent_prob <- integrate(dbeta, lower = 0, upper = 0.485, shape1 = 1648, shape2 = 1829)[[1]]\ncat(\"Probability that the ratio < 0.485 under population prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under population prior = 0.904\n\n\n\nUnder uniform prior 95% posterior interval was [0.415, 0.477]\nAnd probability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "title": "Bayesian Inference",
    "section": "Visualizing Bayesian Rebalancing",
    "text": "Visualizing Bayesian Rebalancing\n\nThe following uses \\(\\text{Beta}(5, 5)\\) prior and N = 10."
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-same-sex-marriage",
    "href": "bayes-course/02-lecture/02-lecture.html#example-same-sex-marriage",
    "title": "Bayesian Inference",
    "section": "Example: Same-sex marriage",
    "text": "Example: Same-sex marriage\n\nThis is Exercise 3.12, p 71 in your book\n\nA 2017 Pew Research survey found that 10.2% of LGBT adults in the U.S. were married to a same-sex spouse. Now it’s the 2020s, and Bayard guesses that \\(\\pi\\), the percent of LGBT adults in the U.S. who are married to a same-sex spouse, has most likely increased to about 15% but could reasonably range from 10% to 25%.\n\nIdentify and plot a Beta model that reflects Bayard’s prior ideas about \\(\\pi\\)\nBayard wants to update his prior, so he randomly selects 90 US LGBT adults and 30 of them are married to a same-sex partner. What is the posterior model for \\(\\pi\\)\nCalculate the posterior mean, mode, and standard deviation of \\(\\pi\\)\nDoes the posterior model more closely reflect the prior information or the data? Explain your reasoning."
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "href": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "title": "Bayesian Inference",
    "section": "Data Order and Batch Invariance",
    "text": "Data Order and Batch Invariance\n\n\nIn Bayesian analysis, we can update all at once or one datum at a time and everything in between and in any order (assuming exchangeable observations)\nThis is a general result, not just for Beta Binomial (see Section 4.5)\nIn practice, when we don’t have analytic posteriors, this is not so easy to do\nSuppose we observe \\(y = y_1 + y_2\\) and \\(N = N_1 + N_2\\) trials all at once\nAlso assume we start with \\(\\text{Beta}(1, 1)\\) prior\nFor all at once case, the posterior is in \\(f(\\theta \\mid y) = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\) as before\nNow, suppose we observe \\(y_1\\) successes in \\(n_1\\) trials first\nThe posterior is \\(f(\\theta \\mid y_1) = \\text{Beta}(\\alpha + y_1, \\,\\beta + N_1 - y_1)\\)\nWe now observe, \\(y_2\\) successes in \\(n_2\\) trials. The posterior is \\(f(\\theta \\mid y= y_1 + y_2) = \\text{Beta}(\\alpha + y_1 + y_2, \\,\\beta + N_1 - y_1 + N_2 - y_2)\\\\ = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\)\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course.html",
    "href": "bayes-course.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "This is the home for APSTA-GE 2123: Bayesian Inference class at NYU.\n\nThis is the current version of the syllbus: syllabus.pdf\nLecture 01: Introduction and Bayesian Workflow\nLecture 02: Conjugate models and Beta-Binomial"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Bayes and Stats-Math",
    "section": "",
    "text": "This website contains teaching materials for Bayesian analysis (APSTA-GE 2123) course and stats-math bootcamp, both taught at NYU. If you find any errors, please email me at eric.novik@nyu.edu."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for stopping by."
  }
]