[
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#introductions",
    "href": "bayes-course/01-lecture/01-lecture.html#introductions",
    "title": "Bayesian Inference",
    "section": "Introductions",
    "text": "Introductions\nWhat is your first name and where were you born?"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#lecture-1-bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Lecture 1: Bayesian Workflow",
    "text": "Lecture 1: Bayesian Workflow\n\n\nOverview of the Course\nStatistics vs AI/ML\nBrief history of Bayesian inference\nReview of basic probability\nIntroduction to Bayesian workflow\nBayes’s rule for events\nBinomial model and the Bayesian Crank\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathscr{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "href": "bayes-course/01-lecture/01-lecture.html#overview-of-the-class",
    "title": "Bayesian Inference",
    "section": "Overview of the class",
    "text": "Overview of the class\n\nSyllabus"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "href": "bayes-course/01-lecture/01-lecture.html#statistics-vs.-aiml",
    "title": "Bayesian Inference",
    "section": "Statistics vs. AI/ML",
    "text": "Statistics vs. AI/ML\n⚠️ What follows is an oversimplified opinion.\n\n\n\n\nAI is great for automating tasks that humans find easy\n\nRecognizing faces, cats, and other objects\nIdentifying tumors on a radiology scan\nPlaying Chess and Go\nDriving a car\nGenerating reasonable-sounding text\n\n\n\n\nStatistics is great at answering questions that humans find hard\n\nHow fast does a drug clear from the body?\nWhat is the expected tumor size 2 months after treatment?\nHow would patients respond under a different treatment?\nShould I take this drug?\n\n\n\n\n\n“Machine learning excels when you have lots of training data that can be reasonably modeled as exchangeable with your test data; Bayesian inference excels when your data are sparse and your model is dense.” — Andrew Gelman"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "href": "bayes-course/01-lecture/01-lecture.html#brief-history",
    "title": "Bayesian Inference",
    "section": "Brief History",
    "text": "Brief History\nSummary of the book The Theory That Would Not Die\n\n\n\n\nThomas Bayes (1702(?) — 1761) is credited with the discovery of the “Bayes’s Rule”\nHis paper was published posthumously by Richard Price in 1763\nLaplace (1749 — 1827) independently discovered the rule and published it in 1774\nScientific context: Newton’s Principia was published in 1687\nBayesian wins: German Enigma cipher, search for a missing H-bomb, Federalist papers, Moneyball, political forecasting, …\n\n\n\n\n\n\n\n\n\n\n\nStephen Stigler gives 3:1 in favor of Nicholas Saunderson for the discovery of Bayes’s [sic] rule."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "href": "bayes-course/01-lecture/01-lecture.html#laplaces-demon",
    "title": "Bayesian Inference",
    "section": "Laplace’s Demon",
    "text": "Laplace’s Demon\n\n\n\nWe may regard the present state of the universe as the effect of its past and the cause of its future. An intellect which at any given moment knew all of the forces that animate nature and the mutual positions of the beings that compose it, if this intellect were vast enough to submit the data to analysis, could condense into a single formula the movement of the greatest bodies of the universe and that of the lightest atom; for such an intellect nothing could be uncertain, and the future just like the past would be present before its eyes.\n\n\n\n\n\n\n\nMarquis Pierre Simon de Laplace (1729 — 1827)\n“Uncertainty is a function of our ignorance, not a property of the world.”"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "href": "bayes-course/01-lecture/01-lecture.html#modern-examples-of-bayesian-analyses",
    "title": "Bayesian Inference",
    "section": "Modern Examples of Bayesian Analyses",
    "text": "Modern Examples of Bayesian Analyses\n\n\nOpenAI DALL·E: Pierre Simon Laplace in the style of Wassily Kandinsky"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#vote-share-analysis",
    "title": "Bayesian Inference",
    "section": "Vote Share Analysis",
    "text": "Vote Share Analysis\n\n\nGelman, A. (2010). Breaking down the 2008 vote. In Atlas of the 2008 Elections."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#pharmacometrics",
    "href": "bayes-course/01-lecture/01-lecture.html#pharmacometrics",
    "title": "Bayesian Inference",
    "section": "Pharmacometrics",
    "text": "Pharmacometrics"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#medical-decision-making",
    "href": "bayes-course/01-lecture/01-lecture.html#medical-decision-making",
    "title": "Bayesian Inference",
    "section": "Medical Decision Making",
    "text": "Medical Decision Making\n\n\nJoensuu, H., Vehtari, A., et al. (2012). Risk of recurrence of gastrointestinal stromal tumour after surgery: An analysis of pooled population-based cohorts. The Lancet Oncology, 13(3), 265–274."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#review-of-probability",
    "title": "Bayesian Inference",
    "section": "Review of Probability",
    "text": "Review of Probability\n\n\nA set of all possible outcomes is called a sample space and denoted by \\(\\Omega\\)\nAn outcome of an experiment is denoted by \\(\\omega \\in \\Omega\\)\nWe typically denote events by capital letters, say \\(A \\subseteq \\Omega\\)\nAxioms of probability:\n\n\\(\\P(A) \\geq 0, \\, \\text{for all } A\\)\n\\(\\P(\\Omega) = 1\\)\nIf \\(A_1, A_2, A_3, \\ldots\\) are disjoint: \\(\\P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} \\P(A_i)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example",
    "href": "bayes-course/01-lecture/01-lecture.html#example",
    "title": "Bayesian Inference",
    "section": "Example",
    "text": "Example\n\nRolling diceOmega\n\n\n\n\nYou roll a fair six-sided die twice\nGive an example of an \\(\\omega \\in \\Omega\\)\nHow many elements are in \\(\\Omega\\)? What is \\(\\Omega\\)?\nDefine an event \\(A\\) as the sum of the two rolls less than 11\nHow many elements are in \\(A\\)?\nWhat is \\(\\P(A)\\)?\n\n\n\n\n\n\n\nouter(1:6, 1:6, FUN = \"+\")\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    2    3    4    5    6    7\n[2,]    3    4    5    6    7    8\n[3,]    4    5    6    7    8    9\n[4,]    5    6    7    8    9   10\n[5,]    6    7    8    9   10   11\n[6,]    7    8    9   10   11   12\n\n\n\n\n\n\\(\\Omega\\) consists of all pairs \\(\\{(1, 1), (1, 2), ... (2, 1), (2, 2), ...\\}\\)\nThere are 36 such pairs\n33 of those pairs result in a sum of less than 11\nTherefore, \\(\\P(A) = \\frac{33}{36} = \\frac{11}{12}\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "href": "bayes-course/01-lecture/01-lecture.html#random-variables-review",
    "title": "Bayesian Inference",
    "section": "Random Variables Review",
    "text": "Random Variables Review\n\nReviewMapping\n\n\n\n\nRandom variable is not random – it is a deterministic mapping from the sample space onto the real line; randomness comes from the experiment\nPMF, PDF, CDF (Blitzstein and Hwang, Ch. 3, 5)\nExpectations (Blitzstein and Hwang, Ch. 4)\nJoint Distributions (Blitzstein and Hwang, Ch. 7)\nConditional Expectations (Blitzstein and Hwang, Ch. 9)\n\n\n\n\n\nRandom variable X for the number of Heads in two flips"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "href": "bayes-course/01-lecture/01-lecture.html#example-simulation",
    "title": "Bayesian Inference",
    "section": "Example Simulation",
    "text": "Example Simulation\n\n\nWe will use R’s sample() function to simulate rolls of a die and replicate() function to repeat the rolling process many times\nModern approach: purrr::map(1:n, \\(i) expression)\n\n\n\n\n\ndie <- 1:6\nroll <- function(x, n) {\n  sample(x, size = n, replace = TRUE)\n}\nroll(die, 2) # roll the die twice\n\n[1] 1 2\n\nrolls <- replicate(1e4, roll(die, 2))\nrolls[, 1:6]\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    5    5    5    1    2    5\n[2,]    4    5    4    5    3    3\n\nroll_sums <- colSums(rolls)\nhead(roll_sums)\n\n[1]  9 10  9  6  5  8\n\nmean(roll_sums < 11) \n\n[1] 0.9165\n\n\n\n\n\nGiven a Random Variable \\(Y\\), \\(y^{(1)}, y^{(2)}, y^{(3)}, \\ldots, y^N\\) are simulations or draws from \\(Y\\)\nFundamental bridge (Blitzstein & Hwang p. 164): \\(\\P(A) = \\E(\\I_A)\\)\nComputationally: \\(\\P(Y < 11) \\approx \\frac{1}{N} \\sum^{N}_{n=1} \\I(y^n < 11)\\)\nIn R, roll_sums < 11 creates an indicator variable\nAnd mean() does the average"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#conditional-probability",
    "title": "Bayesian Inference",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\nFor arbitrary \\(A\\) and \\(B\\), if \\(\\P(B) > 0\\):\n\nConditional probability: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)}\\)\nConditional probability: \\(\\P(B \\mid A) = \\frac{\\P(AB)}{\\P(A)}\\)\n\nBayes’s rule: \\(\\P(A \\mid B) = \\frac{\\P(AB)}{\\P(B)} = \\frac{\\P(B \\mid A) \\P(A)}{\\P(B)}\\)\n\\(A\\) and \\(B\\) are independent if observing \\(B\\) does not give you any more information about \\(A\\): \\(\\P(A \\mid B) = \\P(A)\\)\nAnd \\(\\P(A B) = \\P(A) \\P(B)\\) (from Bayes’s rule)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\nTitanic carried approximately 2,200 passengers and sank on 15 April 1912\nLet \\(G: \\{m, f\\}\\) represent Gender and \\(S: \\{n, y\\}\\) represent Survival\n\n\n\n\nsurv <- apply(Titanic, c(2, 4), sum) |> \n  as_tibble()\nsurv_prop <- round(surv / sum(surv), 3)\nbind_cols(Sex = c(\"Male\", \"Female\"), \n          surv_prop) |> \n  adorn_totals(c(\"row\", \"col\")) |>\n  knitr::kable(caption = \n               \"Titanic survival proportions\")\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n\n\n\n\\(\\P(G = m \\cap S = y) =\\) 0.167\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) =\\) ??\n\\(\\P(S = y) = \\sum_{i \\in \\{m, f\\}} \\P(S = y \\, \\cap G = i) = 0.323\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "href": "bayes-course/01-lecture/01-lecture.html#joint-marginal-and-conditional-1",
    "title": "Bayesian Inference",
    "section": "Joint, Marginal, and Conditional",
    "text": "Joint, Marginal, and Conditional\n\n\n\n\nWhat is \\(\\P(G = m \\mid S = y)\\), probability of being male given survival?\nTo compute that, we only consider the column  where S = Yes\n\\(\\P(G = m \\mid S = y) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(S = y)} = \\frac{0.167}{0.323} \\\\ \\approx 0.52\\)\nYou want \\(\\P(S = y \\mid G = m)\\), comparing it to \\(\\P(S = y \\mid G = f)\\)\n\\(\\P(S = y \\mid G = m) = \\frac{\\P(G = m \\, \\cap \\, S = y)}{\\P(G = m)} = \\frac{0.167}{0.787} \\\\ \\approx 0.21\\)\n\\(\\P(S = y \\mid G = f) = \\frac{\\P(G = f \\, \\cap \\, S = y)}{\\P(G = f)} = \\frac{0.156}{0.213} \\\\ \\approx 0.73\\)\nHow would you calculate \\(\\P(S = n \\mid G = m)\\)?\n\\(\\P(S = n \\mid G = m) = 1 - \\P(S = y \\mid G = m)\\)\n\n\n\n\n\n\nTitanic survival proportions\n\n\nSex\nNo\nYes\nTotal\n\n\n\n\nMale\n0.620\n0.167\n0.787\n\n\nFemale\n0.057\n0.156\n0.213\n\n\nTotal\n0.677\n0.323\n1.000\n\n\n\n\n\n “Untergang der Titanic”, as conceived by Willy Stöwer, 1912"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "href": "bayes-course/01-lecture/01-lecture.html#law-of-total-probability-lotp",
    "title": "Bayesian Inference",
    "section": "Law of Total Probability (LOTP)",
    "text": "Law of Total Probability (LOTP)\n\\[\n\\P(B) = \\sum_{i=1}^{n} \\P(B \\cap A_i) = \\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)\n\\]\n\n\nImage from Blitzstein and Hwang (2019), Page 55"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "href": "bayes-course/01-lecture/01-lecture.html#bayess-rule-for-events",
    "title": "Bayesian Inference",
    "section": "Bayes’s Rule for Events",
    "text": "Bayes’s Rule for Events\n\n\nWe can combine the definition of conditional probability with the LOTP to come up with Bayes’s rule for events, assuming \\(\\P(B) \\neq 0\\)\n\n\\[\n\\P(A \\mid B) = \\frac{\\P(B \\cap A)}{\\P(B)} =\n               \\frac{\\P(B \\mid A) \\P(A)}{\\sum_{i=1}^{n} \\P(B \\mid A_i) \\P(A_i)}\n\\]\n\nWe typically think of \\(A\\) is some unknown we wish to learn (e.g., the status of a disease) and \\(B\\) as the data we observe (e.g., the result of a diagnostic test)\nWe call \\(\\P(A)\\) prior probability of A (e.g., how prevalent is the disease in the population)\nWe call \\(\\P(A \\mid B)\\), the posterior probability of the unknown \\(A\\) given data \\(B\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "href": "bayes-course/01-lecture/01-lecture.html#example-medical-testing",
    "title": "Bayesian Inference",
    "section": "Example: Medical Testing",
    "text": "Example: Medical Testing\n\nThe authors calculated the sensitivity and specificity of the Abbott PanBio SARS-CoV-2 rapid antigen test to be 45.4% and 99.8%, respectively. Suppose the prevalence is 0.1%.\n\n\nYour child tests positive on this test. What is the probability that she has COVID? That is, we want to know \\(\\P(D^+ \\mid T^+)\\)\n\\(\\text{Specificity } := \\P(T^- \\mid D^-) = 0.998\\)\nFalse positive rate \\(\\text{FP} := 1 - \\text{Specificity } = 1 - \\P(T^- \\mid D^-) = \\P(T^+ \\mid D^-) = 0.002\\)\n\\(\\text{Sensitivity } := \\P(T^+ \\mid D^+) = 0.454\\)\nFalse negative rate \\(\\text{FP} := 1 - \\text{Sensitivity } = 1 - \\P(T^+ \\mid D^+) = \\P(T^- \\mid D^+) = 0.546\\)\nPrevalence: \\(\\P(D^+) = 0.001\\)\n\n\\[\n\\begin{eqnarray}\n\\P(D^+ \\mid T^+) = \\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+)} & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\sum_{i=1}^{n}\\P(T^+ \\mid D^i) \\P(D^i) } & = & \\\\\n\\frac{\\P(T^+ \\mid D^+) \\P(D^+)}{\\P(T^+ \\mid D^+) \\P(D^+) + \\P(T^+ \\mid D^-) \\P(D^-)} & = & \\\\\n\\frac{0.454 \\cdot 0.001}{0.454 \\cdot 0.001 + 0.002 \\cdot 0.999} & \\approx & 0.18\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-analysis",
    "title": "Bayesian Inference",
    "section": "Bayesian Analysis",
    "text": "Bayesian Analysis\n\n\n\n\nSuppose, we enrolled 5 people in an early cancer clinical trial\nEach person was given an active treatment\nFrom previous trials, we have some idea of historical response rates (proportion of people responding)\nAt the end of the trial, \\(Y = y \\in \\{0,1,...,5 \\}\\) people will have responded1 to the treatment\nWe are interested in estimating the probability that the response rate is greater great or equal to 50%\n\n\n\n Image from Fokko Smits, Martijn Dirksen, and Ivo Schoots: RECIST 1.1 - and more\n\n\nPartial response is a decrease in the sum of the longest diameters of target lesions of at least 30% compared to the baseline measurement."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "href": "bayes-course/01-lecture/01-lecture.html#notation-confusion",
    "title": "Bayesian Inference",
    "section": "Notation Confusion",
    "text": "Notation Confusion\n\n\n\\(\\theta\\): unknowns or parameters to be estimated; could be multivariate, discrete, and continuous (your book uses \\(\\pi\\))\n\\(y\\): observations or measurements to be modelled (\\(y_1, y_2, ...\\))\n\\(\\widetilde{y}\\) : unobserved but observable quantities (in your book \\(y'\\))\n\\(x\\): covariates\n\\(f( \\theta )\\): a prior model, P[DM]F of \\(\\theta\\)\n\\(f_y(y \\mid \\theta, x)\\): an observational model, P[DM]F when it is a function of \\(y\\) (in your book: \\(f(y \\mid \\pi)\\)); we typically drop the \\(x\\) to simplify the notation.\n\\(f_{\\theta}(y \\mid \\theta)\\): is a likelihood function when it is a function of \\(\\theta\\) (in your book: \\(\\L(\\pi \\mid y)\\))\nSome people write \\(\\L(\\theta; y)\\) or simply \\(\\L(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "href": "bayes-course/01-lecture/01-lecture.html#general-approach",
    "title": "Bayesian Inference",
    "section": "General Approach",
    "text": "General Approach\n\n\nBefore observing the data, we need to specify a prior model \\(f(\\theta)\\) on all unknowns \\(\\theta\\)1\nPick a data model \\(f(y | \\theta, x)\\) — this is typically more important than the prior; this includes the model for the conditional mean: \\(\\E(y | x)\\)\nFor more complex models, we construct a prior predictive distribution, \\(f(y)\\)\n\nWe will define this quantity later — it will help us assess if our choice of priors makes sense on the observational scale: \\(f(\\theta) \\rightarrow f(y)\\)\n\nAfter we observe data \\(y\\), we treat \\(f(y | \\theta, x)\\) as the likelihood of observing \\(y\\) under all plausible values of \\(\\theta\\)\nDerive a posterior model for \\(\\theta\\), \\(\\, f(\\theta | y, x)\\) using Bayes’s rule or by simulation\nEvaluate model quality: 1) quality of the inferences; 2) quality of predictions. Revise the model if necessary\nCompute all the quantities of interest from the posterior, such as event probabilities, e.g., \\(\\P(\\theta > 0.5)\\), posterior predictive distribution \\(f(\\widetilde{y} | y)\\), decision functions, etc.\n\n\n\nFor a more complete workflow, see Bayesian Workflow by Gelman et al. (2020)\n\nThere is always a prior, even in frequentist inference."
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "href": "bayes-course/01-lecture/01-lecture.html#example-prior-model",
    "title": "Bayesian Inference",
    "section": "Example Prior Model",
    "text": "Example Prior Model\n\n\nWe will construct a prior model for our clinical trial\nFrom previous trials, we construct a discretized version of the prior distribution of the resposne rate\nThe most likely value for response rate is 30%\n\n\n\ndot_plot <- function(x, y) {\n  p <- ggplot(data.frame(x, y), aes(x, y))\n  p + geom_point(aes(x = x, y = y), size = 0.5) +\n    geom_segment(aes(x = x, y = 0, xend = x, \n                     yend = y), linewidth = 0.2) +\n    xlab(expression(theta)) + \n    ylab(expression(f(theta)))\n}\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\ndot_plot(theta, prior) +\n  ggtitle(\"Prior probability of response\")\n\n\n\n\n\n\n\n\n\n\n\nEven though \\(\\theta\\) is a continuous parameter, we can still specify a discrete prior\nThe posterior will also be discrete and of the same cardinality"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#data-model",
    "href": "bayes-course/01-lecture/01-lecture.html#data-model",
    "title": "Bayesian Inference",
    "section": "Data Model",
    "text": "Data Model\n\n\nWe consider each person’s response rate to be independent, given the treatment\nWe have a fixed number of people in the trial, \\(N = 5\\), and \\(0\\) to \\(5\\) successes\nWe will therefore consider: \\(y | \\theta \\sim \\text{Bin}(N,\\theta) = \\text{Bin}(5,\\theta)\\)\n\\(f(y \\mid \\theta) = \\text{Bin} (y \\mid 5,\\theta) = \\binom{5}{y} \\theta^y (1 - \\theta)^{5 - y}\\) for \\(y \\in \\{0,1,\\ldots,5\\}\\)\nIs this a valid probability distribution as a function of \\(y\\)?\n\n\nN = 5; y <- 0:N; theta <- 0.5\n(f_y <- dbinom(x = y, size = N, prob = theta) |>\n    fractions())\n\n[1] 1/32 5/32 5/16 5/16 5/32 1/32\n\nsum(dbinom(x = y, size = N, prob = theta))\n\n[1] 1"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\nWe ran the trial and observed 3 out of 5 responders\nWe can now construct a likelihood function for \\(y = 3\\) as a function of \\(\\theta\\)\nLet’s check if this function is a probability distribution: \\[\nf (y) = \\int_{0}^{1} \\binom{N}{y} \\theta^y (1 - \\theta)^{N - y}\\, d\\theta = \\frac{1}{N + 1}\n\\]\n\n\ndbinom_theta <- function(theta, N, y) {\n  choose(N, y) * theta^y * (1 - theta)^(N - y) \n}\nintegrate(dbinom_theta, lower = 0, upper = 1, \n          N = 5, y = 3)[[1]] |> fractions()\n\n[1] 1/6\n\n\n\n\\(f(y)\\) is called marginal distribution of the data or, more aptly, prior predictive distribution\nIt tells us that prior to observing \\(y\\), all values of \\(y\\) are equally likely"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "href": "bayes-course/01-lecture/01-lecture.html#likelihood-function-1",
    "title": "Bayesian Inference",
    "section": "Likelihood Function",
    "text": "Likelihood Function\n\n\n\n\n\n\n\n\n\n\nCompare with the data model"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "href": "bayes-course/01-lecture/01-lecture.html#compute-the-posterior",
    "title": "Bayesian Inference",
    "section": "Compute the Posterior",
    "text": "Compute the Posterior\n\n\n\n\nFirst, we compute the likelihood\n\n\\(\\binom{5}{3} \\theta^3 (1 - \\theta)^{2}\\) for \\(\\theta \\in \\{0.10, 0.30, 0.50, 0.70, 0.90\\}\\)\n\nNext, we multiply the likelihood by the prior, to get the numerator\n\n\\(f(y = 3 \\mid \\theta) f(\\theta)\\)\n\nSum the numerator to get the marginal likelihood\n\n\\(f(y = 3) = \\sum_{\\theta} f(y = 3 | \\theta) f(\\theta) \\approx 0.2\\)\n\nFinally, compute the posterior\n\n\\(f(\\theta \\mid y = 3) = \\frac{f(y = 3 \\mid \\theta) f(\\theta)}{\\sum_{\\theta} f(y = 3 | \\theta) f(\\theta)}\\)\n\n\n\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.05, 0.45, 0.30, 0.15, 0.05)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "href": "bayes-course/01-lecture/01-lecture.html#computing-event-probability",
    "title": "Bayesian Inference",
    "section": "Computing Event Probability",
    "text": "Computing Event Probability\n\n\n\n\nTo compute event probabilities, we integrate (or sum) the relevant regions of the parameter space \\[\n\\P(\\theta \\geq 0.5) = \\int_{0.5}^{1} f(\\theta \\mid y) \\, d\\theta\n\\]\nIn this case, we only have discrete quantities, so we sum:\n\n\n\nprobs <- d |>\n  filter(theta >= 0.50) |>\n  dplyr::select(prior, post) |>\n  colSums() |>\n  round(2)\nprobs\n\nprior  post \n 0.50  0.71 \n\n\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.05 \n    0.01 \n    0.00 \n    0.00 \n  \n  \n    0.3 \n    0.45 \n    0.13 \n    0.06 \n    0.29 \n  \n  \n    0.5 \n    0.30 \n    0.31 \n    0.09 \n    0.46 \n  \n  \n    0.7 \n    0.15 \n    0.31 \n    0.05 \n    0.23 \n  \n  \n    0.9 \n    0.05 \n    0.07 \n    0.00 \n    0.02 \n  \n  \n    Total \n    1.00 \n    0.83 \n    0.20 \n    1.00 \n  \n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.5 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.71"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "href": "bayes-course/01-lecture/01-lecture.html#what-if-we-used-a-flat-prior",
    "title": "Bayesian Inference",
    "section": "What If We Used a Flat Prior?",
    "text": "What If We Used a Flat Prior?\n\nFlat or uniform prior means that we consider all values of \\(\\theta\\) equality likely\n\n\nN <- 5; y <- 3\ntheta <- c(0.10, 0.30, 0.50, 0.70, 0.90)\nprior <- c(0.20, 0.20, 0.20, 0.20, 0.20)\nlik <- dbinom(y, N, theta)\nlik_x_prior <-  lik * prior\nconstant <- sum(lik_x_prior)\npost <- lik_x_prior / constant\n\n\n\n\n\n \n  \n    theta \n    prior \n    lik \n    lik_x_prior \n    post \n  \n \n\n  \n    0.1 \n    0.2 \n    0.01 \n    0.00 \n    0.01 \n  \n  \n    0.3 \n    0.2 \n    0.13 \n    0.03 \n    0.16 \n  \n  \n    0.5 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.7 \n    0.2 \n    0.31 \n    0.06 \n    0.37 \n  \n  \n    0.9 \n    0.2 \n    0.07 \n    0.01 \n    0.09 \n  \n  \n    Total \n    1.0 \n    0.83 \n    0.17 \n    1.00 \n  \n\n\n\n\n\n\n\n\n\n\\(\\P(\\theta \\geq 0.50) =\\) 0.6 and \\(\\P(\\theta | y \\geq 0.50) =\\) 0.83"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "href": "bayes-course/01-lecture/01-lecture.html#bayesian-workflow",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\n\nGelman, A. et al. (2020). Bayesian Workflow. ArXiv:2011.01808 [Stat]. http://arxiv.org/abs/2011.01808"
  },
  {
    "objectID": "bayes-course/01-lecture/01-lecture.html#homework",
    "href": "bayes-course/01-lecture/01-lecture.html#homework",
    "title": "Bayesian Inference",
    "section": "Homework",
    "text": "Homework\nHomework is due on Monday before 5 pm.\n\nExercise 2.3 (Binomial practice)\nExercise 2.10 (LGBTQ students: rural and urban)\nExercise 2.15 (Cuckoo birds)\nExercise 2.19 (Cuckoo birds redux)\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-models-and-posterior-sampling",
    "title": "Bayesian Inference",
    "section": "Conjugate Models and Posterior Sampling",
    "text": "Conjugate Models and Posterior Sampling\n\n\nGamma-Poisson family\nNormal-Normal family\nIntroduction to posterior sampling on a grid\nIntroduction to Stan\nBasic Markov Chain diagnostics\nEffective sample size\nComputing the R-Hat statistic\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson",
    "title": "Bayesian Inference",
    "section": "Poisson",
    "text": "Poisson\n\n\nPoisson distribution arises when we are interested in counts\nIn practice, we rarely use Poisson due to its restrictive nature\nPoisson RV \\(Y\\) is paremeterized with a rate of occurance \\(\\lambda\\): \\(Y|\\lambda \\sim \\text{Pois}(\\lambda)\\)\n\\[\nf(y \\mid \\lambda) =  \\frac{e^{-\\lambda} \\lambda^y}{y!}\\;\\; \\text{ for } y \\in \\{0,1,2,\\ldots\\}\n\\]\nNotice, the Tailor series for \\(e^\\lambda = \\sum_{y=0}^{\\infty} \\frac{\\lambda^y}{y!}\\) immediately validates that \\(f\\) is a PDF\nAlso, \\(\\E(Y | \\lambda) = \\V(Y | \\lambda) = \\lambda\\), which is the restrictive case mentioned about — real count data seldom satisfied this constraint"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-poission",
    "title": "Bayesian Inference",
    "section": "Visualizing Poission",
    "text": "Visualizing Poission\n\n\nNotice that as the rate increases, so does the expected number of events as well as variance, which immediately follows from \\(\\E(Y) = \\V(Y) = \\lambda\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#motivating-example",
    "href": "bayes-course/03-lecture/03-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nServing in Prussian cavalry in the 1800s was a perilous affair\nAside from the usual dangers of military service, you were at risk of being killed by a horse kick\nData from the book The Law of Small Numbers by Ladislaus Bortkiewicz (1898)\nBortkiewicz was a Russian economist and statistician of Polish ancestry"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#poisson-likelihood",
    "title": "Bayesian Inference",
    "section": "Poisson Likelihood",
    "text": "Poisson Likelihood\n\n\nAssume we observe \\(Y_1, Y_2, ..., Y_n\\) independant Poisson random variables\nThe joint lilelihood, a function of the parameter \\(\\lambda\\) for \\(y_i \\in \\mathbb{Z}^+\\) and \\(\\lambda > 0\\), is given by: \\[\n\\begin{eqnarray}\nf(y \\mid \\lambda) & = & \\prod_{i=1}^n f(y_i \\mid \\lambda) = f(y_1 \\mid \\lambda)  f(y_2 \\mid \\lambda) \\cdots f(y_n \\mid \\lambda)  =  \\prod_{i=1}^{n}\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\\\\n& = &\\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\cdot \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\cdots \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n& =  &\\frac{\\left(\\lambda^{y_1}\\lambda^{y_2} \\cdots \\lambda^{y_n}\\right) \\left(e^{-\\lambda}e^{-\\lambda} \\cdots e^{-\\lambda}\\right)}{y_1! y_2! \\cdots y_n!} \\\\\n& = &\\frac{\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i!} \\propto\n\\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\n\\end{eqnarray}\n\\]\nWe call \\(\\sum_{i=1}^{n} y_i\\) a sufficient statistic"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "href": "bayes-course/03-lecture/03-lecture.html#conjugate-prior-for-poisson",
    "title": "Bayesian Inference",
    "section": "Conjugate Prior for Poisson",
    "text": "Conjugate Prior for Poisson\n\n\nThe likelihood has a form of \\(\\lambda^{x} e^{-y\\lambda}\\) so we expect the conjugate prior to be of the same form\nGamma PDF satisfied this condition: \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nMatching up the exponents, we can interpret \\((\\alpha - 1)\\) as the total number of incidents \\(\\sum y_i\\) out of \\(\\beta\\) prior observations\nFull Gamma density is \\(\\text{Gamma}(\\lambda|\\alpha,\\beta)=\\frac{\\beta^{\\alpha}} {\\Gamma(\\alpha)} \\, \\lambda^{\\alpha - 1}e^{-\\beta \\, \\lambda}\\) for \\(\\lambda, \\alpha, \\beta \\in \\mathbb{R}^+\\) \\[\n\\begin{equation}\n\\begin{split}\n\\E(\\lambda) & = \\frac{\\alpha}{\\beta} \\\\\n\\text{Mode}(\\lambda) & = \\frac{\\alpha - 1}{\\beta} \\;\\; \\text{ for } \\alpha \\ge 1 \\\\\n\\V(\\lambda) & = \\frac{\\alpha}{\\beta^2} \\\\\n\\end{split}\n\\end{equation}\n\\]\nWhen \\(\\alpha = 1\\), \\(\\lambda \\sim \\text{Exp}(\\beta)\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "href": "bayes-course/03-lecture/03-lecture.html#visualizing-gamma",
    "title": "Bayesian Inference",
    "section": "Visualizing Gamma",
    "text": "Visualizing Gamma\n\n\nNotice that variance, mean, and mode are increasing with \\(\\alpha\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#gamma-posterior",
    "title": "Bayesian Inference",
    "section": "Gamma Posterior",
    "text": "Gamma Posterior\n\n\nPrior is \\(f(\\lambda) \\propto \\lambda^{\\alpha - 1} e^{-\\beta\\lambda}\\)\nLikelihood is \\(f(y | \\lambda) \\propto \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda}\\) \\[\n\\begin{eqnarray}\nf(\\lambda \\mid y) & \\propto & \\text{prior} \\cdot \\text{likelihood} \\\\\n& = & \\lambda^{\\alpha - 1} e^{-\\beta\\lambda} \\cdot  \\lambda^{\\sum_{i=1}^{n} y_i}e^{-n\\lambda} \\\\\n& = & \\lambda^{\\alpha + \\sum_{i=1}^{n} y_i - 1} \\cdot e^{-\\beta\\lambda - n\\lambda} \\\\\n& = & \\lambda^{\\color{red}{\\alpha + \\sum_{i=1}^{n} y_i} - 1} \\cdot e^{-\\color{red}{(\\beta + n)} \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma}\\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + n \\right)\n\\end{eqnarray}\n\\]\nAs before, we can guess the Gamma kernel without doing the integration"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-constant",
    "title": "Bayesian Inference",
    "section": "Checking the Constant",
    "text": "Checking the Constant\n\n\nGamma prior integration constant: \\(\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\)\nThe posterior kernel integration constant must be: \\(\\frac{\\Gamma(\\alpha + \\sum y_i)}{(\\beta + n)^{\\alpha + \\sum y_i}}\\)\nWe can sanity check this in Mathematica where \\(t = \\sum y_i\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n\nFrom 1875 to 1894, there were 14 different cavalry corps\nEach reported a number of deaths by horse kick every year\nThere are 20 years x 14 corps making 280 observations\n\n\n\n\nlibrary(gt)\nd <- vroom::vroom(\"data/horsekicks.csv\")\nd |> filter(Year < 1883) |> gt()\n\n\n\n\n\n  \n    \n    \n      Year\n      GC\n      C1\n      C2\n      C3\n      C4\n      C5\n      C6\n      C7\n      C8\n      C9\n      C10\n      C11\n      C14\n      C15\n    \n  \n  \n    1875\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n    1876\n2\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n    1877\n2\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\n2\n0\n    1878\n1\n2\n2\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n    1879\n0\n0\n0\n1\n1\n2\n2\n0\n1\n0\n0\n2\n1\n0\n    1880\n0\n3\n2\n1\n1\n1\n0\n0\n0\n2\n1\n4\n3\n0\n    1881\n1\n0\n0\n2\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n    1882\n1\n2\n0\n0\n0\n0\n1\n0\n1\n1\n2\n1\n4\n1"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-1",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\n\n# this flattens the data frame into a vector\ndd <- unlist(d[, -1]) \np <- ggplot(aes(y), data = data.frame(y = dd))\np1 <- p + geom_histogram() +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Total reported counts of deaths\")\np2 <- p + geom_histogram(aes(y = ..count../sum(..count..))) +\n  xlab(\"Number of deaths reported\") + ylab(\"\") +\n  ggtitle(\"Proportion of reported counts of deaths\")\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "href": "bayes-course/03-lecture/03-lecture.html#prussian-army-hourse-kicks-2",
    "title": "Bayesian Inference",
    "section": "Prussian Army Hourse Kicks",
    "text": "Prussian Army Hourse Kicks\n\nPrior and PosteriorPlots\n\n\n\n\nLet’s assume that before seeing the data, your friend told you that last year, in 1874, there were no deaths reported in his corps\nThat would imply \\(\\beta = 1\\) and \\(\\alpha - 1 = 0\\) or \\(\\alpha = 1\\)\nThe prior on lambda would therefore be \\(\\text{Gamma}(1, 1)\\)\n\n\n\nN <- length(dd)\nsum_yi <- sum(dd)\ny_bar <- sum_yi/N\ncat(\"Total number of observations N =\", N)\n\nTotal number of observations N = 280\n\ncat(\"Total number of deaths =\", sum_yi)\n\nTotal number of deaths = 196\n\ncat(\"Average number of deaths =\", y_bar)\n\nAverage number of deaths = 0.7\n\n\n\n\nThe posterior is \\(\\text{Gamma}\\left( \\alpha + \\sum y_i, \\, \\beta + N \\right) = \\text{Gamma}(197, \\, 281)\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "href": "bayes-course/03-lecture/03-lecture.html#likelihood-dominates",
    "title": "Bayesian Inference",
    "section": "Likelihood Dominates",
    "text": "Likelihood Dominates\n\n\n\n\nWith so much data relative to prior observations, the likelihood completely dominates the prior\n\n\n\n\nplot_gamma_poisson(1, 1, sum_y = sum_yi, n = N) + \n  xlim(0, 3)\n\n\n\n\n\n\n\n\n\n\n\n\nmap <- (1 + sum_yi - 1) / (1 + N) \nsd_post <- sqrt((1 + sum_yi) / (1 + N)^2) \ncat(\"Mode or MAP =\", map |> round(3))\n\nMode or MAP = 0.698\n\ncat(\"Average rate =\", y_bar)\n\nAverage rate = 0.7\n\ncat(\"Posterior sd =\", sd_post |> round(2))\n\nPosterior sd = 0.05"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "href": "bayes-course/03-lecture/03-lecture.html#checking-the-fit",
    "title": "Bayesian Inference",
    "section": "Checking the Fit",
    "text": "Checking the Fit\n\n\nWe can plug the MAP estimate for \\(\\lambda\\) into the Poisson PMF\n\n\n\n\nmap <- (1 + sum_yi - 1) / (1 + N)\ndeaths <- 0:4\npred <- dpois(deaths, lambda = map)\nactual <- as.numeric(table(dd) / N)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "href": "bayes-course/03-lecture/03-lecture.html#adding-exposure",
    "title": "Bayesian Inference",
    "section": "Adding Exposure",
    "text": "Adding Exposure\n\n\nPoisson likelihood seems to work well for these data\nIt is likely that each of the 14 corps had about the same number of soldier-horses, a common military practice\nSuppose each corps has a different number of cavalrymen\nWe need to introduce an exposure variable \\(x_i\\) for each corps unit \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(x_i \\lambda)\\\\\n\\lambda & \\sim & \\text{Gamma}(\\alpha, \\beta) \\\\\nf(y \\mid \\lambda) & \\propto & \\lambda^{\\sum_{i=1}^{n} y_i}e^{- (\\sum_{i=1}^{n} x_i) \\lambda} \\\\\nf(\\lambda \\mid y) & = & \\text{Gamma} \\left( \\alpha + \\sum_{i=1}^{n} y_i, \\, \\beta + \\sum_{i=1}^{n} x_i \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\nThe last conjugate distribution we will introduce is Normal\nWe will only consider a somewhat unrealistic case of known variance \\(\\sigma \\in \\mathbb{R}^+\\) and unknown mean \\(\\mu \\in \\mathbb{R}\\)\nNormal PDF is for one observation \\(y\\) is given by: \\[\n\\begin{eqnarray}\n\\text{Normal}(y \\mid \\mu,\\sigma) & = &\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\E(Y) & = & \\text{ Mode}(Y) = \\mu \\\\\n\\V(Y) & = & \\sigma^2 \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-pdf-1",
    "title": "Bayesian Inference",
    "section": "Normal PDF",
    "text": "Normal PDF\n\n\nNormal arises when many independent small contributions are added up\nIt is a limiting distribution of means of an arbitrary distribution\n\n\n\n\n\nplot_xbar <- function(n_repl, n_samples) {\n  x <- seq(0.6, 1.4, len = 100)\n  xbar <- replicate(n_repl, \n                    mean(rexp(n_samples, rate = 1)))\n  mu <- dnorm(x, mean = 1, sd = 1/sqrt(n_samples))\n  p <- ggplot(aes(x = xbar), \n              data = tibble(xbar))\n  p + geom_histogram(aes(y = ..density..), \n                     bins = 30, alpha = 0.6) +\n    geom_line(aes(x = x, y = mu), \n              color = 'red', \n              linewidth = 0.3, \n              data = tibble(x, y)) +\n    ylab(\"\") + theme(axis.text.y = element_blank())\n}\n\np1 <- plot_xbar(1e4, 100) + \n  ggtitle(\"Sampling means from rexp(100, 1)\")\np2 <- plot_xbar(1e4, 300) + \n  ggtitle(\"Sampling means from rexp(300, 1)\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "href": "bayes-course/03-lecture/03-lecture.html#joint-normal-likelihood",
    "title": "Bayesian Inference",
    "section": "Joint Normal Likelihood",
    "text": "Joint Normal Likelihood\n\n\nAfter observing data \\(y\\), we can compute the joint normal likelihood, assuming \\(\\sigma\\) is known \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n& \\propto & \\prod_{i=1}^{n} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2 \\right) \\\\  \n& = & \\exp \\left( {-\\frac{\\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2}}\\right) \\\\\n&\\propto& \\exp\\left({-\\frac{(\\bar{y}-\\mu)^2}{2\\sigma^2/n}}\\right)\n\\end{eqnarray}\n\\]\nThe last line is derived by expanding the square and dropping terms that don’t depend on \\(\\mu\\); \\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-prior",
    "title": "Bayesian Inference",
    "section": "Normal Prior",
    "text": "Normal Prior\n\n\nWe can now define the prior on \\(\\mu\\)\nWe will choose \\(\\mu\\) to be normal: \\(\\mu \\sim \\text{Normal}(\\theta, \\tau^2)\\) \\[\n\\begin{eqnarray}\nf(\\mu \\mid \\theta, \\tau) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-posterior",
    "title": "Bayesian Inference",
    "section": "Normal Posterior",
    "text": "Normal Posterior\n\nFor one observation \\(y\\):\n\n\\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\nf(\\mu) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 \\right) \\\\\nf(\\mu \\mid y) & \\propto &  \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{\\mu - \\theta}{\\tau} \\right)^2 - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\\\\n& = & \\exp \\left( -\\frac{1}{2} \\left(  \\frac{(\\mu - \\theta)^2}{\\tau^2} + \\frac{(y - \\mu)^2}{\\sigma^2} \\right)\\right) \\\\\n& = &  \\exp \\left(  -\\frac{1}{2\\tau_1^2} \\left( \\mu - \\mu_1 \\right)^2    \\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#normal-posterior-1",
    "href": "bayes-course/03-lecture/03-lecture.html#normal-posterior-1",
    "title": "Bayesian Inference",
    "section": "Normal Posterior",
    "text": "Normal Posterior\n\nFor one observation: \\[\n\\begin{eqnarray}\nf(\\mu \\mid y) &\\propto& \\exp \\left(  -\\frac{1}{2\\tau_1^2} \\left( \\mu - \\mu_1 \\right)^2    \\right) \\\\\n\\mu_1 &=& \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{1}{\\sigma^2} y}{\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} &=& \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}\n\\end{eqnarray}\n\\]\nFor multiple observations: \\[\n\\mu_1 = \\frac{\\frac{1}{\\tau^2} \\theta + \\frac{n}{\\sigma^2} \\overline{y}}{\\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}} \\\\\n\\frac{1}{\\tau_1^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}\n\\]"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#posterior-simulation",
    "title": "Bayesian Inference",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nAriannaThe Paper\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArianna Rosenbluth Dies at 93, The New York Times"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\n\n\nMost posterior distributions do not have an analytical form\nIn those cases, we must resort to sampling methods\nSampling in high dimensions requires specialized algorithms\nHere, we will look at one-dimensional sampling on the grid (of parameter values)\nAt the end of this lecture, we look at some output of a state-of-the-art HMC NUTS sampler\nNext week, we will examine the Metropolis-Hastings-Rosenbluth algorithm"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "href": "bayes-course/03-lecture/03-lecture.html#grid-approximation-1",
    "title": "Bayesian Inference",
    "section": "Grid approximation",
    "text": "Grid approximation\n\n\nWe already saw how given samples from the target distribution, we could compute quantities of interest\nThe grid approach to getting samples:\n\nGenerate discreet points in parameter space \\(\\theta\\)\nDefine our likelihood function \\(f(y|\\theta)\\) and prior \\(f(\\theta)\\)\nFor each point on the grid, compute the product \\(f(y|\\theta)f(\\theta)\\)\nNormalize the product to sum to 1\nSample from the resulting distribution in proportion to the posterior probability\n\nWhat are some limitations of this approach?"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "href": "bayes-course/03-lecture/03-lecture.html#early-clinical-trial-example",
    "title": "Bayesian Inference",
    "section": "Early Clinical Trial Example",
    "text": "Early Clinical Trial Example\n\nDeriving \\(f(\\theta, y)\\)R ImplementationGraphs\n\n\n\n\nFor simplicity we will assume uniform \\(\\text{Beta}(1, 1)\\) prior \\[\n\\begin{eqnarray}\nf(\\theta)f(y\\mid \\theta) &\\propto& \\theta^{a -1}(1 - \\theta)^{b-1} \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^0(1 - \\theta)^0 \\cdot \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\prod_{i=1}^{n} \\theta^{y_i} (1 - \\theta)^{1 - y_i} \\\\\n&=& \\theta^{\\sum_{i=1}^{n} y_i} \\cdot (1 - \\theta)^{\\sum_{i=1}^{n} (1- y_i)}\n\\end{eqnarray}\n\\]\nOn the log scale: \\(\\log f(\\theta, y) \\propto \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\n\nRecall we had 3 responders out of 5 patients\nModel: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\nlp <- function(theta, data) {\n# log(theta) * sum(data$y) + log(1 - theta) * sum(1 - data$y)\n  lp <- 0\n  for (i in 1:data$N) {\n    lp <- lp + log(theta) * data$y[i] + log(1 - theta) * (1 - data$y[i])\n  }\n  return(lp)\n}\ndata <- list(N = 5, y = c(0, 1, 1, 0, 1))\n# generate theta parameter grid\ntheta <- seq(0.01, 0.99, len = 100)\n# compute log likelihood and prior for every value of the grid\nlog_lik <- lp(theta, data); log_prior <- log(dbeta(theta, 1, 1))\n# compute log posterior\nlog_post <- log_lik + log_prior\n# convert back to the original scale and normalize\npost <- exp(log_post); post <- post / sum(post)\n# sample theta in proportion to the posterior probability\ndraws <- sample(theta, size = 1e5, replace = TRUE, prob = post)\n\n\n\n\n\n\nFrom the first lecture, we know the posterior is \\(\\text{Beta}(1 + 3, 1 + 5 - 3) = \\text{Beta}(4, 3)\\)\nWe can compare this density to the posterior draws\n\n\n\n\nbeta_dens <- dbeta(theta, 4, 3)\n(mle <- sum(data$y) / data$N)\n\n[1] 0.6"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "href": "bayes-course/03-lecture/03-lecture.html#monte-carlo-integration-and-mcmc",
    "title": "Bayesian Inference",
    "section": "Monte Carlo Integration and MCMC",
    "text": "Monte Carlo Integration and MCMC\n\nIntroductionExample 1Example 2MCMC\n\n\n\n\nWe already saw a special case of MC integration\nSuppose we can draw samples from PDF \\(f\\), \\((\\theta^{(1)}, \\theta^{(2)},..., \\theta^{(N)})\\)\nIf we want to compute an expectation of some function \\(h\\): \\[\n\\begin{eqnarray}\n\\E_f[h(\\theta)] &=& \\int h(\\theta)f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} h \\left( \\theta^{(i)} \\right)\n\\end{eqnarray}\n\\]\nThe Law of Large Numbers tells us that these approximations improve with \\(N\\)\nIn practice, the challenge is obtaining draws from \\(f\\)\nThat’s where MCMC comes in\n\n\n\n\n\n\nSuppose we want to estimate the mean and variance of standard normal\nIn case of the mean, \\(h(\\theta) := \\theta\\) and variance, \\(h(\\theta) := \\theta^2\\), and \\(f(\\theta) = \\frac{1}{\\sqrt{2 \\pi} \\ } e^{-\\theta^2/2}\\) \\[\n\\begin{eqnarray}\n\\E[\\theta] &=& \\int_{-\\infty}^{\\infty} \\theta f(\\theta)\\, d\\theta\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\theta^{(i)} \\\\\n\\E[\\theta^2] &=& \\int_{-\\infty}^{\\infty} \\theta^2 f(\\theta)\\, d\\theta &\\approx&\n\\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\theta^{(i)} \\right)^2\n\\end{eqnarray}\n\\]\n\n\n\n\nN <- 1e5\ntheta <- rnorm(N, 0, 1)          # draw theta from N(0, 1)\n(1/N * sum(theta)) |> round(2)   # E(theta),   same as mean(theta)\n\n[1] 0\n\n(1/N * sum(theta^2)) |> round(2) # E(theta^2), same as mean(theta^2)\n\n[1] 1\n\n\n\n\n\n\n\nSuppose we want to estimate a CDF of Normal(0, 1) at some point \\(t\\)\nWe let \\(h(\\theta) = \\I(\\theta < t)\\), where \\(\\I\\) is an indicator function that returns 1 when \\(\\theta < t\\) and \\(0\\) otherwise \\[\n\\begin{eqnarray}\n\\E[h(\\theta)] = \\E[\\I(\\theta < t)] &=& \\int_{-\\infty}^{\\infty} \\I(\\theta < t) f(\\theta)\\, d\\theta =\n\\int_{-\\infty}^{t}f(\\theta)\\, d\\theta = \\Phi(t) \\\\\n&\\approx& \\frac{1}{N} \\sum_{i = 1}^{N} \\I(\\theta^{(i)} < t) \\\\\n\\end{eqnarray}\n\\]\n\n\n\n\npnorm(1, 0, 1) |> round(2)          # Evalute N(0, 1) CDF at 1\n\n[1] 0.84\n\nN <- 1e5\ntheta <- rnorm(N, 0, 1)             # draw theta from N(0, 1)\n(1/N * sum(theta < 1)) |> round(2)  # same as mean(theta < 1)\n\n[1] 0.84\n\n\n\n\n\n\n\nMCMC is a very general method for computing expectations (integrals)\nIt produces dependant (autocorrelated) samples, but for a good algorithm, the dependence is manageable\nStan’s MCMC algorithm is very efficient (more on that later) and requires all parameters to be continuous (data can be discrete)\nIt solves the problem of drawing from distribution \\(f(\\theta)\\) where \\(f\\) is not one of the fundamental distributions and \\(\\theta\\) is high dimentional\nWhat is high-dimensional? Modern algorithms like NUTS can jointly sample tens of thousands and more parameters\nThat’s 10,000+ dimensional integrals of complicated functions!\n\n\n\n\n\n\nInclude an example where h(x) is an indicator function, and we want to evaluate Normal CDF at some point t."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#introduction-to-stan",
    "title": "Bayesian Inference",
    "section": "Introduction to Stan",
    "text": "Introduction to Stan\n\n\nStan is a procedural, statically typed, Turning complete, probabilistic programming language\nStan language expresses a probabilistic model\nStan transpiler converts it to C++\nStan inference algorithms perform parameter estimation\nOur model: \\(\\text{lp}(\\theta) = \\log(\\theta) \\cdot\\sum_{i=1}^{n} y_i + \\log(1 - \\theta) \\cdot\\sum_{i=1}^{n} (1-y_i)\\)\n\n\n\n\n\n\n// 01-bernoulli.stan\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}\n\n\n\n\n\n// 02-bernoulli.stan\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  theta ~ beta(1, 1);\n  y ~ bernoulli(theta);\n}"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#homework-1",
    "href": "bayes-course/03-lecture/03-lecture.html#homework-1",
    "title": "Bayesian Inference",
    "section": "Homework 1",
    "text": "Homework 1\n\nModify the program to incorporate Beta(2, 2) without using theta ~ beta(2, 2)\nModify the program to account for an arbitrary Beta(a, b) distribution\nVerify that you got the right result by comparing Stan’s posterior means and posterior standard deviations to the means and standard deviations under the conjugate model. You can also modify the R code to get a third point of comparison.\n\n\nHint: Pass parameters a and b in the data block\n\n\ndata {\n  int<lower=0> N;\n  array[N] int<lower=0, upper=1> y;\n}\nparameters {\n  real<lower=0, upper=1> theta;\n}\nmodel {\n  for (i in 1:N) {\n    target += log(theta * y[i] + \n              log(1 - theta) * (1 - y[i]);\n  }\n}"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "href": "bayes-course/03-lecture/03-lecture.html#running-stan",
    "title": "Bayesian Inference",
    "section": "Running Stan",
    "text": "Running Stan\n\n\nlibrary(cmdstanr)\n\nm1 <- cmdstan_model(\"stan/01-bernoulli.stan\") # compile the model\ndata <- list(N = 5, y = c(0, 1, 1, 0, 1))\nf1 <- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 500  # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <num>  <num> <num> <num>  <num>  <num> <num>    <num>    <num>\n1 lp__     -5.26  -4.99  0.671 0.289 -6.64  -4.78   1.00     964.    1020.\n2 theta     0.579  0.587 0.167 0.180  0.285  0.842  1.00     721.     985."
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "href": "bayes-course/03-lecture/03-lecture.html#working-with-posterior-draws",
    "title": "Bayesian Inference",
    "section": "Working With Posterior Draws",
    "text": "Working With Posterior Draws\n\n\n\n\nlibrary(tidybayes)\n\ndraws <- gather_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy long format\n\n# A tibble: 6 × 5\n# Groups:   .variable [1]\n  .chain .iteration .draw .variable .value\n   <int>      <int> <int> <chr>      <dbl>\n1      4        495  1995 lp__       -4.85\n2      4        496  1996 lp__       -4.93\n3      4        497  1997 lp__       -5.01\n4      4        498  1998 lp__       -5.83\n5      4        499  1999 lp__       -4.79\n6      4        500  2000 lp__       -4.99\n\ndraws |> \n  group_by(.variable) |> \n  summarize(mean = mean(.value))\n\n# A tibble: 2 × 2\n  .variable   mean\n  <chr>      <dbl>\n1 lp__      -5.26 \n2 theta      0.579\n\nmedian_qi(draws, .width = 0.90)\n\n# A tibble: 2 × 7\n  .variable .value .lower .upper .width .point .interval\n  <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 lp__      -4.99  -6.64  -4.78     0.9 median qi       \n2 theta      0.587  0.285  0.842    0.9 median qi       \n\n\n\n\n\n\ndraws <- spread_draws(f1, theta, lp__)\ntail(draws) # draws is a tidy wide format\n\n# A tibble: 6 × 5\n  .chain .iteration .draw theta  lp__\n   <int>      <int> <int> <dbl> <dbl>\n1      4        495  1995 0.503 -4.85\n2      4        496  1996 0.467 -4.93\n3      4        497  1997 0.443 -5.01\n4      4        498  1998 0.306 -5.83\n5      4        499  1999 0.599 -4.79\n6      4        500  2000 0.688 -4.99\n\ntheta <- seq(0.01, 0.99, len = 100)\np <- ggplot(aes(theta), data = draws)\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 30, alpha = 0.6) +\n    geom_line(aes(theta, beta_dens), \n              linewidth = 0.5, color = 'red',\n              data = tibble(theta, beta_dens)) +\n  ylab(\"\") + xlab(expression(theta)) +\n  ggtitle(\"Posterior draws from Stan\")"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#whats-a-chain",
    "title": "Bayesian Inference",
    "section": "What’s a Chain",
    "text": "What’s a Chain"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "href": "bayes-course/03-lecture/03-lecture.html#dynamic-simulation",
    "title": "Bayesian Inference",
    "section": "Dynamic Simulation",
    "text": "Dynamic Simulation\nMCMC Demo"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "href": "bayes-course/03-lecture/03-lecture.html#mcmc-diagnostics",
    "title": "Bayesian Inference",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\n\n\n\nIn general, you want to assess (1) the quality of the draws and (2) the quality of predictions\nThere are no guarantees in either case, but the former is easier than the latter\nThe folk theorem of statistical computing: computational problems often point to problems in the model (AG)\nWe will address the quality of the draws now and the quality of predictions later in the course"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "href": "bayes-course/03-lecture/03-lecture.html#good-chain-bad-chain",
    "title": "Bayesian Inference",
    "section": "Good Chain Bad Chain",
    "text": "Good Chain Bad Chain\n\n\nlibrary(bayesplot)\ndraws <- f1$draws(\"theta\")\ncolor_scheme_set(\"viridis\")\np1 <- mcmc_trace(draws, pars = \"theta\") + ylab(expression(theta)) +\n  ggtitle(\"Good Chain\")\nbad_post <- readRDS(\"data/bad_post.rds\")\nbad_draws <- bad_post$draws(\"mu\")\np2 <- mcmc_trace(bad_draws, pars = \"mu\") + ylab(expression(theta)) +\n  ggtitle(\"Bad Chain\")\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "href": "bayes-course/03-lecture/03-lecture.html#effective-sample-size-and-autocorrelation",
    "title": "Bayesian Inference",
    "section": "Effective Sample Size and Autocorrelation",
    "text": "Effective Sample Size and Autocorrelation\n\nESSACF\n\n\n\n\nMCMC generates dependent draws from the target distribution\nDependent samples are less efficient as you need more of them to estimate the quantity of interest\nESS or \\(N_{eff}\\) is approximately how many independent samples you have\nTypically, \\(N_{eff} < N\\) for MCMC, as there is some autocorrelation\nESS should be considered relative to \\(N\\): \\(\\frac{N_{eff}}{N}\\)\nGenerally, we don’t like to see \\(\\text{ratio} < 0.10\\)\n\n\n\n\nneff_ratio(f1, pars = \"theta\") |> round(2)\n\ntheta \n 0.36 \n\nneff_ratio(bad_post, pars = \"mu\") |> round(2)\n\n  mu \n0.09"
  },
  {
    "objectID": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "href": "bayes-course/03-lecture/03-lecture.html#computing-r-hat",
    "title": "Bayesian Inference",
    "section": "Computing R-Hat",
    "text": "Computing R-Hat\n\n\nAutocorrelation assesses the quality of a single chain\nSplit R-Hat estimates the extent to which the chains are consistent with one another\nIt does it by assessing the mixing of chains by comparing variances within and between chains (technically sequences, as chains are split up) \\[\n\\begin{eqnarray}\n\\hat{R} = \\sqrt{\\frac{\\frac{n-1}{n}\\V_W + \\frac{1}{n}\\V_B}{\\V_W}} = \\sqrt{\\frac{\\V_{\\text{total}}}{\\V_{W}}}\n\\end{eqnarray}\n\\]\n\\(\\V_W\\) is within chain variance and \\(\\V_B\\) is between chain variance\nWe don’t like to see R-hats greater than 1.02 and really don’t like them greater than 1.05\n\n\n\n\nbayesplot::rhat(f1, pars = \"theta\") |> round(3)\n\ntheta \n1.001 \n\nbayesplot::rhat(bad_post, pars = \"sigma\")  |> round(3)\n\nsigma \n1.043 \n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "href": "bayes-course/02-lecture/02-lecture.html#conjugate-models-and-beta-binomial",
    "title": "Bayesian Inference",
    "section": "Conjugate models and Beta-Binomial",
    "text": "Conjugate models and Beta-Binomial\n\n\nBeta distribution\nGreat expectations\nTuning the prior model for the clinical trial analysis\nBinomial likelihood with continuous \\(\\theta\\)\nDeriving the conjugate posterior\nAnalysis of the sex ratio\nCompromise between priors and data model\nCoherence of Bayesian inference\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#homework",
    "href": "bayes-course/02-lecture/02-lecture.html#homework",
    "title": "Bayesian Inference",
    "section": "Homework",
    "text": "Homework\n\nHow did you find the homework?\nHomework review"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introduction",
    "href": "bayes-course/02-lecture/02-lecture.html#introduction",
    "title": "Bayesian Inference",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\nLast time, we presented the discretized version of the prior\nIn practice, we are typically working in continuous space\nIn general, the prior model can be arbitrarily complex\nThere is a family of distributions called the exponential family, for which the prior has the same kernel as the posterior\nThis is called conjugacy\nIn practice, we seldom rely on conjugate relationships\nBeta distribution is conjugate to Binomial"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "href": "bayes-course/02-lecture/02-lecture.html#continous-probability-distributions",
    "title": "Bayesian Inference",
    "section": "Continous Probability Distributions",
    "text": "Continous Probability Distributions\n\n\nRecall that for the continuous RVs we have, the CDF is defined as \\[\n\\begin{eqnarray}\nF(x) & = & \\int_{-\\infty}^{x} f(t) \\, dt, \\, \\text{and} \\\\\nf(x) & = & F'(x), \\, \\text{where } F \\text{ is differentiable}\n\\end{eqnarray}\n\\]\nTo compute event probabilities: \\[\n\\begin{eqnarray}\n\\P(a \\le X \\leq b) & = & F(b) − F(a) = \\int_{a}^{b} f(x) \\, dx \\\\\n\\P(X \\in A) & = & \\int_{A} f(x) \\, dx\n\\end{eqnarray}\n\\]\nAs in the case of PMFs: \\[\n\\begin{eqnarray}\nf(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#introducing-beta",
    "title": "Bayesian Inference",
    "section": "Introducing Beta",
    "text": "Introducing Beta\n\n\n\n\nBeta distribution has the following functional form for \\(\\alpha > 0, \\, \\beta > 0, \\, \\theta \\in (0, 1)\\) \\[\n\\begin{eqnarray}\n\\text{Beta}(\\theta \\mid \\alpha,\\beta) & = &\n\\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta^{\\alpha - 1} \\, (1 -\n\\theta)^{\\beta - 1} \\\\\n\\text{B}(a,b) \\ & = & \\ \\int_0^1 u^{a - 1} (1 - u)^{b - 1} \\,\ndu \\ = \\ \\frac{\\Gamma(a) \\, \\Gamma(b)}{\\Gamma(a+b)} \\\\\n\\Gamma(x) & = &\n\\int_0^{\\infty} u^{x - 1} \\exp(-u) \\, du\n\\end{eqnarray}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFor positive integrers \\(n\\): \\(\\, \\Gamma(n+1) = n!\\) and in general \\(\\Gamma(z + 1) = z \\Gamma(z)\\)\n\\(\\text{Beta}(1, 1)\\) is equivalent to \\(\\text{Unif(0, 1)}\\)\nThe above makes Beta a natural choice for priors on the probability scale"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-beta",
    "title": "Bayesian Inference",
    "section": "Visualizing Beta",
    "text": "Visualizing Beta\n\n\nThe mode of Beta, \\(\\text{Mode}(\\theta) = \\argmax_\\theta \\text{Beta}(\\theta \\mid \\alpha, \\beta)\\) is shown in red and the function maximum in blue"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#expectations",
    "title": "Bayesian Inference",
    "section": "Expectations",
    "text": "Expectations\n\n\nRecall the definition of expectations for continuous random variables\nWe often write \\(\\mu\\) or \\(\\mu_X\\) for expected value of \\(X\\) \\[\n\\mu_X = \\E(X) = \\int x \\cdot f(x) \\, dx\n\\]\nVariance is a type of expectation, which we often denote by \\(\\sigma^2\\) \\[\n\\sigma_X = \\V(X) = \\E(X - \\mu)^2 = \\int (X - \\mu)^2 f(x) \\, dx\n\\]\nIt is often more convenient to write the variance as \\[\n\\V(X) = \\E(X^2) - \\mu^2\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nKeeping in mind that a Gamma function is a continuous analog of the factorial: \\[\n\\begin{eqnarray}\n\\E(\\theta) &=& \\int_{0}^{1} \\frac{1}{\\mathrm{B}(\\alpha,\\beta)} \\, \\theta \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta \\\\\n&=& \\frac{1}{\\mathrm{B}(\\alpha,\\beta)}\\int_{0}^{1}  \\color{red}{\\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1}} \\, d\\theta \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\, \\Gamma(\\beta)} \\cdot\n\\color{red}{\\frac{\\Gamma(1 + \\alpha) \\Gamma(\\beta)}{\\Gamma(1 + a + b)}} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{\\Gamma(1 + \\alpha)}{\\Gamma(1 + \\alpha + \\beta)} \\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)} \\cdot\n\\frac{a \\Gamma(\\alpha)}{(\\alpha + \\beta) \\Gamma(\\alpha + \\beta)} \\\\\n&=& \\frac{\\alpha}{\\alpha + \\beta}.\n\\end{eqnarray}\n\\]\nWhat is the value of \\(\\int_{0}^{1} \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta} \\, d\\theta\\)\n\n\n\nCheck the integral \\(\\int_{0}^{1} \\, \\theta^\\alpha \\, (1 - \\theta)^{\\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "href": "bayes-course/02-lecture/02-lecture.html#what-to-expect-from-beta-1",
    "title": "Bayesian Inference",
    "section": "What to expect from Beta",
    "text": "What to expect from Beta\n\n\nWe can find the mode of this distribution by taking the log, differentiating with respect to \\(\\theta\\), and setting the derivative function to zero \\[\n\\begin{eqnarray}\n\\E(\\theta) & = & \\frac{\\alpha}{\\alpha + \\beta} \\\\\n\\text{Mode}(\\theta) & = & \\frac{\\alpha - 1}{\\alpha + \\beta - 2} \\;\\; \\text{ when } \\; \\alpha, \\beta > 1. \\\\\n\\end{eqnarray}\n\\]\nThe variance of \\(\\theta\\) can be derived using the definition of the variance operator \\[\n\\begin{eqnarray}\n\\V(\\theta) & = & \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{eqnarray}\n\\]\nNotice when \\(\\alpha = \\beta\\), \\(\\E(\\theta) = \\text{Mode}(\\theta)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\n\n\nWe borrow an example from BDA3: the probability of girl birth given placenta previa (PP)\nPlacenta previa is a condition when the placenta completely or partially covers the opening of the uterus\nA PP study in Germany found that out of 980 births, 437 or \\(\\approx 45\\%\\) were female\nSex ratio in the population has been stable over space and time at 48.5% females with deviations within no more than 1%1\nOur task is to assess the evidence for \\(\\P(\\text{ratio} < .485 \\mid \\text{PP})\\)\n\n\n\n\n\n\n\n\n\n\nGelman, A., & Weakliem, D. (2009). Of Beauty, Sex and Power. American Scientist, 97(4), 310. https://doi.org/10.1511/2009.79.310"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "href": "bayes-course/02-lecture/02-lecture.html#biomomial-likelihood",
    "title": "Bayesian Inference",
    "section": "Biomomial Likelihood",
    "text": "Biomomial Likelihood\n\n\nRecall that Likelihood is the function of the parameter \\(\\theta\\), assuming \\(\\theta \\in [0,1]\\) \\[\n\\text{Bin}(y \\mid \\theta) = \\binom{N}{y}\n\\theta^y (1 - \\theta)^{N - y} \\propto \\theta^y (1 - \\theta)^{N - y}\n\\]\nAssuming \\(N = 10\\), the likelihood for \\(\\theta\\), given a few possible values of \\(y\\) successes"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "href": "bayes-course/02-lecture/02-lecture.html#deriving-the-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving the Posterior Distribution",
    "text": "Deriving the Posterior Distribution\n\n\nThe denominator is constant in \\(\\theta\\); we will derive the posterior up to a proportion \\[\n\\begin{eqnarray}\nf(y \\mid \\theta) & \\propto & \\theta^y (1 - \\theta)^{N - y} \\\\\nf(\\theta) & \\propto & \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\nf(\\theta \\mid y) & \\propto & f(y \\mid \\theta) f(\\theta) \\\\\n& = & \\theta^y (1 - \\theta)^{N - y} \\cdot \\theta^{\\alpha - 1} \\, (1 - \\theta)^{\\beta - 1} \\\\\n& = & \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\\\\nf(\\theta \\mid y) & = & \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\n\\end{eqnarray}\n\\]\nThe last equality comes from matching the kernel \\(\\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1}\\) to the normalized Beta PDF which has a normalizing constant \\(\\frac{\\Gamma (\\alpha +\\beta + N)}{\\Gamma (\\alpha + y) \\Gamma (\\beta + N - y)}\\)\nSince the posterior is in the same family as the prior, we say that Beta is conjugate to Binomial\n\n\n\nCheck the kernel integral, \\(\\int_{0}^{1} \\theta^{y + \\alpha - 1} (1 - \\theta)^{N - y + \\beta - 1} \\, d\\theta\\) using Wolfram Alpha"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-expectations",
    "title": "Bayesian Inference",
    "section": "Posterior Expectations",
    "text": "Posterior Expectations\n\n\\[\n\\begin{eqnarray}\n\\E(\\theta \\mid Y=y)  & = & \\frac{\\alpha + y}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n}\\cdot \\E(\\theta) + \\frac{n}{\\alpha + \\beta + n}\\cdot\\frac{y}{n} \\\\\n\\V(\\theta \\mid Y=y)  & = & \\frac{(\\alpha + y)(\\beta + n - y)}{(\\alpha + \\beta + n)^2(\\alpha + \\beta + n + 1)} \\\\\n\\text{Mode}(\\theta \\mid Y=y) & = & \\frac{\\alpha + y - 1}{\\alpha + \\beta + n - 2} = \\frac{\\alpha + \\beta - 2}{\\alpha + \\beta + n - 2} \\cdot\\text{Mode}(\\theta) + \\frac{n}{\\alpha + \\beta + n - 2} \\cdot\\frac{y}{n}\n\\end{eqnarray}\n\\]\n\n\n\nNote tha the posterior expectation is between \\(y/n\\) and \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\nAlso note that as sample becomes very large \\(\\E(\\theta \\mid Y=y) \\rightarrow \\frac{y}{n}\\) and \\(\\V(\\theta \\mid Y=y) \\rightarrow 0\\)\nAs before, \\(\\text{Mode}(\\theta \\mid Y=y) = \\E(\\theta \\mid Y=y)\\), when \\(\\alpha = \\beta\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "href": "bayes-course/02-lecture/02-lecture.html#example-placenta-previa-1",
    "title": "Bayesian Inference",
    "section": "Example: Placenta Previa",
    "text": "Example: Placenta Previa\n\n\nLet’s derive the analytical posterior and compare it with the samples from the posterior distribution\nFirst, we will consider the uniform \\(\\text{Beta}(1, 1)\\) prior\nWe are told that data are \\(N = 980\\) and \\(y = 437\\) female births\nThe posterior is \\(\\text{Beta}(1 + 437, 1 + 980 - 437) = \\text{Beta}(438, 544)\\)\n\\(\\E(\\theta \\mid Y=437) = \\frac{\\alpha + 437}{\\alpha + \\beta + 980} = \\frac{438}{982} \\approx 0.446\\)\n\\(\\sqrt{\\V(\\theta \\mid Y=y)} \\approx\\) 0.016\n\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 438, 544) |> round(3)\nlower <- qbeta(l, 438, 544) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.415, 0.477]\n\nevent_prob <- integrate(dbeta, lower = 0, upper = 0.485, shape1 = 438, shape2 = 544)[[1]]\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993\n\npbeta(0.485, shape1 = 438, shape2 = 544) |> round(3)\n\n[1] 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "href": "bayes-course/02-lecture/02-lecture.html#obtaining-quantiles-by-sampling",
    "title": "Bayesian Inference",
    "section": "Obtaining Quantiles by Sampling",
    "text": "Obtaining Quantiles by Sampling\n\n\nWe can use R’s rbeta() RNG to generate draws from the posterior\n\n\n\n\n\ndraws <- rbeta(n = 1e5, 438, 544)\np <- ggplot(aes(draws), data = data.frame(draws))\np + geom_histogram(bins = 30) + ylab(\"\") +\n  geom_vline(xintercept = 0.485, \n             colour = \"red\", size = 0.3) +\n  ggtitle(\"Draws from Beta(438, 544)\") + \n  theme(axis.text.y = element_blank()) + \n  xlab(expression(theta))\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the quantile() function to get the posterior interval and compute the event probability by evaluating the expectation of the indicator function as before\n\n\n\n\nquantile(draws, probs = c(0.025, 0.5, 0.975)) |> round(3)\n\n 2.5%   50% 97.5% \n0.415 0.446 0.477 \n\ncat(\"Probability that the ratio < 0.485 under uniform prior =\", mean(draws < 0.485) |> round(3))\n\nProbability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#population-priors",
    "title": "Bayesian Inference",
    "section": "Population Priors",
    "text": "Population Priors\n\n\n\n\nWhat priors should we use if we think the sample is drawn from the sex ratio “hyper-population”?\nWe know that the population mean is 0.485 and the standard deviation is about 0.01\nBack out the parameters of the population Beta distribution\n\n\n\n\\[\n\\begin{eqnarray}\n\\begin{cases}\n\\frac{\\alpha}{\\alpha + \\beta} & = & 0.485 \\\\\n\\sqrt{\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}} & = & 0.01\n\\end{cases}\n\\end{eqnarray}\n\\]\n\n\n\n\\(\\alpha \\approx 1211\\) and \\(\\beta \\approx 1286\\)\n\n\n\n\n\nCheck the result with the simulation\n\n\n\n\nx <- rbeta(1e4, 1211, 1286)\nmean(x) |> round(3)\n\n[1] 0.485\n\nsd(x) |> round(3)\n\n[1] 0.01\n\n\n\n\n\nWe can let Mathematica do the algebra"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "href": "bayes-course/02-lecture/02-lecture.html#posterior-with-population-priors",
    "title": "Bayesian Inference",
    "section": "Posterior with Population Priors",
    "text": "Posterior with Population Priors\n\n\n\\(f(\\theta | y) = \\text{Beta}(1211 + 437, 1286 + 543) = \\text{Beta}(1648,1829)\\)\nWe can compare the prior and posterior using summarize_beta_binomial() in the bayesrules package\n\n\n\nlibrary(bayesrules)\nsummarize_beta_binomial(alpha = 1211, beta = 1286, y = 437, n = 980)\n\n      model alpha beta      mean      mode          var          sd\n1     prior  1211 1286 0.4849820 0.4849699 9.998978e-05 0.009999489\n2 posterior  1648 1829 0.4739718 0.4739568 7.168560e-05 0.008466735\n\n\n\n\n\nint <- 0.95; l <- (1 - int)/2; u <- 1 - l\nupper <- qbeta(u, 1648, 1829) |> round(3)\nlower <- qbeta(l, 1648, 1829) |> round(3)\ncat(\"95% posterior interval is [\", lower, \", \", upper, \"]\", sep = \"\")\n\n95% posterior interval is [0.457, 0.491]\n\n\n\n\n\nevent_prob <- pbeta(0.485, shape1 = 1648, shape2 = 1829)\ncat(\"Probability that the ratio < 0.485 under population prior =\", round(event_prob, 3))\n\nProbability that the ratio < 0.485 under population prior = 0.904\n\n\n\n\nUnder uniform prior 95% posterior interval was [0.415, 0.477]\nAnd probability that the ratio < 0.485 under uniform prior = 0.993"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "href": "bayes-course/02-lecture/02-lecture.html#visualizing-bayesian-rebalancing",
    "title": "Bayesian Inference",
    "section": "Visualizing Bayesian Rebalancing",
    "text": "Visualizing Bayesian Rebalancing\n\n\nThe following uses \\(\\text{Beta}(5, 5)\\) prior and N = 10:"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "href": "bayes-course/02-lecture/02-lecture.html#data-order-and-batch-invariance",
    "title": "Bayesian Inference",
    "section": "Data Order and Batch Invariance",
    "text": "Data Order and Batch Invariance\n\n\nIn Bayesian analysis, we can update all at once or one datum at a time and everything in between and in any order (assuming exchangeable observations)\nThis is a general result, not just for Beta Binomial (see Section 4.5)\nIn practice, when we don’t have analytic posteriors, this is not so easy to do\nSuppose we observe \\(y = y_1 + y_2\\) and \\(N = N_1 + N_2\\) trials all at once\nAlso assume we start with \\(\\text{Beta}(1, 1)\\) prior\nFor all at once case, the posterior is in \\(f(\\theta \\mid y) = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\) as before\nNow, suppose we observe \\(y_1\\) successes in \\(n_1\\) trials first\nThe posterior is \\(f(\\theta \\mid y_1) = \\text{Beta}(\\alpha + y_1, \\,\\beta + N_1 - y_1)\\)\nWe now observe, \\(y_2\\) successes in \\(n_2\\) trials. The posterior is \\(f(\\theta \\mid y= y_1 + y_2) = \\text{Beta}(\\alpha + y_1 + y_2, \\,\\beta + N_1 - y_1 + N_2 - y_2)\\\\ = \\text{Beta}(\\alpha + y, \\,\\beta + N - y)\\)"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#general-case",
    "href": "bayes-course/02-lecture/02-lecture.html#general-case",
    "title": "Bayesian Inference",
    "section": "General Case",
    "text": "General Case\n\n\nSuppose we observe data point \\(y_1\\), and then data point \\(y_2\\)\n\n\n\n\\[\nf(\\theta \\mid y_1,y_2) =  \\frac{f(\\theta)f(y_1 \\mid \\theta)f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\]\n\n\n\nObserving data point \\(y_2\\), and then data point \\(y_1\\), will results in the same distribution\nWhat if observed both points at once?\n\n\n\n\\[\n\\begin{split}\nf(\\theta \\mid y_1,y_2)\n& = \\frac{f(\\theta)f(y_1,y_2 \\mid \\theta)}{f(y_1)f(y_2)} \\\\\n& = \\frac{f(\\theta)f(y_1 \\mid \\theta)f(y_2 \\mid \\theta)}{f(y_1)f(y_2)}\n\\end{split}\n\\]"
  },
  {
    "objectID": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow-chapter-2",
    "href": "bayes-course/02-lecture/02-lecture.html#bayesian-workflow-chapter-2",
    "title": "Bayesian Inference",
    "section": "Bayesian Workflow Chapter 2",
    "text": "Bayesian Workflow Chapter 2\n\n\n\n\nWhat do we do before fitting a model\n\nChoosing an initial model\nModular construction\nScaling and transforming the parameters\nPrior predictive checking\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-regression-and-introduction-to-hierarchical-models",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-regression-and-introduction-to-hierarchical-models",
    "title": "Bayesian Inference",
    "section": "Logistic Regression and Introduction to Hierarchical Models",
    "text": "Logistic Regression and Introduction to Hierarchical Models\n\n\nGLMs and logistic regression\nUnderstanding logistic regression\nSimulating data\nPrior predictive simulations\nExample: Diabetes in Pima native americans\nIntroducing hierarchical/multi-level models\nPooling: none, complete, and partial\nExample hierarchical model\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logit-and-inverse-logit",
    "href": "bayes-course/07-lecture/07-lecture.html#logit-and-inverse-logit",
    "title": "Bayesian Inference",
    "section": "Logit and Inverse Logit",
    "text": "Logit and Inverse Logit\n\n\nIn logistic regression, we have to map from probability space to the real line and from the real line back to probability\nLogistic function (log odds) achieves the former: \\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\n\\]\nInverse logit achieves the latter: \\[\n\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}\n\\]\nNotice that \\(\\text{logit}^{-1}(5)\\) is very close to 1 and \\(\\text{logit}^{-1}(-5)\\) is very close to 0\nIn R, you can set logit <- qlogis and invlogit <- plogis"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#glms-and-models-01-outcomes",
    "href": "bayes-course/07-lecture/07-lecture.html#glms-and-models-01-outcomes",
    "title": "Bayesian Inference",
    "section": "GLMs and Models 0/1 Outcomes",
    "text": "GLMs and Models 0/1 Outcomes\n\n\nModeling a probability of an event can be framed in the GLM context (just like with counts)\nThe general setup is that we have:\n\nResponse vector \\(y\\) consisting of zeros and ones\nThe data model is \\(y_i \\sim \\text{Bernoulli}(p_i)\\)\nLinear predictor: \\(\\eta = \\alpha + X\\beta\\), where \\(X\\) is a matrix\nIn general: \\(\\E(y | X) = g^{-1}(\\eta)\\), where \\(g^{-1}\\)is the inverse link function that maps the linear predictor onto the observational scale\nIn particular: \\(\\E(y_i | x_i) = \\text{logit}^{-1}(\\alpha + x_i^{\\top}\\beta) = p_i\\)\n\n\\(\\text{logit}(p_i) = \\eta_i\\) and \\(p_i = \\text{logit}^{-1}(\\eta_i)\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-posterior",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-posterior",
    "title": "Bayesian Inference",
    "section": "Logistic Posterior",
    "text": "Logistic Posterior\n\n\nTo derive the posterior distribution for Poisson, we consider K regression inputs and independent priors on all \\(K + 1\\) unknowns: \\(\\alpha\\) and \\(\\beta_1, \\beta_2, ..., \\beta_k\\)\nBernoulli likelihood is: \\(f(y | p) = p^y (1 - p)^{1-y}\\) with \\(y \\in \\{0, 1\\}\\)\nAnd each \\(p_i = \\text{logit}^{-1}(\\alpha + x_i^\\top\\beta)\\) \\[\nf\\left(\\alpha,\\beta \\mid y,X\\right) \\propto\nf_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot \\\\\n\\prod_{i=1}^N \\left(\\text{logit}^{-1}(\\alpha + x_i^\\top\\beta) \\right)^{y_i} \\left(1 - \\text{logit}^{-1}(\\alpha + x_i^\\top\\beta)\\right)^{1-y_i}\n\\]\nIn Stan, the likelihood term can be written on a log scale as y ~ bernoulli_logit_glm(x, alpha, beta) or bernoulli_logit_glm_lupmf(y | x, alpha, beta)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#logistic-simulation",
    "href": "bayes-course/07-lecture/07-lecture.html#logistic-simulation",
    "title": "Bayesian Inference",
    "section": "Logistic Simulation",
    "text": "Logistic Simulation\n\n\nAs before, we can forward simulate data for logistic regression\nWe will fit the data and try to recover the parameters\n\n\n\n\n\nset.seed(123)\nlogit <- qlogis; invlogit <- plogis\nn <- 100\na <- 1.2\nb <- 0.4\nx <- runif(n, -15, 10)\neta <- a + x * b\nPr <- invlogit(eta)\ny <- rbinom(n, 1, Pr)\nsim <- tibble(y, x, Pr)\n\np <- ggplot(aes(x, y), data = sim)\np <- p + geom_point(size = 0.5) +\n  geom_line(aes(x, Pr), linewidth = 0.2) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 0.2, \n             linetype = \"dashed\", alpha = 1/3) +\n  geom_hline(yintercept = invlogit(a), color = \"red\", linewidth = 0.2, \n             linetype = \"dashed\", alpha = 1/3) +\n  geom_hline(yintercept = 0.50, linewidth = 0.2, linetype = \"dashed\", alpha = 1/3) +\n  ggtitle(TeX(\"$y_i \\\\sim Bernoulli(logit^{-1}(1.2 + 0.4x_i))$\")) +\n  annotate(\"text\", x = -5.5, y = invlogit(a) - 0.02,\n           label = TeX(\"Intercept = $logit^{-1}(1.2)$ \\\\approx 0.77\")) +\n  annotate(\"text\", x = -8, y = 0.53,\n           label = TeX(\"$slope_{.5} = \\\\frac{0.4}{4} = 0.10$\")) +\n  ylab(TeX(\"$logit^{-1}(1.2 + 0.4x)$\")); print(p)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#interpreting-logistic-coefficients",
    "href": "bayes-course/07-lecture/07-lecture.html#interpreting-logistic-coefficients",
    "title": "Bayesian Inference",
    "section": "Interpreting Logistic Coefficients",
    "text": "Interpreting Logistic Coefficients\n\n\n\n\nThe intercept is the log odds of an event when \\(x = 0\\), \\(\\text{logit}^{-1}(1.2) = 0.77\\)\nThe slope changes depending on where you are on the curve\nWhen you are near 0.50, the slope of logit(x) is 1/4 and so you can divide your coefficient by 4 to get a rough estimate\nThis implies that if we go from \\(x = -3\\) to \\(x = -2\\), the probability will increase by about 0.10\n\n\n\n\n(invlogit(1.2 + 0.4 * -2) - \n   invlogit(1.2 + 0.4 * -3)) |> \n  round(2)\n\n[1] 0.1"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#fitting-simulated-data",
    "href": "bayes-course/07-lecture/07-lecture.html#fitting-simulated-data",
    "title": "Bayesian Inference",
    "section": "Fitting Simulated Data",
    "text": "Fitting Simulated Data\n\n\nComplex and non-linear models may have a hard time recovering parameters from forward simulations\nThe process for fitting simulated data may give some insight into the data-generating process and priors\n\n\n\n\n\n# fitting from eta = 1.2 +  0.4 * x\nm1 <- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = binomial(link = \"logit\"), \n               data = sim,\n               chains = 4,\n               refresh = 0,\n               iter = 1000)\nsummary(m1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      y ~ x\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 1.5    0.5  0.9   1.5   2.1  \nx           0.5    0.1  0.4   0.5   0.7  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.5    0.0  0.5   0.5   0.6  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1132 \nx             0.0  1.0  1106 \nmean_PPD      0.0  1.0  1584 \nlog-posterior 0.0  1.0   817 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#generating-probability-data",
    "href": "bayes-course/07-lecture/07-lecture.html#generating-probability-data",
    "title": "Bayesian Inference",
    "section": "Generating Probability Data",
    "text": "Generating Probability Data\n\n\nTo get a sense of the variability in probability we can simulate from the prior distribution on the probability scale\n\n\n\n\n\nprior_pred_logit <- function(x) {\n  a <- rnorm(1, mean = 1.2, sd = 0.5)\n  b <- rnorm(1, mean = 0.4, sd = 0.1)\n  Pr <- invlogit(a + b * x)\n  return(Pr)\n}\nprior_pred <- replicate(50, prior_pred_logit(x)) |>\n  as.data.frame()\n\ndf_long <- prior_pred |>\n  mutate(x = x) |>\n  pivot_longer(cols = -x, names_to = \"line\", values_to = \"y\")\n\np <- ggplot(aes(x, y), data = df_long)\np + geom_line(aes(group = line), linewidth = 0.2, alpha = 1/5) +\n  geom_line(aes(y = Pr), data = sim, linewidth = 0.5, color = 'red') +\n  ylab(TeX(\"$logit^{-1}(\\\\alpha + \\\\beta x)$\")) +\n  ggtitle(TeX(\"Simulating from prior $logit^{-1}(\\\\alpha + \\\\beta x_i))$\"),\n  subtitle = TeX(\"$\\\\alpha \\\\sim Normal(1.2, 0.5)$ and $\\\\beta \\\\sim Normal(0.4, 0.1)$\"))"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThis example comes from US National Institute of Diabetes and Digestive and Kidney Diseases from a population of women who were at least 21 years old, of Pima Indian heritage, and living near Phoenix, Arizona\nIt is available as part of the R package pdp\nThe outcome \\(diabetes\\), is an indicator of the disease\nSome other variables are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npregnant\nglucose\npressure\ntriceps\ninsulin\nmass\npedigree\nage\ndiabetes\n\n\n\n\n6\n148\n72\n35\nNA\n33.6\n0.627\n50\npos\n\n\n1\n85\n66\n29\nNA\n26.6\n0.351\n31\nneg\n\n\n8\n183\n64\nNA\nNA\n23.3\n0.672\n32\npos\n\n\n1\n89\n66\n23\n94\n28.1\n0.167\n21\nneg\n\n\n0\n137\n40\n35\n168\n43.1\n2.288\n33\npos\n\n\n5\n116\n74\nNA\nNA\n25.6\n0.201\n30\nneg"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#missing-value-imputation",
    "href": "bayes-course/07-lecture/07-lecture.html#missing-value-imputation",
    "title": "Bayesian Inference",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\n\n\n\n\nThis dataset contains missing values\nPeople typically either delete them or replace them with average values of something like that\nNone of these are good approaches\nIn R, we recommend a combination of the mice package and brms, which works nicely with mice\nFrom a Bayesian perspective, a missing value is just another unknown parameter in the model, which can be modeled\n\n\n\n\n\nFollowing is an example of a simple missing value estimation from the Stan manual\n\n\n\n\ndata {\n  int<lower=0> N_obs;\n  int<lower=0> N_mis;\n  array[N_obs] real y_obs;\n}\nparameters {\n  real mu;\n  real<lower=0> sigma;\n  array[N_mis] real y_mis;\n}\nmodel {\n  y_obs ~ normal(mu, sigma);\n  y_mis ~ normal(mu, sigma);\n}\n\n\n\n\nNotice, that you need to make an assumption about the model for missing values"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-1",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-1",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\nCodePlot\n\n\n\n\nIt is well-known that people with high BMI are at risk for Type 2 diabetes\nWe can do some exploratory analysis to check this\n\n\n\n\np1 <- ggplot(pima, aes(x = mass)) + \n  geom_density(aes(group = diabetes, fill = diabetes, color = diabetes), alpha = 1/5) +\n  xlab(\"BMI\")\np2 <- pima |>\n  drop_na() |>\n  mutate(bmi_cut = cut(mass, breaks = seq(15, 70, by = 5))) |>\n  group_by(bmi_cut) |>\n  summarize(p = mean(diabetes == \"pos\"),\n            n = n(),\n            se = sqrt(p * (1 - p) / n),\n            lower = p + se,\n            upper = p - se) |>\n  ggplot(aes(x = bmi_cut, y = p)) + \n  geom_point() + geom_linerange(aes(ymin = lower, ymax = upper), linewidth = 0.2) +\n  xlab(\"BMI range\") + ylab(\"Proportion\") +\n  ggtitle(\"Proportion of diabetics by BMI +/- 1 SE\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-2",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-2",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nBefore building a model, we should scale the variables\n\n\n\n\nlibrary(pdp)\nd <- pima |>\n  as_tibble() |>\n  select(diabetes, age, pedigree, mass, glucose) |>\n  drop_na() |> # in an important analysis, you should not do this; instead, impute\n  mutate(diab = if_else(diabetes == \"pos\", 1, 0),\n         age = (age - mean(age)) / 10,\n         pedigree = (pedigree - mean(pedigree)),\n         bmi = ((mass - mean(mass)) / 10),\n         glucose = ((glucose - mean(glucose)) / sd(glucose))) \nhead(d)\n\n# A tibble: 6 × 7\n  diabetes     age pedigree  mass glucose  diab    bmi\n  <fct>      <dbl>    <dbl> <dbl>   <dbl> <dbl>  <dbl>\n1 pos       1.67      0.154  33.6   0.852     1  0.115\n2 neg      -0.231    -0.122  26.6  -1.21      0 -0.585\n3 pos      -0.131     0.199  23.3   2.00      1 -0.915\n4 neg      -1.23     -0.306  28.1  -1.08      0 -0.435\n5 pos      -0.0312    1.81   43.1   0.492     1  1.06 \n6 neg      -0.331    -0.272  25.6  -0.194     0 -0.685"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-3",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-3",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe will be fitting the following statistical model, where \\(x_1\\) is the BMI and \\(\\bar x_1\\) is the average BMI in the sample\nWe divide by ten so that a unit increase in BMI is a meaningful change\nWe need to pick priors on \\(\\alpha\\), and \\(\\beta_1\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta_1 \\left( \\frac{x_{1i} - \\bar x_1}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(\\mu_\\alpha ,\\ \\sigma_\\alpha) \\\\\n\\beta_1 &\\sim& \\text{Normal}(\\mu_\\beta , \\ \\sigma_\\beta)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-4",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-4",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThe prior on the intercept corresponds to the log odds of developing diabetes when a person has an average BMI, which is about 29 in the US (which is considered high-risk)\nIt is likely that the probability is between 20% and 80%, and so a weakly informative prior can be expressed as \\(\\text{Normal}(0, 0.5)\\)\nSuppose, we also put a weakly informative \\(\\text{Normal}(0, 1)\\) pior on \\(\\beta_1\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta_1 \\left( \\frac{x_{1i} - \\bar x_1}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(0 ,\\ 0.5) \\\\\n\\beta_1 &\\sim& \\text{Normal}(0 , \\ 1)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-5",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-5",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nLet’s perform prior predictive simulation\n\n\n\n\n\nm_prior <- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0, 1), \n               family = binomial(link = \"logit\"), \n               refresh = 100,\n               prior_PD = TRUE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\n\nsummary(m_prior)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  0.0    0.5 -0.6   0.0   0.6 \nbmi          0.0    1.0 -1.3   0.0   1.3 \n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1215 \nbmi           0.0  1.0  1108 \nlog-posterior 0.0  1.0   932 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-6",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-6",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nd |>\n  add_epred_draws(m_prior, ndraws = 100) |>\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\") +\n  ggtitle(\"Possible Logit Curves Implied by the Prior\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-7",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-7",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThe negative associations are implausible given everything we know about diabetes\nSuppose the risk of diabetes doubles for every 10 \\(kg/m^2\\) (unverified)\nThat would imply average \\(\\beta_1 \\approx 0.7\\), since the multiplicative change in odds is \\(e^{0.7} \\approx 2\\)\nWe will set the standard deviation to 0.2 to avoid negative effects and allow the odds to be as high as 3.5\n\n\n\n\\[\n\\begin{eqnarray}\ny_i &\\sim& \\text{Bernoulli}(p_i) \\\\\n\\eta_i &=& \\alpha + \\beta_1 \\left( \\frac{x_{1i} - \\bar x_1}{10} \\right) \\\\\np_i &=& \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\\\\n\\alpha &\\sim& \\text{Normal}(0 ,\\ 0.5) \\\\\n\\beta_1 &\\sim& \\text{Normal}(0.7 , \\ 0.2)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-8",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-8",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWith new priors, let’s repeat the prior predictive simulation\n\n\n\n\n\nm_prior <- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0.7, 0.2), \n               family = binomial(link = \"logit\"), \n               refresh = 100,\n               prior_PD = TRUE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m_prior)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  0.0    0.5 -0.6   0.0   0.6 \nbmi          0.7    0.2  0.4   0.7   1.0 \n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1215 \nbmi           0.0  1.0  1108 \nlog-posterior 0.0  1.0   932 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-9",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-9",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nd |>\n  add_epred_draws(m_prior, ndraws = 100) |>\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\") +\n  ggtitle(\"Possible Logit Curves Implied by the Prior\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-10",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-10",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWith this new prior we are ready to fit the model\n\n\n\n\n\nm2 <- stan_glm(diab ~ bmi,\n               prior_intercept = normal(0, 0.5),\n               prior = normal(0.7, 0.2), \n               family = binomial(link = \"logit\"), \n               prior_PD = FALSE,\n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m2)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.6    0.1 -0.7  -0.6  -0.5 \nbmi          0.9    0.1  0.8   0.9   1.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1315 \nbmi           0.0  1.0  1299 \nmean_PPD      0.0  1.0  1651 \nlog-posterior 0.0  1.0   780 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-11",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-11",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nThere are no sampling problems, but we should still check the diagnostics\n\n\n\n\nlibrary(bayesplot)\np1 <- mcmc_trace(m2)\np2 <- mcmc_acf(m2)\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-12",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-12",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can examine the posterior probability of diabetes among this population (women who were at least 21 years old, and of Pima Indian heritage)\n\n\n\n\nd |>\n  add_epred_draws(m2, ndraws = 100) |>\n  ggplot(aes(x = mass, y = diab)) +\n  geom_line(aes(y = .epred, group = .draw), size = 0.1) +\n  geom_point(aes(mass, diab), alpha = 1/20, size = 0.5) +\n  xlab(\"BMI\") + ylab(\"Probability\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-13",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-13",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can also see the posterior predictive distribution of the probability of diabetes\n\n\n\n\nprop_diab <- function(x) {\n  mean(x == 1)\n}\npp_check(m2, nreps = 100,\n         plotfun = \"stat\", stat = \"prop_diab\") + xlab(\"Probability of Diabetes\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-14",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-14",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nAs before, we can compute predictions for new data using posterior_predict or by constructing the posterior distribution directly\nLet’s say we want to predict the probability of diabetes for a person with BMI = 40\n\n\n\n\nbmi_scaled <- (40 - mean(d$mass)) / 10\nyepred_m2 <- posterior_epred(m2, newdata = data.frame(bmi = bmi_scaled))\nquantile(yepred_m2, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n0.47 0.51 0.56 \n\nd_m2 <- as_tibble(m2) |>\n  mutate(log_odds = `(Intercept)` + bmi * bmi_scaled,\n         prob = invlogit(log_odds),\n         ypred = rbinom(2e3, size = 1, prob = prob))\nd_m2[1:3, ]\n\n# A tibble: 3 × 5\n  `(Intercept)`   bmi  log_odds  prob ypred\n          <dbl> <dbl>     <dbl> <dbl> <int>\n1        -0.676 0.897  0.000969 0.500     1\n2        -0.746 0.936 -0.0403   0.490     0\n3        -0.682 0.957  0.0402   0.510     0\n\nquantile(d_m2$prob, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n0.47 0.51 0.56"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-15",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-15",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe will extend this model and evaluate the model performance\nHere we set weakly informative priors on the other three coefficients\n\n\n\n\n\npriors <- normal(location = c(0.7, 0, 0, 0), \n                 scale = c(0.2, 1, 1, 1))\nm3 <- stan_glm(diab ~ bmi + pedigree + \n                 age + glucose,\n               prior_intercept = normal(0, 0.5),\n               prior = priors,\n               family = binomial(link = \"logit\"), \n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m3)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi + pedigree + age + glucose\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   5\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -0.8    0.1 -0.9  -0.8  -0.7 \nbmi          0.8    0.1  0.7   0.8   1.0 \npedigree     0.8    0.3  0.5   0.8   1.2 \nage          0.3    0.1  0.2   0.3   0.4 \nglucose      1.1    0.1  0.9   1.1   1.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2663 \nbmi           0.0  1.0  2623 \npedigree      0.0  1.0  2128 \nage           0.0  1.0  2551 \nglucose       0.0  1.0  2861 \nmean_PPD      0.0  1.0  2566 \nlog-posterior 0.1  1.0   747 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#shinystan-demo",
    "href": "bayes-course/07-lecture/07-lecture.html#shinystan-demo",
    "title": "Bayesian Inference",
    "section": "ShinyStan Demo",
    "text": "ShinyStan Demo\n\nTo use install.packages(\"shinystan\") and launch_shinystan(m3)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#making-additional-improvements",
    "href": "bayes-course/07-lecture/07-lecture.html#making-additional-improvements",
    "title": "Bayesian Inference",
    "section": "Making Additional Improvements",
    "text": "Making Additional Improvements\n\n\nThere is a good reason to believe that glucose and heredity interact so we will include an interaction term\nAge effects are rarely linear and so we include a B-Spline for non-linear age effects\n\n\n\n\n\nlibrary(splines)\npriors <- normal(location = c(0.7, rep(0, 7)), \n                 scale = c(0.2, rep(1, 7)))\nm4 <- stan_glm(diab ~ bmi + pedigree + \n                 bs(age, df = 4) + \n                 glucose + glucose:pedigree,\n               prior_intercept = normal(0, 0.5),\n               prior = priors,\n               family = binomial(link = \"logit\"), \n               data = d, \n               refresh = 0,\n               seed = 123,\n               iter = 1000)\nsummary(m4)\n\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      diab ~ bmi + pedigree + bs(age, df = 4) + glucose + glucose:pedigree\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 752\n predictors:   9\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      -1.6    0.3 -2.0  -1.6  -1.2 \nbmi               0.8    0.1  0.6   0.8   0.9 \npedigree          0.9    0.3  0.6   0.9   1.3 \nbs(age, df = 4)1  0.3    0.4 -0.2   0.3   0.8 \nbs(age, df = 4)2  2.6    0.6  1.9   2.6   3.3 \nbs(age, df = 4)3  0.7    0.7 -0.2   0.7   1.6 \nbs(age, df = 4)4 -0.6    0.8 -1.6  -0.6   0.4 \nglucose           1.1    0.1  1.0   1.1   1.3 \npedigree:glucose -0.6    0.3 -0.9  -0.6  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.3   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  1643 \nbmi              0.0  1.0  2228 \npedigree         0.0  1.0  2015 \nbs(age, df = 4)1 0.0  1.0  1758 \nbs(age, df = 4)2 0.0  1.0  1999 \nbs(age, df = 4)3 0.0  1.0  1853 \nbs(age, df = 4)4 0.0  1.0  2086 \nglucose          0.0  1.0  1749 \npedigree:glucose 0.0  1.0  1633 \nmean_PPD         0.0  1.0  1912 \nlog-posterior    0.1  1.0   960 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#example-diabetes-16",
    "href": "bayes-course/07-lecture/07-lecture.html#example-diabetes-16",
    "title": "Bayesian Inference",
    "section": "Example: Diabetes",
    "text": "Example: Diabetes\n\n\nWe can perform model comparison using several methods\nOne way is to assess classification accuracy under different probability cut points, which is often done in Machine Learning (ROC/AUC)\nA better way is to use LOO (loo and loo_compare in R)\n\n\n\n\n\npar(mar = c(3,3,2,1), \n    mgp = c(2,.7,0), \n    tck = -.01, \n    bg  = \"#f0f1eb\")\nm2_loo <- loo(m2)\nm3_loo <- loo(m3)\nm4_loo <- loo(m4)\nloo_compare(m2_loo, m3_loo, m4_loo)\n\n\n   elpd_diff se_diff\nm4    0.0       0.0 \nm3  -15.6       4.6 \nm2 -104.5      12.4 \n\nplot(m4_loo)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#introduction-to-hierarchical-models",
    "href": "bayes-course/07-lecture/07-lecture.html#introduction-to-hierarchical-models",
    "title": "Bayesian Inference",
    "section": "Introduction to Hierarchical Models",
    "text": "Introduction to Hierarchical Models\n\n\nYou can think about hierarchical (sometimes called multi-level or mixed effects) models from a data or parameter perspective, although the parameter view is more fundamental\nThe basic idea is that want to model all sources of potential variability in the data\nData often arrive in clusters, such as students within schools, patients with hospitals, voters within states, and so on\nSince there are local influences, it is often a good idea to assume that units share similar attributes within clusters as well as between clusters\nThese two sources of variability are likely different and we should treat them as such"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nLet’s look at a classic dataset called sleepstudy from lme4 package\nThese data are from the study described in Belenky et al. (2003), for the most sleep-deprived group (3 hours time-in-bed) and for the first 10 days of the study, up to the recovery period\n\n\n\n\np <- ggplot(aes(Days, Reaction), data = lme4::sleepstudy)\np + geom_point(size = 0.3) + geom_line(linewidth = 0.1) + facet_wrap(vars(Subject)) + ylab(\"Reaction time (ms)\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-1",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-1",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nTo get a better sense of the differences in per-Subject reaction time distributions, we can plot them side by side"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#pooling",
    "href": "bayes-course/07-lecture/07-lecture.html#pooling",
    "title": "Bayesian Inference",
    "section": "Pooling",
    "text": "Pooling\n\n\nPooling has to do with how much regularization we induce on parameter estimates in each cluster; sometimes this is called shrinkage\nComplete pooling ignores the clusters and estimates a global parameter\nEven though reaction time \\(y_i\\), belongs to subject \\(j\\), we ignore the groups and index all the \\(y\\)s together"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#pooling-1",
    "href": "bayes-course/07-lecture/07-lecture.html#pooling-1",
    "title": "Bayesian Inference",
    "section": "Pooling",
    "text": "Pooling\n\n\nComplete pooling is the opposite of no pooling, where we estimate a separate model for each group\nHere, we have \\(n\\) subjects, and \\(y_{ij}\\) refers to the \\(i\\)th reaction time in subject \\(j\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#partial-pooling",
    "href": "bayes-course/07-lecture/07-lecture.html#partial-pooling",
    "title": "Bayesian Inference",
    "section": "Partial Pooling",
    "text": "Partial Pooling\n\n\nPartial pooling is the compromise between the two extremes\nLike any other parameter in a Bayesian model, the global hyperparameter \\(\\tau\\) is given a prior and is learned from the data\nThere could be multiple levels of nesting, say students within schools, within states, etc."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#partial-pooling-compromise",
    "href": "bayes-course/07-lecture/07-lecture.html#partial-pooling-compromise",
    "title": "Bayesian Inference",
    "section": "Partial Pooling Compromise",
    "text": "Partial Pooling Compromise\n\n\nPosterior \\(f(\\theta | y)\\) is is a compromise between prior \\(f(\\theta)\\) and likelihood \\(f(y | \\theta)\\)\nIn the same spirit, the pooled parameter \\(\\theta_j\\) (say reaction time for subject \\(j\\)) is a compromise between within-subject parameters, and among-subject parameters\n\n\n\n\\[\n\\theta_j \\approx \\frac{\\frac{n_j}{\\sigma_{y}^2} \\overline y_j + \\frac{1}{\\sigma_{\\tau}^2} \\overline y_{\\tau}} {\\frac{n_j}{\\sigma_{y}^2} + \\frac{1}{\\sigma_{\\tau}^2}}\n\\]\n\n\n\n\\(\\overline y_j\\) is no-pool estimate of average reaction time for subject \\(j\\), and \\(\\overline y_{\\tau}\\) is complete-pool estimate\n\\(n_j\\) is the number of observations for subject \\(j\\), \\(\\sigma_{y}^2\\) is within subject variance of reaction times, and \\(\\sigma_{\\tau}^2\\) is the between-subject variance"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-2",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-2",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nLet’s build three models for the sleep data, starting with complete pooling, which is what we have been doing all along\nWe will start with the intercept-only model\nThese data contain the same number of observations per Subject, which is unusual, so we will make it more realistic by removing 30% of the measurements\n\n\n\n\n\nset.seed(123)\nn <- nrow(lme4::sleepstudy)\ns <- sample(1:n, n * 0.7)\nd <- lme4::sleepstudy[s, ] \np1 <- ggplot(aes(Days, Reaction), \n            data = d)\np1 <- p1 + geom_jitter(size = 0.3, width = 0.2)\np2 <- ggplot(aes(Reaction), \n            data = d)\np2 <- p2 + geom_histogram(binwidth = 20)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-3",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-3",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nThis is a simple model of the mean reaction time \\(\\mu\\)\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} & \\sim & \\text{Normal}(\\mu, \\ \\sigma^2) \\\\\n\\mu    & \\sim & \\text{Normal}(300, 10^2) \\\\\n\\sigma & \\sim & \\text{Exponential}(0.02)\n\\end{eqnarray}\n\\]\n\n\n\n\nm1 <- stan_glm(Reaction ~ 1,\n               prior_intercept = normal(300, 10),\n               prior_aux = exponential(0.02),\n               family = gaussian,\n               data = d, \n               iter = 5000,\n               refresh = 0,\n               seed = 123)\nsummary(m1)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Reaction ~ 1\n algorithm:    sampling\n sample:       10000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n predictors:   1\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept) 294.5    4.2 289.2 294.5 299.9\nsigma        52.9    3.4  48.7  52.7  57.3\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 294.4    6.3 286.4 294.4 302.5\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.1  1.0  6347 \nsigma         0.0  1.0  6890 \nmean_PPD      0.1  1.0  8264 \nlog-posterior 0.0  1.0  4152 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-4",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-4",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can compare the predictions from the complete-pooling model to the reaction times of each subject\n\n\n\n\nnew <- d |> group_by(Subject) |> summarise(Reaction = mean(Reaction)) |>\n  arrange(Reaction)\nnew_pred <- posterior_predict(m1, newdata = new)\nppc_intervals(new$Reaction, yrep = new_pred) +\n  scale_x_continuous(labels = new$Subject,  breaks = 1:nrow(new)) +\n  xlab(\"Subject\") + ylab(\"Reaction\")"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-5",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-5",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nNext, we will fit a no-pool model, which means we are fitting 18 separate models, one for each subject\nYou can fit them separately, by making 18 calls to stan_glm, or you can do in all at once\nSince we will not run 18 separate regressions, it will be simpler to estimate one global variance parameter \\(\\sigma\\), which means there is some variance pooling\nThis will not affect the inferences for \\(\\mu_j\\) which is our main focus here\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} & \\sim & \\text{Normal}(\\mu_j, \\ \\sigma^2) \\\\\n\\mu_j    & \\sim & \\text{Normal}(300, s_j^2) \\\\\n\\sigma & \\sim & \\text{Exponential}(0.02)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-6",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-6",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nm2 <- stan_glm(Reaction ~ Subject - 1,\n               prior = normal(300, 10, autoscale = TRUE),\n               prior_aux = exponential(0.02),\n               family = gaussian,\n               data = d, \n               iter = 1000,\n               refresh = 0,\n               seed = 123)\nsummary(m2)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      Reaction ~ Subject - 1\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n predictors:   18\n\nEstimates:\n             mean   sd    10%   50%   90%\nSubject308 353.4   17.1 331.0 353.8 375.1\nSubject309 213.0   14.6 194.4 212.8 231.8\nSubject310 223.9   12.9 207.0 224.0 240.2\nSubject330 305.2   12.7 288.6 305.1 321.6\nSubject331 310.0   15.3 290.1 310.2 329.3\nSubject332 272.0   15.2 252.6 271.8 291.7\nSubject333 306.1   16.9 284.3 306.0 327.7\nSubject334 296.9   12.2 281.1 297.1 312.5\nSubject335 248.6   13.2 231.4 248.8 265.1\nSubject337 359.4   14.3 341.2 359.4 378.3\nSubject349 282.0   15.6 262.2 282.0 301.1\nSubject350 336.6   14.5 317.6 336.7 354.8\nSubject351 280.5   18.3 256.7 280.8 303.6\nSubject352 346.1   16.5 325.2 345.9 367.1\nSubject369 294.7   14.3 276.4 294.6 313.6\nSubject370 262.1   14.9 243.7 262.2 281.4\nSubject371 295.4   11.7 280.3 295.2 309.9\nSubject372 317.6   11.9 302.7 317.6 333.0\nsigma       37.4    2.6  34.4  37.2  40.7\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.2    4.8 287.1 293.2 299.2\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\nSubject308    0.3  1.0  2742 \nSubject309    0.3  1.0  3019 \nSubject310    0.3  1.0  2411 \nSubject330    0.3  1.0  2263 \nSubject331    0.3  1.0  2563 \nSubject332    0.3  1.0  2924 \nSubject333    0.3  1.0  2560 \nSubject334    0.2  1.0  2928 \nSubject335    0.2  1.0  3386 \nSubject337    0.3  1.0  2424 \nSubject349    0.3  1.0  2629 \nSubject350    0.3  1.0  2490 \nSubject351    0.4  1.0  2272 \nSubject352    0.3  1.0  2909 \nSubject369    0.3  1.0  2915 \nSubject370    0.3  1.0  2594 \nSubject371    0.2  1.0  2399 \nSubject372    0.2  1.0  2760 \nsigma         0.1  1.0  1113 \nmean_PPD      0.1  1.0  2215 \nlog-posterior 0.1  1.0   719 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-7",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-7",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can compare the predictions from the no-pooling model to the reaction times of each subject\n\n\n\n\nnew_pred <- posterior_predict(m2, newdata = new)\nppc_intervals(new$Reaction, yrep = new_pred) +\n  scale_x_continuous(labels = new$Subject,  breaks = 1:nrow(new)) +\n  xlab(\"Subject\") + ylab(\"Reaction\")\n\n\n\n\n\n\n\n\n\n\n\nWhat are some of the limitations of this approach?"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-hierarchical-model",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-hierarchical-model",
    "title": "Bayesian Inference",
    "section": "Building a Hierarchical Model",
    "text": "Building a Hierarchical Model\n\n\n\n\n\n\n\n\\(\\mu_j\\) is an average reaction time for subject \\(j\\)\n\\(\\sigma_y\\) is the within-subject variability of reaction times\n\\(\\mu\\) is the global average of reaction times across all subject\n\\(\\sigma_{\\mu}\\) is subject to subject variability of reaction times\nTop-level parameters get fixed priors and induce the degree of pooling"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-8",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-8",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can now write out the full model giving priors to all global parameters\nNotice, how the model could be extended if instead of \\(\\mu_j\\) we had our typical linear predictor\n\n\n\n\\[\n\\begin{eqnarray}\ny_{ij} &\\sim& \\text{Normal}(\\mu_j, \\ \\sigma_y^2) \\\\\n\\mu_{j} &\\sim& \\text{Normal}(\\mu, \\ \\sigma_{\\mu}^2) \\\\\n\\mu &\\sim& \\text{Normal}(300, 50^2) \\\\\n\\sigma_y &\\sim& \\text{Exponential}(0.02) \\\\\n\\sigma_{\\mu} &\\sim& \\text{Exponential}(1)\n\\end{eqnarray}\n\\]\n\n\n\nThis model can also be written such that \\(\\mu_j = \\mu + b_j\\), where each \\(b_j \\sim \\text{Normal}(0, \\sigma_{\\mu}^2)\\)"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-9",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-9",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\n\n\nm3 <- stan_glmer(Reaction ~ (1 | Subject),\n                 prior_intercept = normal(300, 50),\n                 prior_aux = exponential(0.02),\n                 prior_covariance = decov(reg = 1, \n                   conc = 1, shape = 1, scale = 1),\n                 family = gaussian, data = d, iter = 1500,\n                 refresh = 0,\n                 seed = 123)\n\n\n\n\n\n(Intercept) = \\(\\mu\\)\nsigma = \\(\\sigma_y\\)\nSigma[Subject:(Intercept),(Intercept)] = \\(\\sigma^2_{\\mu}\\)\nb[(Intercept) Subject:XYZ] = \\(b_j\\), and so \\(\\mu_j = \\mu + \\beta_j\\)\n\n\n\n\n\n\nsummary(m3)\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      Reaction ~ (1 | Subject)\n algorithm:    sampling\n sample:       3000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n groups:       Subject (18)\n\nEstimates:\n                                         mean   sd     10%    50%    90% \n(Intercept)                             294.0   10.2  281.0  294.0  306.9\nb[(Intercept) Subject:308]               49.3   18.7   25.7   49.2   73.3\nb[(Intercept) Subject:309]              -71.1   16.5  -91.8  -71.0  -50.1\nb[(Intercept) Subject:310]              -62.1   15.5  -82.0  -61.9  -42.4\nb[(Intercept) Subject:330]               10.1   15.4   -9.0    9.8   29.3\nb[(Intercept) Subject:331]               14.1   17.0   -8.1   14.3   35.6\nb[(Intercept) Subject:332]              -19.2   16.8  -40.2  -19.5    2.3\nb[(Intercept) Subject:333]                9.9   17.5  -12.3   10.0   31.7\nb[(Intercept) Subject:334]                3.0   14.9  -15.6    3.0   22.1\nb[(Intercept) Subject:335]              -40.2   15.6  -60.1  -40.2  -20.2\nb[(Intercept) Subject:337]               57.2   16.4   36.5   57.2   78.3\nb[(Intercept) Subject:349]              -10.5   17.1  -32.2  -10.7   11.1\nb[(Intercept) Subject:350]               37.4   15.7   18.1   37.3   57.4\nb[(Intercept) Subject:351]              -11.0   18.4  -34.5  -10.8   12.8\nb[(Intercept) Subject:352]               43.4   18.1   21.0   43.0   67.4\nb[(Intercept) Subject:369]               -0.1   16.0  -20.7    0.2   20.0\nb[(Intercept) Subject:370]              -27.5   17.3  -49.3  -27.9   -5.2\nb[(Intercept) Subject:371]                0.9   14.8  -17.6    0.7   19.8\nb[(Intercept) Subject:372]               21.6   14.8    2.8   21.5   40.8\nsigma                                    37.6    2.6   34.3   37.5   41.0\nSigma[Subject:(Intercept),(Intercept)] 1647.0  718.9  915.6 1502.6 2558.2\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.0    4.6 287.0 293.0 299.1\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                       mcse Rhat n_eff\n(Intercept)                             0.4  1.0  571 \nb[(Intercept) Subject:308]              0.5  1.0 1444 \nb[(Intercept) Subject:309]              0.5  1.0 1212 \nb[(Intercept) Subject:310]              0.4  1.0 1193 \nb[(Intercept) Subject:330]              0.4  1.0 1195 \nb[(Intercept) Subject:331]              0.5  1.0 1357 \nb[(Intercept) Subject:332]              0.4  1.0 1458 \nb[(Intercept) Subject:333]              0.5  1.0 1406 \nb[(Intercept) Subject:334]              0.5  1.0  984 \nb[(Intercept) Subject:335]              0.5  1.0 1200 \nb[(Intercept) Subject:337]              0.5  1.0 1152 \nb[(Intercept) Subject:349]              0.5  1.0 1383 \nb[(Intercept) Subject:350]              0.4  1.0 1234 \nb[(Intercept) Subject:351]              0.4  1.0 1802 \nb[(Intercept) Subject:352]              0.5  1.0 1446 \nb[(Intercept) Subject:369]              0.5  1.0 1161 \nb[(Intercept) Subject:370]              0.5  1.0 1352 \nb[(Intercept) Subject:371]              0.5  1.0 1060 \nb[(Intercept) Subject:372]              0.5  1.0 1014 \nsigma                                   0.1  1.0 2069 \nSigma[Subject:(Intercept),(Intercept)] 26.5  1.0  734 \nmean_PPD                                0.1  1.0 3059 \nlog-posterior                           0.2  1.0  549 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#computing-mu_j",
    "href": "bayes-course/07-lecture/07-lecture.html#computing-mu_j",
    "title": "Bayesian Inference",
    "section": "Computing \\(\\mu_j\\)",
    "text": "Computing \\(\\mu_j\\)\n\n\nmuj <- m3 |>\n  spread_draws(`(Intercept)`, b[ ,Subject]) |>\n  mutate(mu_j = `(Intercept)` + b) |>\n  select(Subject, mu_j) |>\n  mean_qi(.width = 0.90)\n\nhead(muj)\n\n# A tibble: 6 × 7\n  Subject      mu_j .lower .upper .width .point .interval\n  <chr>       <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n1 Subject:308  343.   316.   370.    0.9 mean   qi       \n2 Subject:309  223.   200.   246.    0.9 mean   qi       \n3 Subject:310  232.   211.   253.    0.9 mean   qi       \n4 Subject:330  304.   284.   325.    0.9 mean   qi       \n5 Subject:331  308.   285.   332.    0.9 mean   qi       \n6 Subject:332  275.   251.   298.    0.9 mean   qi"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#sleep-data-10",
    "href": "bayes-course/07-lecture/07-lecture.html#sleep-data-10",
    "title": "Bayesian Inference",
    "section": "Sleep Data",
    "text": "Sleep Data\n\n\nWe can see the effects of hierarchical pooling below\nSubjects that are farther away from \\(\\E(\\mu) = 294\\)(ms) and the ones that have fewer observations are pooled more towards the global mean"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model",
    "title": "Bayesian Inference",
    "section": "Building a Full Hierarchical Model",
    "text": "Building a Full Hierarchical Model\n\n\nThe model we have built so far is for average reaction time only\nWhat we would like to do, is to build a pooled regression model for the reaction time of each subject\nThe mechanics are similar but now we need a model for the slopes and intercepts for each subject, their covariance, and priors\n\n\n\n\nm4 <- stan_glmer(Reaction ~ Days + (Days | Subject),\n                 prior_intercept = normal(300, 50),\n                 prior = normal(0, 2, autoscale = TRUE),\n                 prior_aux = exponential(0.02),\n                 prior_covariance = decov(reg = 1, \n                   conc = 1, shape = 1, scale = 1),\n                 family = gaussian, data = d, iter = 1500,\n                 cores = 4, seed = 123, refresh = 0)\nsummary(m4)\n\n\nModel Info:\n function:     stan_glmer\n family:       gaussian [identity]\n formula:      Reaction ~ Days + (Days | Subject)\n algorithm:    sampling\n sample:       3000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 125\n groups:       Subject (18)\n\nEstimates:\n                                         mean   sd     10%    50%    90% \n(Intercept)                             256.6    7.7  246.9  256.7  266.4\nDays                                      9.0    1.7    6.8    9.0   11.1\nb[(Intercept) Subject:308]               41.2   22.4   15.0   38.8   72.3\nb[Days Subject:308]                       0.9    4.4   -5.0    1.4    6.1\nb[(Intercept) Subject:309]              -43.6   14.0  -61.7  -43.5  -25.7\nb[Days Subject:309]                      -7.9    3.1  -11.8   -7.8   -4.0\nb[(Intercept) Subject:310]              -43.0   13.7  -60.4  -42.5  -26.2\nb[Days Subject:310]                      -5.0    2.9   -8.8   -5.0   -1.4\nb[(Intercept) Subject:330]               23.3   14.4    5.0   23.2   41.9\nb[Days Subject:330]                      -4.1    2.7   -7.6   -4.2   -0.6\nb[(Intercept) Subject:331]               18.6   12.6    2.6   18.5   34.6\nb[Days Subject:331]                       0.7    2.9   -3.0    0.8    4.2\nb[(Intercept) Subject:332]                2.4   13.1  -14.1    2.2   19.2\nb[Days Subject:332]                      -5.1    3.0   -9.0   -5.0   -1.4\nb[(Intercept) Subject:333]               11.4   14.5   -6.6   10.7   30.7\nb[Days Subject:333]                       0.1    3.1   -3.9    0.2    4.0\nb[(Intercept) Subject:334]               -9.9   13.0  -26.8   -9.7    6.6\nb[Days Subject:334]                       2.3    2.7   -1.0    2.2    5.7\nb[(Intercept) Subject:335]               -7.4   14.1  -25.2   -7.8   10.5\nb[Days Subject:335]                      -8.9    2.8  -12.5   -8.9   -5.4\nb[(Intercept) Subject:337]               33.0   13.2   16.0   32.8   49.7\nb[Days Subject:337]                       9.7    3.1    5.8    9.7   13.7\nb[(Intercept) Subject:349]              -25.5   18.2  -50.2  -24.1   -3.8\nb[Days Subject:349]                       1.4    3.4   -2.8    1.2    5.9\nb[(Intercept) Subject:350]              -11.1   16.6  -32.2  -10.4    9.9\nb[Days Subject:350]                       8.1    3.1    4.2    8.0   12.2\nb[(Intercept) Subject:351]                2.4   17.3  -18.4    1.5   24.8\nb[Days Subject:351]                      -4.0    3.8   -8.8   -3.9    0.5\nb[(Intercept) Subject:352]               26.0   16.4    5.7   25.4   46.7\nb[Days Subject:352]                       3.5    3.2   -0.5    3.5    7.5\nb[(Intercept) Subject:369]               -0.7   13.0  -17.4   -0.6   15.6\nb[Days Subject:369]                       1.1    2.8   -2.4    1.1    4.6\nb[(Intercept) Subject:370]              -31.2   14.8  -50.1  -30.8  -12.4\nb[Days Subject:370]                       4.7    3.6    0.1    4.7    9.4\nb[(Intercept) Subject:371]               -2.3   12.5  -18.3   -2.3   13.7\nb[Days Subject:371]                       0.1    2.6   -3.1    0.1    3.3\nb[(Intercept) Subject:372]                9.7   12.8   -6.1    9.5   26.0\nb[Days Subject:372]                       2.3    2.5   -0.9    2.3    5.5\nsigma                                    22.2    1.8   20.0   22.0   24.5\nSigma[Subject:(Intercept),(Intercept)]  843.3  439.5  390.6  753.7 1416.4\nSigma[Subject:Days,(Intercept)]           8.6   64.6  -70.4   13.9   79.6\nSigma[Subject:Days,Days]                 44.2   22.6   21.4   39.6   72.3\n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD 293.3    2.9 289.6 293.3 296.9\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                       mcse Rhat n_eff\n(Intercept)                             0.2  1.0 1326 \nDays                                    0.0  1.0 1395 \nb[(Intercept) Subject:308]              0.5  1.0 2089 \nb[Days Subject:308]                     0.1  1.0 2190 \nb[(Intercept) Subject:309]              0.3  1.0 2647 \nb[Days Subject:309]                     0.1  1.0 2656 \nb[(Intercept) Subject:310]              0.3  1.0 2099 \nb[Days Subject:310]                     0.1  1.0 2209 \nb[(Intercept) Subject:330]              0.3  1.0 1920 \nb[Days Subject:330]                     0.1  1.0 1767 \nb[(Intercept) Subject:331]              0.3  1.0 2055 \nb[Days Subject:331]                     0.1  1.0 2328 \nb[(Intercept) Subject:332]              0.3  1.0 2318 \nb[Days Subject:332]                     0.1  1.0 2490 \nb[(Intercept) Subject:333]              0.3  1.0 2660 \nb[Days Subject:333]                     0.1  1.0 2089 \nb[(Intercept) Subject:334]              0.3  1.0 2165 \nb[Days Subject:334]                     0.1  1.0 1884 \nb[(Intercept) Subject:335]              0.3  1.0 2025 \nb[Days Subject:335]                     0.1  1.0 2125 \nb[(Intercept) Subject:337]              0.3  1.0 2728 \nb[Days Subject:337]                     0.1  1.0 2569 \nb[(Intercept) Subject:349]              0.4  1.0 1744 \nb[Days Subject:349]                     0.1  1.0 1996 \nb[(Intercept) Subject:350]              0.4  1.0 1529 \nb[Days Subject:350]                     0.1  1.0 1472 \nb[(Intercept) Subject:351]              0.3  1.0 2654 \nb[Days Subject:351]                     0.1  1.0 2187 \nb[(Intercept) Subject:352]              0.3  1.0 2524 \nb[Days Subject:352]                     0.1  1.0 2224 \nb[(Intercept) Subject:369]              0.3  1.0 2247 \nb[Days Subject:369]                     0.1  1.0 2225 \nb[(Intercept) Subject:370]              0.4  1.0 1623 \nb[Days Subject:370]                     0.1  1.0 1856 \nb[(Intercept) Subject:371]              0.2  1.0 2557 \nb[Days Subject:371]                     0.1  1.0 2073 \nb[(Intercept) Subject:372]              0.3  1.0 2428 \nb[Days Subject:372]                     0.1  1.0 2246 \nsigma                                   0.0  1.0 1572 \nSigma[Subject:(Intercept),(Intercept)] 12.3  1.0 1280 \nSigma[Subject:Days,(Intercept)]         2.2  1.0  872 \nSigma[Subject:Days,Days]                0.7  1.0 1088 \nmean_PPD                                0.1  1.0 3134 \nlog-posterior                           0.3  1.0  584 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model-1",
    "href": "bayes-course/07-lecture/07-lecture.html#building-a-full-hierarchical-model-1",
    "title": "Bayesian Inference",
    "section": "Building a Full Hierarchical Model",
    "text": "Building a Full Hierarchical Model"
  },
  {
    "objectID": "bayes-course/07-lecture/07-lecture.html#predicting-for-a-new-subject",
    "href": "bayes-course/07-lecture/07-lecture.html#predicting-for-a-new-subject",
    "title": "Bayesian Inference",
    "section": "Predicting for a New Subject",
    "text": "Predicting for a New Subject\n\n\nWe saw that we can make predictions for reaction times of subjects that were part of the model\nBut we can also make predictions for unobserved subjects by drawing from the “population” distribution, as opposed to subject-specific parameters\n\n\n\n\n\nnew_subj <- data.frame(Days = 0:9, \n              Subject = as.factor(rep(400, 10)))\nypred_subj <- posterior_predict(m4, \n                        newdata = new_subj)\nnew_subj |>\n  add_predicted_draws(m4) |>\n  ggplot(aes(x = Days)) +\n  stat_lineribbon(aes(y = .prediction), \n                  width = c(.9, .8, .5), \n                  alpha = 0.25) +\n  ylab(\"Reaction time\") + \n  ggtitle(\"Prediction for an unobserved subject\") +\n  scale_fill_brewer(palette = \"Greys\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#more-linear-models-and-modeling-counts",
    "href": "bayes-course/06-lecture/06-lecture.html#more-linear-models-and-modeling-counts",
    "title": "Bayesian Inference",
    "section": "More Linear Models and Modeling Counts",
    "text": "More Linear Models and Modeling Counts\n\n\n\n\nImproving the model by thinking about the DGP\nMore on model evaluation and comparison\nModeling count data with Poisson\nModel evaluation and overdispersion\nNegative binomial model for counts\nGeneralized linear models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#motivating-example",
    "href": "bayes-course/06-lecture/06-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\nAt the end of the last lecture we saw that the linear model did not capture the relationship between height and weight very well\nThat’s not surprising: the process can’t be linear as it has a natural lower and upper bound\nTo remedy this situation, we have to think generatively: either biologically or geometrically/physically\nThe biology of growth is very complex – we would have to think about what causes primate (or animal) growth and how growth translates into height and weight, which is likely affected by genetic and environmental factors\nFortunately, there is a more straightforward, geometrical approach\nRichard McElreath has a nice presentation in Chapter 16 of his book (2nd edition) – we reproduce a simplified version here"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#deriving-the-model",
    "href": "bayes-course/06-lecture/06-lecture.html#deriving-the-model",
    "title": "Bayesian Inference",
    "section": "Deriving the model",
    "text": "Deriving the model\n\n\n\n\nIn the spirit of the spherical cow, we can think of a person as a cylinder\nThe volume of the cylinder is: \\(V = \\pi r^2 h\\), where \\(r\\) is a person’s radius and \\(h\\) is the height\nIt seems reasonable to assume that a person’s width (\\(2r\\)) is proportional to the height \\(h\\): \\(r = kh\\) where \\(k\\) is the proportionality constant\nTherefore: \\(V = \\pi r^2 h = \\pi (kh)^2 h = \\theta h^3\\) where \\(\\theta\\) absorbed other constant terms\nIf the human body has approximately the same density, weight should be proportional to Volume: \\(w = kV\\), \\(w = k\\theta h^3\\)\nWe will absorbe \\(k\\) into \\(\\theta\\), and so \\(w = \\theta h^3\\), so the weight is proportional to the cube of height"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#deriving-the-model-1",
    "href": "bayes-course/06-lecture/06-lecture.html#deriving-the-model-1",
    "title": "Bayesian Inference",
    "section": "Deriving the model",
    "text": "Deriving the model\n\n\nWe can therefore write the model in the following way: \\[\n\\begin{eqnarray}\nw_i & \\sim & \\text{LogNormal}(\\mu_i, \\sigma)  \\\\\n\\exp(\\mu_i) & = & \\theta h_i^3 \\\\\n\\theta & \\sim & \\text{prior}_{\\theta}(.) \\\\\n\\sigma & \\sim & \\text{Exponetial}(1)\n\\end{eqnarray}\n\\]\nWeight is a positive quantity and we give it a LogNormal distribution\n\\(\\exp(\\mu_i)\\) is the median of a LogNormal, which is where we specify our cubic relationship between weight and height\nNotice that the model for the conditional median is \\(\\mu_i = \\log(\\theta) + 3 \\log(h_i)\\), in other words, we do not need to estimate the coefficient on height, we only need the intercept\nIn RStanArm, we can estimate a similar model as a linear regression of log weight on log height; (in Stan, we can write this model directly)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#choosing-priors",
    "href": "bayes-course/06-lecture/06-lecture.html#choosing-priors",
    "title": "Bayesian Inference",
    "section": "Choosing Priors",
    "text": "Choosing Priors\n\n\nIn our log-log linear regression we have an intercept and coefficient on log height, which we said was 3\nInstead of fixing it at 3, we will estimate it and give it an informative prior, where most of the mass is between 2 and 4\nThe implies something like \\(\\beta \\sim \\text{Normal}(3, 0.3)\\)\nWe will leave our \\(\\sigma \\sim \\text{Exponetial}(1)\\)\nWe have less intuition about the intercept, so we will give it a wider prior on a scale of centered predictors (RStanArm centers by default): \\(\\alpha \\sim \\text{Normal}(0, 5)\\)\nHow do we know these priors are reasonable on the predictive scale (weight)?\nWe will perform another prior predictive simulation"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nCompute the new log variables:\n\n\n\n\n\n\n\nd <- readr::read_csv(\"../05-lecture/data/howell.csv\")\nd <- d |>\n  mutate(log_h = log(height),\n         log_w = log(weight))\n\n\n\n\nRun prior predictive simulation:\n\n\n\n\nm3 <- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 5),\n  prior_PD = 1,  # don't evaluate the likelyhood\n  seed = 1234,\n  chains = 4,\n  iter = 600\n)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-1",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-1",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nlibrary(tidybayes)\nd |>\n  add_epred_draws(m3, ndraws = 100) |>\n  ggplot(aes(y = log_w, x = log_h)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = .epred, group = .draw), alpha = 0.25) +\n  xlab(\"Log Height\") + ylab(\"Log Weight\") + ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-2",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-2",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nWe can examine what this looks like on the original scale by exponentiating the predictions:\n\n\n\n\nd |>\n  add_epred_draws(m3, ndraws = 100) |>\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = exp(.epred), group = .draw), color = 'green', alpha = 0.25) +\n  xlab(\"Height\") + ylab(\"Weight\") + ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-3",
    "href": "bayes-course/06-lecture/06-lecture.html#prior-predictive-simulation-3",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nOur intercept scale seems too wide, so we will make some adjustments:\n\n\n\n\n\nm3 <- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 2.5),\n  prior_PD = 1,  # don't evaluate the likelihood\n  seed = 1234,\n  refresh = 0,\n  chains = 4,\n  iter = 600\n)\nd |>\n  add_epred_draws(m3, ndraws = 100) |>\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(y = exp(.epred), group = .draw), \n            alpha = 0.25, color = 'green') +\n  xlab(\"Height\") + ylab(\"Weight\") + \n  ggtitle(\"Prior Predictive Simulation\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#fitting-the-model",
    "href": "bayes-course/06-lecture/06-lecture.html#fitting-the-model",
    "title": "Bayesian Inference",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nWe can likely do better with these priors, but most of the simulations are covering the data and so we proceed to model fitting\n\n\n\n\nm3 <- stan_glm(\n  log_w ~ log_h,\n  data = d,\n  family = gaussian,\n  prior = normal(3, 0.3),\n  prior_aux = exponential(1),\n  prior_intercept = normal(0, 2.5), \n  seed = 1234,\n  refresh = 0,\n  chains = 4,\n  iter = 600\n)\nprior_summary(m3)\n\nPriors for model 'm3' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 2.5)\n\nCoefficients\n ~ normal(location = 3, scale = 0.3)\n\nAuxiliary (sigma)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\nsummary(m3)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      log_w ~ log_h\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 544\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -8.0    0.1 -8.1  -8.0  -7.8 \nlog_h        2.3    0.0  2.3   2.3   2.4 \nsigma        0.1    0.0  0.1   0.1   0.1 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 3.4    0.0  3.4   3.4   3.5  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   619 \nlog_h         0.0  1.0   621 \nsigma         0.0  1.0   923 \nmean_PPD      0.0  1.0  1436 \nlog-posterior 0.1  1.0   529 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#comparing-to-the-linear-model",
    "href": "bayes-course/06-lecture/06-lecture.html#comparing-to-the-linear-model",
    "title": "Bayesian Inference",
    "section": "Comparing to the Linear Model",
    "text": "Comparing to the Linear Model\n\n\n\nm2 <- readr::read_rds(\"../05-lecture/models/m2.rds\")\np1 <- pp_check(m2) + xlab(\"Weight (kg)\") + ggtitle(\"Linear Model\")\np2 <- pp_check(m3) + xlab(\"Log Weight\") + ggtitle(\"Log-Log Model\")\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#plotting-prediction-intervals",
    "href": "bayes-course/06-lecture/06-lecture.html#plotting-prediction-intervals",
    "title": "Bayesian Inference",
    "section": "Plotting Prediction Intervals",
    "text": "Plotting Prediction Intervals\n\n\nd |>\n  add_predicted_draws(m3) |>\n  ggplot(aes(y = weight, x = height)) +\n  geom_point(size = 0.5, alpha = 0.2) +\n  stat_lineribbon(aes(y = exp(.prediction)), .width = c(0.90, 0.50), alpha = 0.25) +\n  xlab(\"Height (cm)\") + ylab(\"Weight (kg)\") + ggtitle(\"In Sample Predictions of Weight from Height\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#predicting-for-new-data",
    "href": "bayes-course/06-lecture/06-lecture.html#predicting-for-new-data",
    "title": "Bayesian Inference",
    "section": "Predicting For New Data",
    "text": "Predicting For New Data\n\n\nlog_h <- seq(0, 5.2, len = 500)\nnew_data <- tibble(log_h)\npred <- add_predicted_draws(new_data, m3)\npred |>\n  ggplot(aes(x = exp(log_h), y = exp(.prediction))) +\n  stat_lineribbon(.width = c(0.90, 0.50), alpha = 0.25) +\n  xlab(\"Height (cm)\") + ylab(\"Weight (kg)\") + ggtitle(\"Predictions of Weight from Height\") + \n  geom_point(aes(y = weight, x = height), size = 0.5, alpha = 0.2, data = d)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nTo build up a larger regression model, we will take a look at the quality of wine dataset from the UCI machine learning repository"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-1",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-1",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nOur task is to predict the (subjective) quality of wine from measurements like acidity, sugar, and chlorides\nThe outcome is ordinal, which should be analyzed using ordinal regression, but we will start with linear regression\n\n\n\n\n\nd <- readr::read_delim(\"data/winequality-red.csv\")\n\n# remove duplicates\nd <- d[!duplicated(d), ]\np1 <- ggplot(aes(x = quality), data = d)\np1 <- p1 + geom_histogram() + \n  ggtitle(\"Red wine quality ratings\")\np2 <- ggplot(aes(quality, alcohol), data = d)\np2 <- p2 + \n  geom_point(position = \n               position_jitter(width = 0.2),\n             size = 0.3)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-2",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-2",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nAs before, we will center the predictors, but this time we will also divide by standard deviation\nThis will make the coefficients comparable\nIf you have binary inputs, it may make sense to divide by 2 standard deviations (Page 186 in Regresion and Other Stories)\nWe will also center the quality score\n\n\n\n\nds <- d |>\n  scale() |>\n  as_tibble() |>\n  mutate(quality = d$quality - 5.5)\nhead(ds)\n\n# A tibble: 6 × 12\n  fixed_acidity volatile_acidity citric_acid residual_sugar chlorides\n          <dbl>            <dbl>       <dbl>          <dbl>     <dbl>\n1        -0.524            0.932       -1.39        -0.461    -0.246 \n2        -0.294            1.92        -1.39         0.0566    0.200 \n3        -0.294            1.26        -1.19        -0.165     0.0785\n4         1.66            -1.36         1.47        -0.461    -0.266 \n5        -0.524            0.713       -1.39        -0.535    -0.266 \n6        -0.236            0.385       -1.09        -0.683    -0.387 \n# ℹ 7 more variables: free_sulfur_dioxide <dbl>, total_sulfur_dioxide <dbl>,\n#   density <dbl>, pH <dbl>, sulphates <dbl>, alcohol <dbl>, quality <dbl>"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-3",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-3",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can now fit our first regression to alcohol only\nAfter standardization, and since we don’t know much about wine, we can set weakly informative priors\n\n\n\n\n\n\n\nm1 <- stan_glm(quality ~ alcohol, \n               data = ds,\n               family = gaussian,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               iter = 500,\n               chains = 4)\nsummary(m1)\n\n\n\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ alcohol\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.1    0.0  0.1   0.1   0.2  \nalcohol     0.4    0.0  0.4   0.4   0.4  \nsigma       0.7    0.0  0.7   0.7   0.7  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.1    0.0  0.1   0.1   0.2  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  759  \nalcohol       0.0  1.0  720  \nsigma         0.0  1.0  905  \nmean_PPD      0.0  1.0  862  \nlog-posterior 0.1  1.0  373  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#priors-in-rstanarm",
    "href": "bayes-course/06-lecture/06-lecture.html#priors-in-rstanarm",
    "title": "Bayesian Inference",
    "section": "Priors in RStanArm",
    "text": "Priors in RStanArm\n\n\nWhen we say prior = normal(0, 1) in RStanArm, every \\(\\beta\\), except for the intercept will be given this prior\nWhen setting informative priors, you may want to set a specific prior for each \\(\\beta\\)\nSuppose your model is: \\[\ny_i \\sim \\mathsf{Normal}\\left(\\alpha + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}, \\, \\sigma\\right)\n\\]\n\n\n\n\nAnd you want to put a \\(\\text{Normal}(-3, 1)\\) on \\(\\beta_1\\) and \\(\\text{Normal}(2, 0.1)\\) on \\(\\beta_2\\)\n\n\n\n\nmy_prior <- normal(location = c(-3, 2), scale = c(1, 0.1))\nstan_glm(y ~ x1 + x2, data = dat, prior = my_prior)\n\n\n\n\nRefer to this vignette for more information about this topic"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-4",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-4",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can look at the inference using mcmc_areas\n\n\n\n\nlibrary(bayesplot)\nmcmc_areas(m1)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-5",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-5",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nLet’s predict the rating at the high and low alcohol content\nOn a standardized scale, that would correspond to an alcohol measurement of 4 and -2 (or about 8 and 15 on the original scale)\n\n\n\n\n\nlibrary(bayesplot)\nd_new <- tibble(alcohol = c(-2, 4))\npred <- m1 |>\n  posterior_predict(newdata = d_new) |>\n  data.frame()\ncolnames(pred) <- c(\"low_alc\", \"high_alc\") \npred <- tidyr::pivot_longer(pred, everything(), \n                            names_to = \"alc\",\n                            values_to = \"value\")\np <- ggplot(aes(x = value), \n            data = pred)\np + geom_density(aes(fill = alc, color = alc), \n                 alpha = 1/4) +\n  geom_histogram(aes(x = quality, \n                     y = after_stat(density)), \n                 alpha = 1/2,\n                 data = ds) +\n  xlab(\"Quality Score (-2.5, +2.5)\") + ylab(\"\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-6",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-6",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nNaive way to show the posterior predictive check\n\n\n\n\nyrep1 <- posterior_predict(m1) # predict at every observation\nppc_dens_overlay(ds$quality, yrep1[sample(nrow(yrep1), 50), ])"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-7",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-7",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can classify each prediction based on the distance to the nearest rating category\n\n\n\n\n\nmap_real_number <- function(x) {\n  if (x < -2) {\n    return(-2.5)\n  } else if (x >= -2 && x < -1) {\n    return(-1.5)\n  } else if (x >= -1 && x < 0) {\n    return(-0.5)\n  } else if (x >= 0 && x < 1) {\n    return(0.5)\n  } else if (x >= 1 && x < 2) {\n    return(1.5)\n  } else if (x >= 2) {\n    return(2.5)\n  }\n}\nmap_real_number <- Vectorize(map_real_number)\nyrep_cat <- map_real_number(yrep1) |>\n  matrix(nrow = nrow(yrep1), ncol = ncol(yrep1))\nppc_dens_overlay(ds$quality, \n            yrep_cat[sample(nrow(yrep1), 50), ])"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-8",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-8",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can take a look at the distribution of a few statistics to check where the model is particularly strong or weak\n\n\n\n\np1 <- ppc_stat(ds$quality, yrep1, stat = \"max\")\np2 <- ppc_stat(ds$quality, yrep1, stat = \"min\")\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-9",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-9",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can also look at predictions directly, and compare them to observed data\n\n\n\n\ns <- sample(nrow(yrep1), 50); \np1 <- ppc_ribbon(ds$quality[s], yrep1[, s])\np2 <- ppc_intervals(ds$quality[s], yrep1[, s])\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-10",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-10",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can now fit a larger model and compare the results\n\n\n\n\n\n\n\nm2 <- stan_glm(quality ~ ., \n               data = ds,\n               family = gaussian,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               iter = 700,\n               chains = 4)\nsummary(m2)\n\n\n\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      quality ~ .\n algorithm:    sampling\n sample:       1400 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 1359\n predictors:   12\n\nEstimates:\n                       mean   sd   10%   50%   90%\n(Intercept)           0.1    0.0  0.1   0.1   0.1 \nfixed_acidity         0.0    0.1  0.0   0.0   0.1 \nvolatile_acidity     -0.2    0.0 -0.2  -0.2  -0.2 \ncitric_acid           0.0    0.0 -0.1   0.0   0.0 \nresidual_sugar        0.0    0.0  0.0   0.0   0.0 \nchlorides            -0.1    0.0 -0.1  -0.1  -0.1 \nfree_sulfur_dioxide   0.0    0.0  0.0   0.0   0.1 \ntotal_sulfur_dioxide -0.1    0.0 -0.1  -0.1  -0.1 \ndensity               0.0    0.0 -0.1   0.0   0.0 \npH                   -0.1    0.0 -0.1  -0.1   0.0 \nsulphates             0.2    0.0  0.1   0.2   0.2 \nalcohol               0.3    0.0  0.3   0.3   0.4 \nsigma                 0.7    0.0  0.6   0.7   0.7 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.1    0.0  0.1   0.1   0.2  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                     mcse Rhat n_eff\n(Intercept)          0.0  1.0  1827 \nfixed_acidity        0.0  1.0   560 \nvolatile_acidity     0.0  1.0  1056 \ncitric_acid          0.0  1.0  1057 \nresidual_sugar       0.0  1.0   926 \nchlorides            0.0  1.0  1373 \nfree_sulfur_dioxide  0.0  1.0  1025 \ntotal_sulfur_dioxide 0.0  1.0  1044 \ndensity              0.0  1.0   551 \npH                   0.0  1.0   722 \nsulphates            0.0  1.0  1244 \nalcohol              0.0  1.0   678 \nsigma                0.0  1.0  1887 \nmean_PPD             0.0  1.0  1464 \nlog-posterior        0.1  1.0   636 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-11",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-11",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nWe can look at all the parameters in one plot, excluding sigma\n\n\n\n\nmcmc_areas(m2, pars = vars(!sigma))"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-12",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-12",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nDid we improve the model?\nWe will check accuracy using MSE for both models\nWe will also check the width of posterior intervals\nFinally, we will compare the models using PSIS-LOO CV (preferred)\n\n\n\n\nyrep2 <- posterior_predict(m2)\np3 <- ppc_intervals(ds$quality[s], yrep2[, s])\ngrid.arrange(p2, p3, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-13",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-13",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nComparing (Root) Mean Square Errors\n\n\n\n\n(colMeans(yrep1) - ds$quality)^2 |> mean() |> sqrt() |> round(2)\n\n[1] 0.72\n\n(colMeans(yrep2) - ds$quality)^2 |> mean() |> sqrt() |> round(2)\n\n[1] 0.66\n\n\n\n\n\nComparing posterior intervals\n\n\n\n\nwidth <- function(yrep, q1, q2) {\n  q <- apply(yrep, 2, function(x) quantile(x, probs = c(q1, q2)))\n  width <- apply(q, 2, diff)\n  return(mean(width))\n}\n\nwidth(yrep1, 0.25, 0.75) |> round(2)\n\n[1] 0.97\n\nwidth(yrep2, 0.25, 0.75) |> round(2)\n\n[1] 0.89"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-14",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-14",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nLet’s estimate PSIS-LOO CV, a measure of out-of-sample predictive performance\n\n\n\n\n\nlibrary(loo)\noptions(mc.cores = 4)\nloo1 <- loo(m1)\nloo2 <- loo(m2)\npar(mar = c(3,3,2,1), \n    mgp = c(2,.7,0), \n    tck = -.01, \n    bg = \"#f0f1eb\")\npar(mfrow = c(2, 1))\nplot(loo1)\nplot(loo2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-15",
    "href": "bayes-course/06-lecture/06-lecture.html#example-quality-of-wine-15",
    "title": "Bayesian Inference",
    "section": "Example: Quality of Wine",
    "text": "Example: Quality of Wine\n\n\nFinally, we can compare the models using loo_compare\n\n\n\n\nloo_compare(loo1, loo2)\n\n   elpd_diff se_diff\nm2    0.0       0.0 \nm1 -117.9      17.7"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#glms-and-models-for-count-data",
    "href": "bayes-course/06-lecture/06-lecture.html#glms-and-models-for-count-data",
    "title": "Bayesian Inference",
    "section": "GLMs and Models for Count Data",
    "text": "GLMs and Models for Count Data\n\n\nModeling count data is typically part of a general GLM framework\nThe general setup is that we have:\n\nResponse vector \\(y\\), and predictor matrix \\(X\\)\nLinear predictor: \\(\\eta = \\alpha + X\\beta\\)\n\\(\\E(y | X) = g^{-1}(\\eta)\\), where \\(g\\) is the link function that maps the linear predictor onto the observational scale\nFor linear regression, \\(g\\) is the identity function (i.e., no transformation)\nThe Poisson data model is \\(y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\(\\lambda_i = \\exp(X_i\\beta)\\), and so our link function \\(g(x) = \\log(x)\\)\nAs stated before, for one observation \\(y\\), \\(f(y|\\lambda) = \\frac{1}{y!} \\lambda^y e^{-\\lambda}\\)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#poisson-posterior",
    "href": "bayes-course/06-lecture/06-lecture.html#poisson-posterior",
    "title": "Bayesian Inference",
    "section": "Poisson Posterior",
    "text": "Poisson Posterior\n\n\nTo derive the posterior distribution for Poisson, we consider K regression inputs and independent priors on all \\(K+1\\): \\(\\alpha\\) and \\(\\beta_1, \\beta_2, ..., \\beta_k\\) \\[\n\\begin{eqnarray}\nf\\left(\\alpha,\\beta \\mid y,X\\right) & \\propto &\nf_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot\n\\prod_{i=1}^N {\\frac{g^{-1}(\\eta_i)^{y_i}}{y_i!} e^{-g^{-1}(\\eta_i)}} \\\\\n& \\propto & f_{\\alpha}\\left(\\alpha\\right) \\cdot \\prod_{k=1}^K f_{\\beta}\\left(\\beta_k\\right) \\cdot\n\\prod_{i=1}^N {\\frac{\\exp(\\alpha + x_i^\\top\\beta)^{y_i}}{y_i!} e^{-\\exp(\\alpha + x_i^\\top\\beta)}}\n\\end{eqnarray}\n\\]\nWhen the rate is observed at different time scales or unit scales, we introduce an exposure \\(u_i\\), which multiplies the rate \\(\\lambda_i\\)\nThe data model then becomes \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}\\left(u_i e^{X_i\\beta}\\right) \\\\\n& = & \\text{Poisson}\\left(e^{\\log(u_i)} e^{X_i\\beta}\\right) \\\\\n& = &\\text{Poisson}\\left(e^{X_i\\beta + \\log(u_i)}\\right)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#poisson-simulation",
    "href": "bayes-course/06-lecture/06-lecture.html#poisson-simulation",
    "title": "Bayesian Inference",
    "section": "Poisson Simulation",
    "text": "Poisson Simulation\n\n\nWe can set up a forward simulation to generate Poisson data\nIt’s a good practice to fit simulated data and see if you can recover the parameters from a known data-generating process\n\n\n\n\n\nset.seed(123)\nn <- 100\na <- 1.5\nb <- 0.5\nx <- runif(n, -5, 5)\neta <- a + x * b   # could be negative\nlambda <- exp(eta) # always positive\ny <- rpois(n, lambda)\nsim <- tibble(y, x, lambda)\np <- ggplot(aes(x, y), data = sim)\np + geom_point(size = 0.5) + \n  geom_line(aes(y = lambda), \n            col = 'red', \n            linewidth = 0.2) +\n  ggtitle(\"Simulated Poission Data\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#fitting-simulated-data",
    "href": "bayes-course/06-lecture/06-lecture.html#fitting-simulated-data",
    "title": "Bayesian Inference",
    "section": "Fitting Simulated Data",
    "text": "Fitting Simulated Data\n\n\nComplex and non-linear models may have a hard time recovering parameters from forward simulations\nThe process for fitting simulated data may give some insight into the data-generating process and priors\n\n\n\n\n\n# fitting from eta = 1.5 +  0.5 * x\nm3 <- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = sim,\n               chains = 4,\n               refresh = 0,\n               iter = 1000)\nsummary(m3)\n\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ x\n algorithm:    sampling\n sample:       2000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 1.5    0.1  1.4   1.5   1.6  \nx           0.5    0.0  0.5   0.5   0.5  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 10.7    0.5 10.1  10.7  11.3 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   451 \nx             0.0  1.0   504 \nmean_PPD      0.0  1.0  1434 \nlog-posterior 0.0  1.0   667 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#checking-poission-assumption",
    "href": "bayes-course/06-lecture/06-lecture.html#checking-poission-assumption",
    "title": "Bayesian Inference",
    "section": "Checking Poission Assumption",
    "text": "Checking Poission Assumption\n\n\nWe know that for Poisson model, \\(\\E(y_i) = \\V(y_i)\\), or equivalently \\(\\sqrt{\\E(y_i)} = \\text{sd}(y_i)\\)\nWe can check that the prediction errors follow this trend since we have a posterior predictive distribution at each \\(y_i\\)\n\n\n\n\n\nlibrary(latex2exp)\nyrep <- posterior_predict(m3)\nd <- tibble(y_mu_hat = sqrt(colMeans(yrep)),\n            y_var = apply(yrep, 2, sd))\np <- ggplot(aes(y_mu_hat, y_var), data = d)\np + geom_point(size = 0.5) +\n  geom_abline(slope = 1, intercept = 0,\n              linewidth = 0.2) +\n  xlab(TeX(r'($\\sqrt{\\widehat{E(y_i)}}$)')) +\n  ylab(TeX(r'($\\widehat{sd(y_i)}$)'))"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#posterior-predictive-checks",
    "href": "bayes-course/06-lecture/06-lecture.html#posterior-predictive-checks",
    "title": "Bayesian Inference",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\n\npred <- add_predicted_draws(sim, m3)\np1 <- pred |>\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, alpha = 0.2)\np2 <- pp_check(m3)\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#adding-exposure",
    "href": "bayes-course/06-lecture/06-lecture.html#adding-exposure",
    "title": "Bayesian Inference",
    "section": "Adding Exposure",
    "text": "Adding Exposure\n\n\nLet’s check the effect of adding an exposure variable to the DGP\n\n\n\n\n\nn <- 100\na <- 1.5\nb <- 0.5\nx <- runif(n, -5, 5)\nu <- rexp(n, 0.2)\neta  <- a + x * b + log(u) \n# or <- a + x * b\n\nlambda <- exp(eta)\ny <- rpois(n, lambda)\n# or rpois(n, u * lambda)\n\nsim_exposure <- tibble(y, x, lambda, \n                       exposure = u)\np <- ggplot(aes(x, y), data = sim_exposure)\np + geom_point(size = 0.5) + \nggtitle(\"Simulated Poission Data with Exposure\")"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#checking-predictions",
    "href": "bayes-course/06-lecture/06-lecture.html#checking-predictions",
    "title": "Bayesian Inference",
    "section": "Checking Predictions",
    "text": "Checking Predictions\n\nCodePlot\n\n\n\n\nSuppose we fit the model with and without the exposure term\n\n\n\n\n\n\nm4 <- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = sim_exposure, refresh = 0,\n               iter = 1200)\nm5 <- stan_glm(y ~ x,\n               prior_intercept = normal(0, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               offset = log(exposure),\n               refresh = 0,\n               data = sim_exposure, iter = 1200)\nyrep_m4 <- posterior_predict(m4)\nyrep_m5 <- posterior_predict(m5)\ns <- sample(nrow(yrep_m4), 100)\np1 <- ppc_dens_overlay(log(sim_exposure$y + 1), \n                       log(yrep_m4[s, ] + 1)) + \n  ggtitle(\"No Exposure (log scale)\")\np2 <- ppc_dens_overlay(log(sim_exposure$y + 1), \n                       log(yrep_m5[s, ] + 1)) + \n  ggtitle(\"With Exposure (log scale)\")\n\n\n\n\n\npred4 <- add_predicted_draws(sim_exposure, m4)\npred5 <- add_predicted_draws(sim_exposure, m5, \n          offset = log(sim_exposure$exposure))\np3 <- pred4 |>\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), \n                  alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, \n             alpha = 0.2)\np4 <- pred5 |>\n  ggplot(aes(x = x, y = .prediction)) +\n  stat_lineribbon(.width = c(0.90, 0.50), \n                  alpha = 0.25) +\n  geom_point(aes(x = x, y = y), size = 0.5, \n             alpha = 0.2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches!",
    "text": "Example: Trapping Roaches!\n\n\nThis example comes from Gelman and Hill (2007)\nThese data come from a pest management program aimed at reducing the number of roaches in the city apartments\nThe outcome \\(y\\), is the number of roaches caught\nThere is a pre-treatment number of roaches, roach1, a treatment indicator, and senior indicator for only elderly residents in a building\nThere is also exposure2, a number of days for which the roach traps were used\n\n\n\n\n# rescale to make sure coefficients are approximately \n# on the same scale\nroaches <- roaches |>\n  mutate(roach100 = roach1 / 100) |>\n  as_tibble()\nhead(roaches)\n\n# A tibble: 6 × 6\n      y roach1 treatment senior exposure2 roach100\n  <int>  <dbl>     <int>  <int>     <dbl>    <dbl>\n1   153 308            1      0      0.8    3.08  \n2   127 331.           1      0      0.6    3.31  \n3     7   1.67         1      0      1      0.0167\n4     7   3            1      0      1      0.03  \n5     0   2            1      0      1.14   0.02  \n6     0   0            1      0      1      0"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-1",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-1",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nOur model has three inputs and an intercepts term\nSince the traps were set for a different number of days, we will include an exposure offset \\(u_i\\)\n\\(b_t\\) is the treatment coefficient, \\(b_r\\) is the baseline roach level, and \\(b_s\\) is the senior coefficient\nWe need to consider reasonable priors on all those \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(?, \\, ?) \\\\\n\\beta & \\sim & \\text{Normal}(?, \\, ?)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-2",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-2",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nIf we look at the exposure, the average number of days that the traps were set was about 1\nHow many roaches do we expect to trap during a whole day? Hundreds would probably be on the high side, so our prior model should not be predicting say 10s of thousands\nWhat is the interpretation of the intercept in this regression?\nThere is no way (to my knowledge) to put a half-normal or exponential distribution on the intercept in rstanarm and if we put Normal(3, 1), it’s unlikely to be negative and the number of roaches can be as high as Exp(5) ~ 150 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(3, 1) \\\\\n\\beta & \\sim & \\text{Normal}(?, \\, ?)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-3",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-3",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow large can we expect the effects to be in this regression?\nLet’s just consider treatment\nSuppose we estimate the coefficient to be -0.05\nThat means it reduces roach infestation by 5% on average (exp(-0.05) = 0.95)\nWhat if it’s -2; that would mean an 86% reduction, an unlikely but possible outcome\nWith this in mind, we will set the betas to Normal(0, 1) \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Poisson}(u_i\\lambda_i)\\\\\n\\eta_i & = & \\alpha + \\beta_t x_{it} + \\beta_r x_{ir} + \\beta_s x_{is} \\\\\n\\lambda_i & = & \\exp(\\eta_i) \\\\\n\\alpha & \\sim & \\text{Normal}(3, 1) \\\\\n\\beta & \\sim & \\text{Normal}(0, \\, 1)\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-4",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-4",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe could do a quick sanity check using the prior predictive distribution\n\n\n\n\nm6 <- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               prior_PD = 1,\n               seed = 123)\nyrep_m6 <- posterior_predict(m6)\nsummary(colMeans(yrep_m6))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    9.88    42.69    47.71   296.27    57.70 52941.62 \n\n\n\n\n\nThe median is not unreasonable but we would not expect the max (of the average!) to be 52,000\nThe numbers or not in another universe, however, so we will go with it\nTry to do what people usually do, which is put the scale on the intercept at 10 or more and scale of the betas at 5 or more, and see what you get"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-5",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-5",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe will now fit the model and evaluate the inferences\n\n\n\n\nm7 <- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               family = poisson(link = \"log\"), \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               seed = 123)\nsummary(m7)\n\n\nModel Info:\n function:     stan_glm\n family:       poisson [log]\n formula:      y ~ roach100 + treatment + senior\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept)  3.1    0.0  3.1   3.1   3.1 \nroach100     0.7    0.0  0.7   0.7   0.7 \ntreatment   -0.5    0.0 -0.5  -0.5  -0.5 \nsenior      -0.4    0.0 -0.4  -0.4  -0.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 25.6    0.4 25.1  25.6  26.2 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0   899 \nroach100      0.0  1.0  1074 \ntreatment     0.0  1.0   950 \nsenior        0.0  1.0  1045 \nmean_PPD      0.0  1.0  1219 \nlog-posterior 0.1  1.0   574 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-6",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-6",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow good is this model?\nLet’s look a the basic posterior predictive check\n\n\n\n\n\nyrep_m7 <- posterior_predict(m7)\ns <- sample(nrow(yrep_m7), 100)\n\n# on the log scale,\n# so we can better see the data\np1 <- ppc_dens_overlay(log(roaches$y + 1), \n                 log(yrep_m7[s, ] + 1)) \n\nprop_zero <- function(y) mean(y == 0)\np2 <- pp_check(m7, plotfun = \"stat\", \n               stat = \"prop_zero\", \n               binwidth = .005)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#negative-binomial",
    "href": "bayes-course/06-lecture/06-lecture.html#negative-binomial",
    "title": "Bayesian Inference",
    "section": "Negative Binomial",
    "text": "Negative Binomial\n\n\nPPCs suggest there is overdispersion in the data\nWe can introduce a likelihood that doesn’t force the mean to be equal to the variance\nThe following is one of the parameterizations that is used in Stan \\[\n\\begin{eqnarray}\n\\text{NegBinomial2}(n \\, | \\, \\mu, \\phi)  &=& \\binom{n + \\phi - 1}{n} \\,\n\\left( \\frac{\\mu}{\\mu+\\phi} \\right)^{\\!n} \\, \\left(\n\\frac{\\phi}{\\mu+\\phi} \\right)^{\\!\\phi} \\\\\n\\E(n) &=& \\mu \\ \\\n\\text{ and } \\ \\V(n) = \\mu + \\frac{\\mu^2}{\\phi}\n\\end{eqnarray}\n\\]\nNotice that the variance term includes \\(\\mu^2 / \\phi > 0\\) allowing for more flexibility than in the case of Poisson"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-7",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-7",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nWe will now fit the model using a Negative Binomial instead of a Poisson\n\n\n\n\nm8 <- stan_glm(y ~ roach100 + treatment + senior, \n               offset = log(exposure2),\n               prior_intercept = normal(3, 1),\n               prior = normal(0, 1),\n               prior_aux = exponential(1),\n               family = neg_binomial_2, \n               data = roaches, \n               iter = 600,\n               refresh = 0,\n               seed = 123)\nsummary(m8)\n\n\nModel Info:\n function:     stan_glm\n family:       neg_binomial_2 [log]\n formula:      y ~ roach100 + treatment + senior\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 262\n predictors:   4\n\nEstimates:\n                        mean   sd   10%   50%   90%\n(Intercept)            2.8    0.2  2.6   2.8   3.1 \nroach100               1.2    0.2  1.0   1.2   1.6 \ntreatment             -0.7    0.2 -1.0  -0.7  -0.4 \nsenior                -0.3    0.3 -0.7  -0.3   0.0 \nreciprocal_dispersion  0.3    0.0  0.2   0.3   0.3 \n\nFit Diagnostics:\n           mean   sd    10%   50%   90%\nmean_PPD  68.8   82.0  24.3  42.5 138.8\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                      mcse Rhat n_eff\n(Intercept)           0.0  1.0  1742 \nroach100              0.0  1.0  1708 \ntreatment             0.0  1.0  1204 \nsenior                0.0  1.0  1573 \nreciprocal_dispersion 0.0  1.0  1604 \nmean_PPD              2.6  1.0  1017 \nlog-posterior         0.1  1.0   515 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-8",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-8",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nHow good is this model?\nLet’s look a the basic posterior predictive check\n\n\n\n\n\nyrep_m8 <- posterior_predict(m8)\ns <- sample(nrow(yrep_m8), 100)\n\n# on the log scale,\n# so we can better see the data\np1 <- ppc_dens_overlay(log(roaches$y + 1), \n                 log(yrep_m8[s, ] + 1)) \n\np2 <- pp_check(m8, plotfun = \"stat\", \n               stat = \"prop_zero\", \n               binwidth = .005)\ngrid.arrange(p1, p2, nrow = 2)"
  },
  {
    "objectID": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-9",
    "href": "bayes-course/06-lecture/06-lecture.html#example-trapping-roaches-9",
    "title": "Bayesian Inference",
    "section": "Example: Trapping Roaches",
    "text": "Example: Trapping Roaches\n\n\nLet’s check the comparison of the out-of-sample predictive performance relative to the Poisson model\n\n\n\n\nloo_pois <- loo(m7)\nloo_nb <- loo(m8)\nloo_compare(loo_pois, loo_nb)\n\n   elpd_diff se_diff\nm8     0.0       0.0\nm7 -5329.4     703.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "href": "bayes-course/04-lecture/04-lecture.html#mcmc-posterior-inference-and-prediction",
    "title": "Bayesian Inference",
    "section": "MCMC, Posterior inference, and Prediction",
    "text": "MCMC, Posterior inference, and Prediction\n\n\nMetropolis-Hastings-Rosenbluth algorithm\nR implementation and testing\nComputational issues\nWhy does MHR work\nPosterior predictive distribution\nPosterior predictive simulation in Stan\nOptimization and code breaking with MCMC\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#high-level-outline-of-mhr",
    "title": "Bayesian Inference",
    "section": "High-Level Outline of MHR",
    "text": "High-Level Outline of MHR\n\n\n\n\nThe idea is to spend “more time” in the area of high posterior volume\nPick a random starting point at \\(i=1\\) with \\(\\theta^{(1)}\\)\nPropose a next possible value of \\(\\theta\\), call it \\(\\theta'\\) from an (easy) proposal distribution\nEvaluate if you should accept or reject the proposal\nIf accepted, go to \\(\\theta^{'(2)}\\), otherwise stay at \\(\\theta^{(2)}\\)\nRinse and repeat\n\n\n\n\n\nIf we can get an independant draw from \\(\\theta\\), we just take it\nThat amounts to regular Monte Carlo sampling, like rnorm(1, mean = 0, sd = 1)\nOtherwise, we need a rule for evaluating when to accept and when to reject the proposal\nWe still need a way to evaluate the density at the proposed value (it will be part of the rule)\nThe big idea: if the proposed \\(\\theta'\\) has a higher plausibility than the current \\(\\theta\\), accept \\(\\theta'\\); if not, sometimes accept \\(\\theta'\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "href": "bayes-course/04-lecture/04-lecture.html#normal-normal-with-known-sigma",
    "title": "Bayesian Inference",
    "section": "Normal-Normal with Known \\(\\sigma\\)",
    "text": "Normal-Normal with Known \\(\\sigma\\)\n\n\nWe will start with a known posterior\nLet \\(y \\sim \\text{Normal}(\\mu, 0.5)\\)\nLet the prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nLet’s say we observe \\(y = 5\\)\nWe can immediately compute the posterior \\(\\mu | y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\n\nnormal_normal_post <- function(y, sd, prior_mu, prior_sd) {\n# for the case where sd is known\n  prior_prec <- 1/prior_sd^2 \n  data_prec <- 1/sd^2\n  n <- length(y)\n  post_mu <- (prior_prec * prior_mu + data_prec * n * mean(y)) /\n             (prior_prec + n * data_prec)\n  post_prec <- prior_prec + n * data_prec\n  post_sd <- sqrt(1/post_prec)\n  return(list(post_mu = post_mu, post_sd = post_sd))\n}\n\nnormal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2) |>\n  unlist() |> round(2)\n\npost_mu post_sd \n   4.71    0.49"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "href": "bayes-course/04-lecture/04-lecture.html#metropolis-hastings-rosenbluth-mhr",
    "title": "Bayesian Inference",
    "section": "Metropolis-Hastings-Rosenbluth (MHR)",
    "text": "Metropolis-Hastings-Rosenbluth (MHR)\n\nMHRNormal Symmetry\n\n\n\n\nWe are trying to generate draws: \\(\\left( \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)}\\right)\\) implied by the target density \\(f\\)\n\nPick the first value of \\(\\theta\\) randomly or deterministically\nDraw \\(\\theta'\\) from a proposal distribution \\(q(\\theta'|\\theta)\\)\nCompute the unnormalized \\(f(\\theta'|y) \\propto f(y|\\theta') f(\\theta') = f_{\\text{prop}}\\)\nCompute the unnormalized \\(f(\\theta|y) \\propto f(y|\\theta) f(\\theta) = f_{\\text{current}}\\)\nCompute \\(\\text{ratio}_f = \\frac{f_{\\text{prop}}}{f_{\\text{current}}}\\) and \\(\\text{ratio}_q = \\frac{q(\\theta|\\theta')}{q(\\theta'|\\theta)}\\)\nAsseptance probability \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\cdot \\text{ratio}_q \\right\\rbrace\\)\nIf \\(q\\) is symmetric, we can drop \\(\\text{ratio}_q\\), in which case \\(\\alpha = \\min\\left\\lbrace 1, \\text{ratio}_f \\right\\rbrace\\)\nIf \\(\\text{ratio} \\geq 1\\), accept \\(\\theta'\\), else flip a coin with \\(\\text{Pr}(X=1) = \\alpha\\) and accept \\(\\theta'\\) if \\(X=1\\), stay with current \\(\\theta\\), if \\(X=0\\)\n\n\n\n\n\n\nWhy in case of normals, \\(q(\\theta'|\\theta) = q(\\theta|\\theta')\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndnorm(1, mean = 0)\n\n[1] 0.2419707\n\ndnorm(0, mean = 1)\n\n[1] 0.2419707"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-in-r",
    "title": "Bayesian Inference",
    "section": "MHR in R",
    "text": "MHR in R\n\nMHR IterationMHR for N Iterations\n\n\n\nmetropolis_iteration_1 <- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # this is just another way of flipping a coint\n    next_value <- proposal_mean  \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}\n\n\n\n\nmhr <- function(y, f, N, start, ...) {\n# y: new observation\n# f: function that implements one MHR iteration\n# N: number of iterations\n# start: initial value of the chain\n# ...: additional arguments to f\n  \n  draws <- numeric(N)\n  draws[1] <- f(y, current_mean = start, ...)\n  \n  for (i in 2:N) {\n    draws[i] <- f(y, current_mean = draws[i - 1], ...)\n  }\n  \n  return(draws)\n}\n\ny <- 5; N <- 5e3; start <- 3\nd <- mhr(y, N, f = metropolis_iteration_1, start, proposal_scale = 2, sd = 0.5,  \n         prior_mean = 0, prior_sd = 2)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "href": "bayes-course/04-lecture/04-lecture.html#markov-chain-animation",
    "title": "Bayesian Inference",
    "section": "Markov Chain Animation",
    "text": "Markov Chain Animation\n   Autocorrelation function (ACF)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#comparing-samples-to-the-true-posterior",
    "href": "bayes-course/04-lecture/04-lecture.html#comparing-samples-to-the-true-posterior",
    "title": "Bayesian Inference",
    "section": "Comparing Samples to the True Posterior",
    "text": "Comparing Samples to the True Posterior\n\n\n\n\nnp <- normal_normal_post(y = y, \n                         sd = 0.5, \n                         prior_mu = 0, \n                         prior_sd = 2)\n\ntheta <- seq(np$post_mu + 3*np$post_sd, \n             np$post_mu - 3*np$post_sd, \n             len = 100)\n\ndn <- dnorm(theta, mean = np$post_mu, \n                     sd = np$post_sd)\n\np <- ggplot(aes(d), data = tibble(d = d))\np + geom_histogram(aes(y = after_stat(density)), \n                   bins = 25, alpha = 0.6) +\n  geom_line(aes(theta, dn), linewidth = 0.5, \n            color = 'red', \n            data = tibble(theta, dn)) + \n  ylab(\"\") + xlab(expression(theta)) + \n  ggtitle(\"MHR draws vs Normal(4.71, 0.49)\")"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "href": "bayes-course/04-lecture/04-lecture.html#what-is-wrong-with-this-code",
    "title": "Bayesian Inference",
    "section": "What is Wrong with this Code",
    "text": "What is Wrong with this Code\n\n\n\nmetropolis_iteration_1 <- function(y, current_mean, proposal_scale, \n                                   sd, prior_mean, prior_sd) {\n# assume prior ~ N(prior_mean, prior_sd) and sd is known\n# proposal sampling distribution q = N(current_mean, proposal_scale)\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale) # q(mu' | mu)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, # proposal prior, f(theta')\n                         sd = prior_sd) *                  \n                   dnorm(y, mean = proposal_mean, sd = sd) # proposal lik, f(y | theta')\n  f_current     <- dnorm(current_mean, mean = prior_mean,  # current prior, f(theta)\n                         sd = prior_sd) *        \n                   dnorm(y, mean = current_mean, sd = sd)  # current lik, f(y | theta)\n\n  ratio <- f_proposal / f_current # [f(theta') * f(y | theta')] / [f(theta) * f(y | theta)]\n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # this is just another way of flipping a coint\n    next_value <- proposal_mean  \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "href": "bayes-course/04-lecture/04-lecture.html#what-if-y-is-a-vector",
    "title": "Bayesian Inference",
    "section": "What if Y is a vector?",
    "text": "What if Y is a vector?\n\n\nRecall the likelihood of many independent observations, is the product of their individual likelihoods \\[\n\\begin{eqnarray}\nf(y \\mid \\mu) & = & \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2 \\pi} \\\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\nf(\\mu \\mid y) & \\propto & \\text{prior} \\cdot \\prod_{i=1}^{n}\\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y_i - \\mu}{\\sigma} \\right)^2     \\right) \\\\\n\\end{eqnarray}\n\\]\nThis suggests the following change:\n\n\n\n\nmetropolis_iteration_2 <- function(y, current_mean, proposal_scale, sd,\n                                 prior_mean, prior_sd) {\n\n  proposal_mean <- rnorm(1, current_mean,  proposal_scale)        \n  f_proposal    <- dnorm(proposal_mean, mean = prior_mean, sd = prior_sd) * \n                   prod(dnorm(y, mean = proposal_mean, sd = sd))   \n  f_current     <- dnorm(current_mean, mean = prior_mean, sd = prior_sd) *        \n                   prod(dnorm(y, mean = current_mean, sd = sd))\n\n  if ((f_proposal || f_current) == 0) {\n    return(\"Error: underflow\") # on my computer double.xmin = 2.225074e-308\n  }\n  ratio <- f_proposal / f_current \n  alpha <- min(ratio, 1)     \n  \n  if (alpha > runif(1)) {         # definitely go if f_mu_prime > f_mu: alpha = 1\n    next_value <- proposal_mean   # if alpha < 1, go if alpha > U(0, 1)\n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "href": "bayes-course/04-lecture/04-lecture.html#handling-underflow",
    "title": "Bayesian Inference",
    "section": "Handling Underflow",
    "text": "Handling Underflow\n\n\nset.seed(123)\ny <- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\nreplicate(20, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n [1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n [5] \"Error: underflow\" \"1.66235834591796\" \"Error: underflow\" \"2.55673000751367\"\n [9] \"Error: underflow\" \"Error: underflow\" \"2.51354952759192\" \"Error: underflow\"\n[13] \"Error: underflow\" \"Error: underflow\" \"3.02905649937656\" \"Error: underflow\"\n[17] \"Error: underflow\" \"2.82278258359193\" \"2.67485301455861\" \"Error: underflow\""
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#always-compute-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "Always Compute on the Log Scale",
    "text": "Always Compute on the Log Scale\n\n\nFor one observation \\(y\\): \\[\n\\begin{eqnarray}\n\\log f(y \\mid \\mu) & \\propto & \\log \\left( \\frac{1}{\n\\sigma} \\exp\\left( - \\, \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2 \\right) \\right) \\\\\n& = & -\\log(\\sigma) - \\frac{1}{2} \\left(  \\frac{y - \\mu}{\\sigma} \\right)^2\n\\end{eqnarray}\n\\]\n\n\n\ndnorm_log <- function(y, mean = 0, sd = 1) {\n# dropping constant terms; not needed for MCMC\n  -log(sd) - 0.5 * ((y - mean) / sd)^2\n}\n\n\n\nFor multiple observations, you can just sum the log-likelihood"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "href": "bayes-course/04-lecture/04-lecture.html#mhr-on-the-log-scale",
    "title": "Bayesian Inference",
    "section": "MHR on the Log Scale",
    "text": "MHR on the Log Scale\n\nmetropolis_iteration_log <- function(y, current_mean, proposal_scale, \n                                     sd, prior_mean, prior_sd) {\n  \n  # draw a proposal q(proposal_mean | current_mean)\n  proposal_mean  <- rnorm(1, current_mean,  proposal_scale)           \n  \n  # construct a proposal: f(mu') * \\prod f(y_i | mu') on the log scale\n  proposal_lik   <- sum(dnorm_log(y, mean = proposal_mean, sd = sd))\n  proposal_prior <- dnorm_log(proposal_mean, mean = prior_mean, sd = prior_sd)\n  f_proposal     <- proposal_prior + proposal_lik\n  \n  # construct a current: f(mu) * \\prod f(y_i | mu) on the log scale\n  current_lik    <- sum(dnorm_log(y, mean = current_mean, sd = sd))\n  current_prior  <- dnorm_log(current_mean, mean = prior_mean, sd = prior_sd)\n  f_current      <- current_prior + current_lik\n  \n  # ratio on the log scale = difference of the logs\n  log_ratio <- f_proposal - f_current\n  log_alpha <- min(log_ratio, 0)     \n  \n  if (log_alpha > log(runif(1))) {  \n    next_value <- proposal_mean     \n  } else {\n    next_value <- current_mean\n  }\n  return(next_value)\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "href": "bayes-course/04-lecture/04-lecture.html#checking-the-work",
    "title": "Bayesian Inference",
    "section": "Checking the Work",
    "text": "Checking the Work\n\n\nset.seed(123)\ny <- rnorm(300, mean = 2, sd = 0.55)\nnormal_normal_post(y = y, sd = 0.55, prior_mu = 0, prior_sd = 1) |> unlist() |>\n  round(2)\n\npost_mu post_sd \n   2.02    0.03 \n\n\n\n\n\nreplicate(4, metropolis_iteration_2(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] \"Error: underflow\" \"Error: underflow\" \"Error: underflow\" \"Error: underflow\"\n\n\n\n\n\nreplicate(4, metropolis_iteration_log(y = y, \n                                     current_mean = 1, \n                                     proposal_scale = 2, \n                                     sd = 0.55, \n                                     prior_mean = 0, \n                                     prior_sd = 1))\n\n[1] 1.000000 1.000000 1.423961 2.379762\n\n\n\n\n\nd <- mhr(y, N, f = metropolis_iteration_log, start, proposal_scale = 2, sd = 0.55,  prior_mean = 0, prior_sd = 1)\nmean(d) |> round(2)\n\n[1] 2.02\n\nsd(d) |> round(2)\n\n[1] 0.03"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nTo show why the algorithm works, we need to show:\n\nThe chain has stationary distribution and it is unique\nThe stationary distribution is our target distribution \\(f\\theta | y)\\)\n\nCondition (a) requires some theory of Markov Chains, but the conditions are mild and are generally satisfied in practice. (See Chapters 11 and 12 in Blitzstein and Hwang for more)\n\nWe will show an outline of the proof for (b)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-1",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nWe will only consider the case of symmetric q\nSuppose you sample two points from the PMF \\(f(\\theta | y)\\), \\(\\theta_a\\) and \\(\\theta_b\\) and assume we are at time \\(t-1\\)\nLet the probability of going from \\(\\theta_a\\) to \\(\\theta_b\\) be: \\(\\P(\\theta^t = \\theta_b) \\mid \\theta^{t-1} = \\theta_a) := p_{a b}\\) and the reverse jump: \\(\\P(\\theta^t = \\theta_a) \\mid \\theta^{t-1} = \\theta_b) := p_{b a}\\)\nWe want to show: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} ,\\, \\text{where} \\\\\np_{a  b} &=& q(\\theta_b \\mid \\theta_a) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace \\\\\np_{b  a} &=& q(\\theta_a \\mid \\theta_b) \\cdot \\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "href": "bayes-course/04-lecture/04-lecture.html#why-does-the-algorithm-work-2",
    "title": "Bayesian Inference",
    "section": "Why Does the Algorithm Work",
    "text": "Why Does the Algorithm Work\n\n\nFor symmetric q: \\(q(\\theta_b | \\theta_a) = q(\\theta_a | \\theta_b)\\) and: \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{\\min \\left \\lbrace 1, \\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)} \\right \\rbrace}{\\min \\left \\lbrace 1, \\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)} \\right \\rbrace}\n\\end{eqnarray}\n\\]\nConsider the case when \\(f(\\theta_b \\mid y) > f(\\theta_a \\mid y)\\) \\[\n\\begin{eqnarray}\n\\frac{p_{a  b}}{p_{b  a}} &=& \\frac{1}{\\frac{f(\\theta_a \\mid y)}{f(\\theta_b \\mid y)}} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\end{eqnarray}\n\\]\nWhen \\(f(\\theta_a \\mid y) > f(\\theta_b \\mid y)\\) \\[\n\\frac{p_{a  b}}{p_{b  a}} = \\frac{\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}}{1} =\n\\frac{f(\\theta_b \\mid y)}{f(\\theta_a \\mid y)}\n\\]"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nRecall the prior predictive distribution, before observing \\(y\\), that appears in the denominator of Bayes’s rule: \\[\nf(y) = \\int f(y, \\theta) \\, d\\theta = \\int f(\\theta) f(y \\mid \\theta) \\, d\\theta\n\\]\nA posterior predictive distribution, \\(f(\\tilde{y} | y)\\) is obtained in a similar manner \\[\n\\begin{eqnarray}\nf(\\tilde{y} \\mid y) &=& \\int f(\\tilde{y}, \\theta \\mid y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta, y) \\, d\\theta \\\\\n&=& \\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta \\\\\n\\end{eqnarray}\n\\]\n\\(f(\\tilde{y} \\mid \\theta, y) = f(\\tilde{y} \\mid \\theta)\\) since \\(y \\perp\\!\\!\\!\\perp \\tilde{y} \\mid \\theta\\) (conditional indepedence)\nTwo sources of variability are accounted for: sampling variability in \\(\\tilde{y}\\) weighted by posterior variability in \\(\\theta\\)\nGiven draws from \\(f(\\theta \\mid y)\\), \\(\\theta^{(m)} \\sim f(\\theta \\mid y)\\), we can compute the integral in a usual way: \\(f(\\tilde{y} \\mid y) \\approx \\frac{1}{M} \\sum_{m = 1}^M f(\\tilde{y} \\mid \\theta^{(m)})\\)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "href": "bayes-course/04-lecture/04-lecture.html#introducing-posterior-predictive-distribution-1",
    "title": "Bayesian Inference",
    "section": "Introducing Posterior Predictive Distribution",
    "text": "Introducing Posterior Predictive Distribution\n\n\nFor simple models we can often evalute \\(\\int f(\\theta \\mid y) f(\\tilde{y} \\mid \\theta) \\, d\\theta\\) directly\nWe will use an inderect approach using our example Normal-Normal model with known variance\nSince we already know \\(f(\\theta | y)\\), we will sample \\(\\theta\\) from it, and then sample \\(\\tilde{y}\\), from \\(f(\\tilde{y} \\mid \\theta)\\)\nRecall, \\(y \\sim \\text{Normal}(\\mu, 0.5)\\) with prior \\(\\mu \\sim \\text{Normal}(0, 2)\\)\nFor \\(y = 5\\), the posterior \\(\\mu | y \\sim \\text{Normal}(4.71, 0.49)\\)\n\n\n\n\nppd <- function(post_mu, post_sd) {\n  mu <- rnorm(1, mean = post_mu, sd = post_sd)\n  y  <- rnorm(1, mean = mu, sd = 0.5)\n}\npd <- normal_normal_post(y = 5, sd = 0.5, prior_mu = 0, prior_sd = 2)\ny <- replicate(1e5, ppd(pd$post_mu, pd$post_sd))\ncat(\"y_tilde | y ~ Normal(\", round(mean(y), 2), \",\", round(sd(y), 2), \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71,0.7)\n\n\n\n\n\nIt can be shown that the posterior predictive distribution will have the same mean as the posterior distribution and the variance as the sum of the posterior variance and data variance:\n\n\n\n\nppd_mu <- pd$post_mu |> round(2)\nppd_sigma <- sqrt(pd$post_sd^2 + 0.5^2) |> round(2)\ncat(\"y_tilde | y ~ Normal(\", ppd_mu, \", \", ppd_sigma, \")\", sep = \"\")\n\ny_tilde | y ~ Normal(4.71, 0.7)"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\n\nYou can compute posterior predictive distribution in R, Stan, and rstanarm\nHere, we will see how to do it in Stan\nWe will show an example in rstanarm in the next lecture\n\n\n\n\ndata {\n  real y;\n  real<lower=0> sigma;\n}\nparameters {\n  real mu;\n}\nmodel {\n  mu ~ normal(0, 2);\n  y ~ normal(mu, sigma);\n}\ngenerated quantities {\n  real y_tilde = normal_rng(mu, sigma);\n}"
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "href": "bayes-course/04-lecture/04-lecture.html#posterior-predictions-in-stan-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions in Stan",
    "text": "Posterior Predictions in Stan\n\n\nlibrary(cmdstanr)\n\nm1 <- cmdstan_model(\"stan/normal_pred.stan\") # compile the model\ndata <- list(y = 5, sigma = 0.5)\nf1 <- m1$sample(       # for other options to sample, help(sample)\n  data = data,         # pass data as a list, match the vars name to Stan\n  seed = 123,          # to reproduce results, Stan does not rely on R's seed\n  chains = 4,          # total chains, the more, the better\n  parallel_chains = 4, # for multi-processor CPUs\n  refresh = 0,         # number of iterations printed on the screen\n  iter_warmup = 500,   # number of draws for warmup (per chain)\n  iter_sampling = 500  # number of draws for samples (per chain)\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nf1$summary()\n\n# A tibble: 3 × 10\n  variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  <chr>    <num>  <num> <num> <num> <num> <num> <num>    <num>    <num>\n1 lp__     -3.46  -3.20 0.728 0.348 -4.81 -2.94  1.00     889.    1128.\n2 mu        4.74   4.72 0.494 0.524  3.97  5.56  1.00     720.     958.\n3 y_tilde   4.72   4.74 0.712 0.715  3.51  5.88  1.00    1091.    1441."
  },
  {
    "objectID": "bayes-course/04-lecture/04-lecture.html#extra-credit-homework",
    "href": "bayes-course/04-lecture/04-lecture.html#extra-credit-homework",
    "title": "Bayesian Inference",
    "section": "Extra Credit Homework",
    "text": "Extra Credit Homework\n\nDianconisCypher textAlgorithmDecoded text\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#homework-feedback",
    "href": "bayes-course/05-lecture/05-lecture.html#homework-feedback",
    "title": "Bayesian Inference",
    "section": "Homework Feedback",
    "text": "Homework Feedback\n\nHow was Homework 4?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#section",
    "href": "bayes-course/05-lecture/05-lecture.html#section",
    "title": "Bayesian Inference",
    "section": "",
    "text": "Introducing linear regression\nPrior predictive simulations\nSampling from the posterior\nExample of linear regression in Stan\nEvaluating the quality of the draws\nPosterior predictions\nCross validation, ELPD, and LOO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\P}{\\mathbb{P}}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\L}{\\mathcal{L}}\n\\DeclareMathOperator{\\I}{\\text{I}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\DeclareMathOperator*{\\argmin}{arg\\,min}\n\\]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "href": "bayes-course/05-lecture/05-lecture.html#motivating-example",
    "title": "Bayesian Inference",
    "section": "Motivating Example",
    "text": "Motivating Example\n\n\n\n\nWe borrow this example from Richard McElreath’s Statistical Rethinking\nThe data sets provided have been produced between 1969 to 2008, based on Nancy Howell’s observations of the !Kung San\nFrom Wikipedia: “The ǃKung are one of the San peoples who live mostly on the western edge of the Kalahari desert, Ovamboland (northern Namibia and southern Angola), and Botswana.”\n\n\n\n\n\n\n\n\n\n\n\nUnivercity of Toronto Data Sets"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\n\n\nData sample and summary:\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n151.765\n47.82561\n63\n1\n\n\n139.700\n36.48581\n63\n0\n\n\n136.525\n31.86484\n65\n0\n\n\n156.845\n53.04191\n41\n1\n\n\n145.415\n41.27687\n51\n0\n\n\n163.830\n62.99259\n35\n1\n\n\n\n\n\n     height           weight            age       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.00  \n 1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00  \n Median :148.59   Median :40.058   Median :27.00  \n Mean   :138.26   Mean   :35.611   Mean   :29.34  \n 3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00  \n Max.   :179.07   Max.   :62.993   Max.   :88.00  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice a non-linearity\nThinking about why should this be, can give you an insight into how to model these data"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-dataset-1",
    "title": "Bayesian Inference",
    "section": "Howell Dataset",
    "text": "Howell Dataset\n\n\nFor now, we will focus on the linear subset of the data\nWe will demonstrate the non-linear model at the end\nWe will restrict our attention to adults (age > 18)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#general-aproach",
    "href": "bayes-course/05-lecture/05-lecture.html#general-aproach",
    "title": "Bayesian Inference",
    "section": "General Aproach",
    "text": "General Aproach\n\n\nAssess the scope of the inferences that you will get with this model\nIn the case of linear regression, there is likely no causal mechanism and the coefficients should be interpreted as comparisons (RAOS, Page 84)\nSet up reasonable priors and likelihood\n(*) For more complex models: perform a forward simulation with fixed parameter values and try to recover them by running an inference algorithm. See this example for a standard epidemiological model.\nPerform a prior predictive simulation\nAdjust your priors\nFit the model to data\nAssess the quality of the inference and the quality of the model\nImprove or fix your model and go back to #3"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe will build a predictive model for adult Weight \\(y\\) given Height \\(x\\) using the Howell dataset\nInitial stab at the model: \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x_i \\\\\n\\alpha & \\sim & \\text{Normal}(\\alpha_{l}, \\, \\alpha_s) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWe have to specify \\(\\alpha_{l}\\) and \\(\\alpha_s\\), where l and s signify location and scale, and r, the rate of the exponential\nIf we work on the original scale for \\(x\\), it is awkward to choose a prior for the intercept : it corresponds to the weight of the person with zero height\nThis can be fixed by subtracting the average height from \\(x\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-1",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe define a new variable, the centered version of \\(x\\): \\(x^c_i = x_i - \\bar{x}\\)\nNow \\(\\alpha\\) corresponds to the weight of an average person\nChecking Wikipedia reveals that the average weight of a person in Africa is about 60 kg\nThey don’t state the standard deviation but it is unlikely that an African adult would weigh less than 30 kg and more than 120 kg and so we will set the prior sd = 10 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(\\beta_{l}, \\, \\beta_s) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the slope \\(\\beta\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-2",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nIn this dataset, the units of \\(\\beta\\) are \\(\\frac{kg}{cm}\\), since the units of height are \\(cm\\)\nFirst thing, \\(\\beta\\) should be positive. Why?\nSecond, \\(beta\\) is likely less than 1. Why?\nWe can consult height-weight tables for the expected value and variance\nIn the dataset, \\(\\E(\\beta) = 0.55\\) with a standard error of 0.006, but since we are uncertain how applicable that is to !Kung, we will allow the prior to vary more \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(r) \\\\\n\\end{eqnarray}\n\\]\nWhat about the error term \\(\\sigma\\)?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "href": "bayes-course/05-lecture/05-lecture.html#howell-regression-3",
    "title": "Bayesian Inference",
    "section": "Howell Regression",
    "text": "Howell Regression\n\n\nWe know that \\(\\sigma\\) must be positive and so a possible choice for a the prior is \\(\\text{Normal}^+\\), Exponential, etc.\nAt this stage, the key is to rule out implausible values, not to get something precise, particularly since we have enough data (> 340 observations)\nFrom the background data, the residual standard error was 4.6, which implies the exponential rate parameter is 0.2 \\[\n\\begin{eqnarray}\ny_i & \\sim & \\text{Normal}(\\mu_i, \\, \\sigma)\\\\\n\\mu_i & = & \\alpha + \\beta x^c_i \\\\\n\\alpha & \\sim & \\text{Normal}(60, \\, 10) \\\\\n\\beta & \\sim & \\text{Normal}(0.55, \\, 0.1) \\\\\n\\sigma & \\sim & \\text{Exp}(0.2) \\\\\n\\end{eqnarray}\n\\]\nWe are now ready to perform a prior predictive simulation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nThe simulation follows the generative process defined by the model\n\n\n\n\n\n\nd <- d |>\n  mutate(height_c = height - mean(height))\nround(mean(d$height_c), 2)\n\n[1] 0\n\nhead(d)\n\n# A tibble: 6 × 5\n  height weight   age  male height_c\n   <dbl>  <dbl> <dbl> <dbl>    <dbl>\n1   152.   47.8    63     1    -2.83\n2   140.   36.5    63     0   -14.9 \n3   137.   31.9    65     0   -18.1 \n4   157.   53.0    41     1     2.25\n5   145.   41.3    51     0    -9.18\n6   164.   63.0    35     1     9.23\n\nprior_pred <- function(data) {\n  alpha <- rnorm(1, 60, 10)\n  beta <- rnorm(1, 0.55, 0.1)\n  sigma <- rexp(1, 0.2)\n  l <- nrow(data); y <- numeric(l)\n  for (i in 1:l) {\n    mu <- alpha + beta * data$height_c[i]\n    y[i] <- rnorm(1, mu, sigma)\n  }\n  return(y)\n}\n\n\n\n\n\nn <- 100\npr_p <- replicate(n = n, prior_pred(d))\n# using library(purrr) functional primitives:\n# pr_p <- map(1:n, \\(i) prior_pred(d))\ndim(pr_p)\n\n[1] 352 100\n\nround(pr_p[1:12, 1:8], 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]\n [1,] 50.02 49.35 67.40 50.84 40.34 47.49 67.09 39.72\n [2,] 46.98 43.39 62.40 53.24 34.19 47.66 62.79 32.45\n [3,] 45.60 41.89 62.17 53.54 34.27 47.21 61.63 30.93\n [4,] 50.96 51.11 69.22 57.28 41.94 58.24 68.66 42.54\n [5,] 48.87 45.78 65.16 52.88 36.66 51.49 64.78 36.91\n [6,] 52.35 54.13 71.59 61.06 44.36 56.78 71.32 47.25\n [7,] 48.64 47.73 66.35 56.74 38.62 44.15 66.08 37.69\n [8,] 55.56 56.36 74.16 55.90 45.27 53.42 73.19 50.21\n [9,] 50.09 47.26 65.32 51.37 39.50 52.89 65.69 39.02\n[10,] 54.01 55.10 72.17 60.16 43.42 50.05 71.78 47.44\n[11,] 52.27 49.91 67.93 61.93 41.40 48.83 67.93 41.93\n[12,] 52.74 48.63 67.90 57.07 40.23 44.49 66.86 39.32"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-predictive-simulation-1",
    "title": "Bayesian Inference",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nTo get a sense for the possible regression lines implied by the prior, we fit a linear model to each simulation draw and plot the lines over observations\n\n\n\n\n\nintercepts <- numeric(n)\nslopes <- numeric(n)\nfor (i in 1:n) {\n  coefs <- coef(lm(pr_p[, i] ~ d$height_c))\n  intercepts[i] <- coefs[1]\n  slopes[i] <- coefs[2]\n}\n\n# using library(purrr) functional primitives:\n# df <- pr_p |> map_dfr(\\(y) coef(lm(y ~ d$height_c)))\n\np <- ggplot(aes(height_c, weight), data = d)\np + geom_point(size = 0.5) + ylim(20, 90) + \n  geom_abline(slope = slopes, \n              intercept = intercepts, \n              alpha = 1/6) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"Kalahari !Kung San people\", \n          subtitle = \"Prior predictive simulation\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you notice about this prior?"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "href": "bayes-course/05-lecture/05-lecture.html#deriving-a-posterior-distribution",
    "title": "Bayesian Inference",
    "section": "Deriving a Posterior Distribution",
    "text": "Deriving a Posterior Distribution\n\n\nWe have seen how to derive the posterior and posterior predictive distribution\nThree dimentional posterior: \\(f(\\alpha, \\beta, \\sigma)\\). What happened to \\(\\mu\\)?\nWe construct the posterior from the prior and data likelihood (for each \\(y_i\\)): \\[\n\\begin{eqnarray}\n&\\text{Prior: }f(\\alpha, \\beta, \\sigma) = f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\\\\n&\\text{Likelihood: }f(y \\mid \\alpha, \\beta, \\sigma) = \\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\\\\n&\\text{Posterior: }f(\\alpha,\\beta,\\sigma \\mid y) = \\frac{f_1(\\alpha) f(_2\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right]}\n{\\int\\int\\int f_1(\\alpha) f_2(\\beta) f_3(\\sigma) \\cdot \\left[\\prod_{i=1}^{n}f_4(y_i \\mid \\alpha, \\beta, \\sigma) \\right] d\\alpha \\, d\\beta \\, d\\sigma}\n\\end{eqnarray}\n\\]\nTo be more precise, we would indicate that \\(f_1, f_2\\) and \\(f_4\\) are Normal with different parameters, and \\(f_3\\) is \\(\\text{Exp}(0.2)\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "href": "bayes-course/05-lecture/05-lecture.html#fitting-the-model",
    "title": "Bayesian Inference",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\n\nEven though our prior is slightly off, 300+ observations is a lot in this case (big data!), and so we proceed to model fitting\nWe will use stan_glm() function in rstanarm\nrstanarm has default priors, but you should specify your own:\n\n\n\n\n\n\nlibrary(rstanarm)\nlibrary(bayesplot)\noptions(mc.cores = parallel::detectCores())\n\nm1 <- stan_glm(\n  weight ~ height_c,\n  data = d,\n  family = gaussian,\n  prior_intercept = normal(60, 10),\n  prior = normal(0.55, 0.1),\n  prior_aux = exponential(0.2),\n  chains = 4,\n  iter = 500,\n  seed = 1234\n)\n\n\n\nBy default, rstanarm samples from the posterior. To get back the prior predictive distribution (instead of doing it in R) use prior_PD = TRUE"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "href": "bayes-course/05-lecture/05-lecture.html#looking-at-the-model-summary",
    "title": "Bayesian Inference",
    "section": "Looking at the Model Summary",
    "text": "Looking at the Model Summary\n\n\nsummary(m1)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ height_c\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 352\n predictors:   2\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 45.0    0.2 44.7  45.0  45.3 \nheight_c     0.6    0.0  0.6   0.6   0.7 \nsigma        4.2    0.2  4.0   4.2   4.5 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 45.0    0.3 44.6  45.0  45.4 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  824  \nheight_c      0.0  1.0  994  \nsigma         0.0  1.0  867  \nmean_PPD      0.0  1.0  970  \nlog-posterior 0.1  1.0  459  \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\nIf you can examine the priors by running prior_summary(m1)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-quality-of-the-inferences",
    "title": "Bayesian Inference",
    "section": "Evaluting Quality of the Inferences",
    "text": "Evaluting Quality of the Inferences\n\n\nneff_ratio(m1) |> round(2)\n\n(Intercept)    height_c       sigma \n       0.82        0.99        0.87 \n\n\n\n\n\nrhat(m1) |> round(2)\n\n(Intercept)    height_c       sigma \n          1           1           1 \n\n\n\n\n\nmcmc_trace(m1, size = 0.3)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "href": "bayes-course/05-lecture/05-lecture.html#same-model-in-stan",
    "title": "Bayesian Inference",
    "section": "Same Model in Stan",
    "text": "Same Model in Stan\n\n\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n  int<lower=0, upper=1> prior_PD;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\ntransformed parameters {\n  vector[N] mu = alpha + beta * x;\n}\nmodel {\n  alpha ~ normal(60, 10);\n  beta ~ normal(0.55, 0.1);\n  sigma ~ exponential(0.2);\n  if (!prior_PD) {\n    y ~ normal(mu, sigma);\n  }\n}\ngenerated quantities {\n  array[N] real yrep = normal_rng(mu, sigma);\n}\n\n\n\n\nYou can pass prior_PD as a flag to enable drawing from the prior predictive distribution"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#prior-vs-posterior",
    "title": "Bayesian Inference",
    "section": "Prior vs Posterior",
    "text": "Prior vs Posterior\n\n\nComparing the prior to the posterior tells us how much the model learned from data\nIt also helps us to validate if our priors were reasonable\nIn rstanarm, you can use the posterior_vs_prior function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour prior should cover the plausible range of parameter values\nWhen we don’t have a lot of data and parameters are complex, setting good priors takes work, but there are guidelines"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\nlibrary(tidybayes)\n\ndraws <- spread_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(head(round(draws, 2)))\n\n\n\n\n.chain\n.iteration\n.draw\n(Intercept)\nheight_c\nsigma\n\n\n\n\n1\n1\n1\n44.97\n0.62\n4.09\n\n\n1\n2\n2\n45.06\n0.61\n4.21\n\n\n1\n3\n3\n45.02\n0.63\n4.13\n\n\n1\n4\n4\n45.07\n0.64\n4.32\n\n\n1\n5\n5\n44.66\n0.64\n4.38\n\n\n1\n6\n6\n45.31\n0.62\n4.12\n\n\n\n\n\n\n\n\nspread_draws will arrange the inferences in columns (wide format)\ngather_draws will arrange the inferences in rows (long format), which is usually more convenient for plotting and computation"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "href": "bayes-course/05-lecture/05-lecture.html#examining-the-posterior-1",
    "title": "Bayesian Inference",
    "section": "Examining the Posterior",
    "text": "Examining the Posterior\n\n\noptions(digits = 3)\ndraws <- gather_draws(m1, `(Intercept)`, height_c, sigma)\nknitr::kable(tail(draws, 4))\n\n\n\n\n.chain\n.iteration\n.draw\n.variable\n.value\n\n\n\n\n4\n247\n997\nsigma\n4.09\n\n\n4\n248\n998\nsigma\n4.22\n\n\n4\n249\n999\nsigma\n4.20\n\n\n4\n250\n1000\nsigma\n4.11\n\n\n\n\n\n\n\n\ndraws |> mean_qi(.width = 0.90) |> knitr::kable() # also see ?median_qi(), etc\n\n\n\n\n.variable\n.value\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\n(Intercept)\n45.000\n44.635\n45.36\n0.9\nmean\nqi\n\n\nheight_c\n0.624\n0.576\n0.67\n0.9\nmean\nqi\n\n\nsigma\n4.250\n3.996\n4.53\n0.9\nmean\nqi\n\n\n\n\n\n\n\n\nFrom the above table: \\(\\E(y|x^c) = 45 (\\text{kg}) + 0.62 (\\text{kg/cm})x^c (\\text{cm})\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "href": "bayes-course/05-lecture/05-lecture.html#variability-in-parameter-inferences",
    "title": "Bayesian Inference",
    "section": "Variability in Parameter Inferences",
    "text": "Variability in Parameter Inferences\n\n\nThe code in section 9.4 in the book doesn’t work as the function and variable names have changed\n\n\n\n\ndpred <- d |> \n  # same as add_epred_draws for lin reg not for other GLMs\n  add_linpred_draws(m1, ndraws = 50)\nhead(dpred, 3)\n\n# A tibble: 3 × 10\n# Groups:   height, weight, age, male, height_c, .row [1]\n  height weight   age  male height_c  .row .chain .iteration .draw .linpred\n   <dbl>  <dbl> <dbl> <dbl>    <dbl> <int>  <int>      <int> <int>    <dbl>\n1   152.   47.8    63     1    -2.83     1     NA         NA     1     43.4\n2   152.   47.8    63     1    -2.83     1     NA         NA     2     43.0\n3   152.   47.8    63     1    -2.83     1     NA         NA     3     42.9\n\n\n\n\n\n\np <- dpred |> ggplot(aes(x = height_c, y = weight)) +\n  geom_line(aes(y = .linpred, group = .draw), \n            alpha = 0.1) + \n  geom_point(data = d, size = 0.05) +\n  ylab(\"Weight (kg)\") + \n  xlab(\"Centered Height (cm)\") +\n  ggtitle(\"100 draws from the slope/intercept posterior\")\nprint(p)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nSuppose we are interested in predicting the weight of a person with a height of 160 cm\nThis corresponds to the centered height of 5.4: (\\(160 - \\bar{x}\\))\nWe can now compute the distribution of the mean weight of a 160 cm person (reflecting variability in the slope and intercept only):\n\n\\(\\mu = \\alpha + \\beta \\cdot 5.4\\), for each posterior draw\n\nAnd a predictive distribution:\n\n\\(y_{\\text{pred}} \\sim \\text{Normal}(\\mu, \\sigma)\\)\n\n\n\n\n\ndraws <- spread_draws(m1, `(Intercept)`, height_c, sigma)\ndraws <- draws |>\n  mutate(mu = `(Intercept)` + height_c * 5.4,\n         y_pred = rnorm(nrow(draws), mu, sigma))\ndraws[1:3, 4:8]\n\n# A tibble: 3 × 5\n  `(Intercept)` height_c sigma    mu y_pred\n          <dbl>    <dbl> <dbl> <dbl>  <dbl>\n1          45.0    0.619  4.09  48.3   50.6\n2          45.1    0.607  4.21  48.3   50.8\n3          45.0    0.628  4.13  48.4   40.5"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "href": "bayes-course/05-lecture/05-lecture.html#posterior-predictions-1",
    "title": "Bayesian Inference",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\n\nWe can compare predictive and average densities\nLeft panel showing the densities on their own\nRight panel showing the same densities in the context of raw observations\n\n\n\n\nmqi <- draws |> median_qi(.width = 0.90)\nselect(mqi, contains(c('mu', 'y_pred'))) |> round(2)\n\n# A tibble: 1 × 6\n     mu mu.lower mu.upper y_pred y_pred.lower y_pred.upper\n  <dbl>    <dbl>    <dbl>  <dbl>        <dbl>        <dbl>\n1  48.4     47.9     48.8   48.4         41.5         55.4"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "href": "bayes-course/05-lecture/05-lecture.html#rstanarm-prediction-functions",
    "title": "Bayesian Inference",
    "section": "RStanArm Prediction Functions",
    "text": "RStanArm Prediction Functions\n\n\nposterior_linpred returns \\(D \\times N\\) matrix with D draws and N data points\n\n\\(\\eta_n = \\alpha + \\sum_{p=1}^P \\beta_p x_{np}\\), where \\(P\\) is the total number of regression inputs\n\nposterior_epred returns an \\(D \\times N\\) matrix that applies the inverse link (in GLMs) to the linear predictor \\(\\eta\\)\n\n\\(\\mu_n = \\E(y | x_n)\\); this is the same as \\(\\eta\\) in Lin Regression\n\nposterior_predict returns an \\(D \\times N\\) matrix of predictions: \\(y \\mid \\mu_n\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "href": "bayes-course/05-lecture/05-lecture.html#predictions-in-rstanarm",
    "title": "Bayesian Inference",
    "section": "Predictions in RStanArm",
    "text": "Predictions in RStanArm\n\n\nPosterior linear predictor\n\n\n\neta <- posterior_linpred(m1, newdata = data.frame(height_c = 5.4))\nquantile(eta, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\nglue::glue('From the R simulation, 90% interval for eta = [{mqi$mu.lower |> round(2)}, {mqi$mu.upper |> round(2)}]')\n\nFrom the R simulation, 90% interval for eta = [47.91, 48.82]\n\n\n\n\nPosterior conditional mean\n\n\n\nmu <- posterior_epred(m1, newdata = data.frame(height_c = 5.4))\nquantile(mu, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n47.9 48.4 48.8 \n\n\n\n\nPosterior prediction\n\n\n\ny_pred <- posterior_predict(m1, newdata = data.frame(height_c = 5.4))\nquantile(y_pred, probs = c(0.05, 0.50, 0.95)) |> round(2)\n\n  5%  50%  95% \n41.0 48.1 55.3 \n\nglue::glue('From the R simulation, 90% interval for y_pred = [{mqi$y_pred.lower |> round(2)}, {mqi$y_pred.upper |> round(2)}]')\n\nFrom the R simulation, 90% interval for y_pred = [41.52, 55.36]"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluting-model-quality",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluting-model-quality",
    "title": "Bayesian Inference",
    "section": "Evaluting Model Quality",
    "text": "Evaluting Model Quality"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model",
    "title": "Bayesian Inference",
    "section": "Evaluating Quality of the Model",
    "text": "Evaluating Quality of the Model\n\n\nThere are at least two stages of model evaluation: 1) the quality of the draws and 2) the quality of predictions\nThere is also a question of model fairness\n\nHow were the data collected?\nPeople will likely interpret the results causally, even if not appropriate\nHow will the model be used?\n\nJust because the draws have good statistical properties (e.g., good mixing, low auto-correlation, etc.), it does not mean the model will perform well\nModel performance is assessed on how well it can make predictions for the types of questions that you are interested in"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model-1",
    "href": "bayes-course/05-lecture/05-lecture.html#evaluating-quality-of-the-model-1",
    "title": "Bayesian Inference",
    "section": "Evaluating Quality of the Model",
    "text": "Evaluating Quality of the Model\n\n\nOnce we are satisfied that the draws are statistically well-behaved, we can focus on evaluating predictive performance\nWe typically evaluate predictive performance in-sample and out-of-sample\nIn general, a good model is well-calibrated and makes sharp predictions (Gneiting et al. 2007)1\nFor in-sample assessment, we perform Posterior Predictive Checks or PPCs\nTo assess out-of-sample performance, we rely on cross-validation or its approximations\nIf you are making causal (counterfactual) predictions, naive cross-validation will not work. Why?\n\n\nGneiting, T., Balabdaoui, F., and Raftery, A. E. (2007) Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2), 243–268."
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#model-evaluation",
    "href": "bayes-course/05-lecture/05-lecture.html#model-evaluation",
    "title": "Bayesian Inference",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nHere is the high-level plan of attack:\n\n\nFit a linear model to the full !Kung dataset (not just adults) and let RStanArm pick the priors (don’t do this at home)\nWe know that this model is not quite right\nEvaluate the model fit\nFix the model by thinking about the relationship between height and weight\nEvaluate the improved model\nCompare the linear model to the improved model"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#model-evaluation-1",
    "href": "bayes-course/05-lecture/05-lecture.html#model-evaluation-1",
    "title": "Bayesian Inference",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\n\nThe following fits a linear model to the full dataset (not just adults)\n\n\n\n\n\n\n\nm2 <- stan_glm(\n  weight ~ height,\n  data = d,\n  family = gaussian,\n  chains = 4,\n  iter = 600,\n  seed = 1234\n)\nsummary(m2)\n\n\n\n\nThere were no sampling problems\nAnd posterior draws look well-behaved\nBut how good is this model?\n\n\n\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      weight ~ height\n algorithm:    sampling\n sample:       1200 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 544\n predictors:   2\n\nEstimates:\n              mean   sd    10%   50%   90%\n(Intercept) -33.8    1.1 -35.1 -33.7 -32.4\nheight        0.5    0.0   0.5   0.5   0.5\nsigma         5.0    0.1   4.8   5.0   5.2\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 35.6    0.3 35.2  35.6  36.0 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  1198 \nheight        0.0  1.0  1228 \nsigma         0.0  1.0   836 \nmean_PPD      0.0  1.0  1126 \nlog-posterior 0.1  1.0   497 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1)."
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nThe idea behind PPCs is to compare the distribution of the observation to posterior predictions\nWe already saw an example of how to do it by hand\nHere, we will do using functions in RStanArm\n\n\n\n\n\nlibrary(bayesplot)\n# bayesplot::pp_check(m2) <-- shortcut\n\n# predict for every observed point\nyrep <- posterior_predict(m2) \n\n# select 50 draws at random\ns <- sample(nrow(yrep), 50)\n\n# plot data against predictive densities\nppc_dens_overlay(d$weight, yrep[s, ])"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-1",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-1",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nWe can also compute the distributions of test statistics:\n\n\n\n\n\nlibrary(gridExtra)\np1 <- ppc_stat(d$weight, yrep, stat = \"mean\"); p2 <- ppc_stat(d$weight, yrep, stat = \"sd\")\nq25 <- function(y) quantile(y, 0.25); q75 <- function(y) quantile(y, 0.75)\np3 <- ppc_stat(d$weight, yrep, stat = \"q25\"); p4 <- ppc_stat(d$weight, yrep, stat = \"q75\"); \ngrid.arrange(p1, p2, p3, p4, ncol = 4)\n\n\n\n\n\n\n\n\n\n\n\nppc_stat is a shorthand for computation on each posterior (predictive) draw\nFor example, ppc_stat(d$weight, yrep, stat = \"mean\") is equivalent to:\n\nSetting \\(T(y) = \\text{mean(d\\$weight)}\\) and \\(T_{\\text{yrep}} = \\text{rowMeans(yrep)}\\)"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-2",
    "href": "bayes-course/05-lecture/05-lecture.html#visual-posterior-predictive-checks-2",
    "title": "Bayesian Inference",
    "section": "Visual Posterior Predictive Checks",
    "text": "Visual Posterior Predictive Checks\n\n\nSince we have a distribution at each observation point, we can plot the predictive distribution at each observation\nWe will first randomly select a subset of 50 people\nEach column of yrep is a prediction for each observation point\n\n\n\n\n\ns <- sample(ncol(yrep), 50)\n\nbayesplot::ppc_intervals(\n  d$weight[s],\n  yrep = yrep[, s],\n  x = d$height[s],\n  prob = 0.5,\n  prob_outer = 0.90\n) + xlab(\"Height (cm)\")  + \n  ylab(\"Weight (kg)\") +\n  ggtitle(\"Predicted vs observed weight\")"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\n\n\nWe looked at some visual evidence for in-sample model accuracy\nThe model is clearly doing a poor job of capturing observed data, particularly in the lower quantiles of the predictive distribution\nIn a situation like this, we would typically proceed to model improvements\nOften, the model is not as bad as this one, and we would like to get some quantitative measures of model fit1\nWe do this, so we can assess the relative performance of the next set of models\n\n\n\n\n\n\n\n\n\n\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-1",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-1",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nOne way to assess model accuracy is to compute an average square deviation from point prediction: mean square error\n\n\\(\\text{MSE} = \\frac{1}{N} \\sum_{n=1}^{N} (y_n - \\E(y_n|\\theta))^2\\)\nOr its scaled version which divides the summand by \\(\\V(y_i | \\theta)\\)\n\nThese measures do not work well for non-normal models\nOn the plus side, it is intuitive and computation is easy\nNote that if we just average the errors, we will get zero by definition\n\n\n\n\n# avarage error\nmean(d$weight - colMeans(yrep)) |> round(2)\n\n[1] -0.01\n\n# mean square error\nmean((d$weight - colMeans(yrep))^2) |> round(2)\n\n[1] 25\n\n# your book reports median absolute error\nmedian(abs(d$weight - colMeans(yrep)))\n\n[1] 3.55"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-2",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-2",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nIn a probabilistic prediction, we are able to assess model calibration\nCalibration says that, for example, 50% intervals contain approximately 50% of the observations, and so on\nA well-calibrated model may still be a bad model (see below) as the uncertainty may be too wide; for two models with the same calibration, we prefer the one with lower uncertainty\n\n\n\n\ninside <- function(y, obs) return(obs >= y[1] & obs <= y[2])\ncalib  <- function(y, data, interval) {\n  mid <- interval / 2\n  l <- 0.5 - mid; u <- 0.5 + mid\n  intervals <- apply(y, 2, quantile, probs = c(l, u))\n  is_inside <- numeric(ncol(intervals))\n  for (i in 1:ncol(intervals)) {\n    is_inside[i] <- inside(intervals[, i], data[i])\n  }\n return(mean(is_inside))\n}\ncalib(yrep, d$weight, 0.50)\n\n[1] 0.474\n\ncalib(yrep, d$weight, 0.90)\n\n[1] 0.901"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-3",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-3",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\n\nA popular “proper” scoring rule for probabilistic prediction is log score\nThis is equivalent to the log predictive density \\(\\log f(y | \\theta)\\)\nWe would like to estimate the model’s ability to predict data it has not seen, even though we do not observe the true data-generating process\nThis quantity is approximated by log-pointwise-predictive-density \\[\n\\text{lppd} = \\sum_{n=1}^{N} \\log \\left( \\frac{1}{S} \\sum_{s=1}^{S} f(y_n \\mid \\theta_{-n, s}) \\right)\n\\]\nTo compute this quantity, fit the model N times, dropping one point at a time (-n)\nS is the number of posterior samples, and \\(\\theta_{-n, s}\\), is the s-th sample without the n-th data point\nPSIS-LOO (Pareto Smoothed Importance Sampling, Vehtari et al, 2017) provides a computationally efficient way to approximate LOO, without refitting the model N times"
  },
  {
    "objectID": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-4",
    "href": "bayes-course/05-lecture/05-lecture.html#quantifying-model-accuracy-4",
    "title": "Bayesian Inference",
    "section": "Quantifying Model Accuracy",
    "text": "Quantifying Model Accuracy\n\nlibrary(loo)\nlm2 <- loo(m2)\nlm2\n\n\nComputed from 1200 by 544 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo  -1648.7 14.3\np_loo         3.0  0.3\nlooic      3297.3 28.6\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nplot(lm2)\n\n\n\n\nhttps://ericnovik.github.io/bayes-course.html"
  },
  {
    "objectID": "bayes-course.html",
    "href": "bayes-course.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "This is the home for APSTA-GE 2123: Bayesian Inference class at NYU.\n\nLectures\n\nThis is the current version of the syllabus: syllabus.pdf\nLecture 01: Introduction and Bayesian Workflow\nLecture 02: Conjugate Models and Beta-Binomial\nLecture 03: More Conjugate Models and Introduction to Posterior Sampling\nLecture 04: MCMC, Posterior Inference, and Prediction\nLecture 05: Linear Regression and Model Evaluation\nLecture 06: Expanding the linear model and modeling counts\nLecture 07: Logistic regression and introduction to hierarchical models\n\nThe final project is due at the end of the semester and the presentations will take place during finals week. Take a look at these guidelines, when working on your proposal and the final report.\n\n\nCode Breaking with MCMC\nThis section contains the material for the optional code-breaking project. This is an application of optimization with MCMC — we are not generating posterior draws, just the most likely alphabet mapping from cyphertext to plain text.\nThe MCMC algorithm that you need to implement is described in the Diaconis 2009 paper [1]. To simplify the task, I created the M (transition) matrix for you. You can download it in the RDS format from here. You can download the encrypted text from here. The punctuation marks are not encrypted. I suggest that you plot the value of the Pl (Plausability Function) at every iteration — it should be trending up. All the computations should be done on the log scale.\nYou should be able to read the original text after about 2,000 iterations.\n[1] Diaconis, P. (2009). The Markov chain Monte Carlo revolution. Bulletin of the American Mathematical Society, 46(2), 179–205. https://doi.org/10.1090/S0273-0979-08-01238-X"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Bayes and Stats-Math",
    "section": "",
    "text": "This website contains teaching materials for Bayesian analysis (APSTA-GE 2123) course and stats-math bootcamp, both taught at NYU. If you find any errors, please email me at eric.novik@nyu.edu."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for stopping by."
  }
]