---
title: "Bayesian Data Analysis"
format:
  html:
    theme: cosmo
    css: styles.css
    toc: true
---

![](bayes-course/dalle-fish.png){fig-align="center" width="300"}

This is the home for APSTA-GE 2123: Bayesian Inference class at NYU.

### Lectures

-   This is the current version of the syllabus: [syllabus.pdf](bayes-course/syllabus.pdf)
-   Lecture 01: [Introduction and Bayesian Workflow](bayes-course/01-lecture/01-lecture.html)
-   Lecture 02: [Conjugate Models and Beta-Binomial](bayes-course/02-lecture/02-lecture.html)
-   Lecture 03: [More Conjugate Models and Introduction to Posterior Sampling](bayes-course/03-lecture/03-lecture.html)
-   Lecture 04: [MCMC, Posterior Inference, and Prediction](bayes-course/04-lecture/04-lecture.html)
-   Lecture 05: [Linear Regression and Model Evaluation](bayes-course/05-lecture/05-lecture.html)
-   Lecture 06: [Expanding the linear model and modeling counts](bayes-course/06-lecture/06-lecture.html)
-   Lecture 07: [Logistic regression and introduction to hierarchical models](bayes-course/07-lecture/07-lecture.html)

The final project is due at the end of the semester and the presentations will take place during finals week. Take a look at [these](bayes-course/project-template.pdf) guidelines, when working on your proposal and the final report.

### Code Breaking with MCMC

This section contains the material for the optional code-breaking project. This is an application of optimization with MCMC --- we are not generating posterior draws, just the most likely alphabet mapping from cyphertext to plain text.

The MCMC algorithm that you need to implement is described in the [Diaconis 2009 paper](https://math.uchicago.edu/~shmuel/Network-course-readings/MCMCRev.pdf) \[1\]. To simplify the task, I created the M (transition) matrix for you. You can download it in the RDS format from [here](bayes-course/08-code-breaking/bigram-matrix.rds). You can download the encrypted text from [here](bayes-course/08-code-breaking/encrypted.txt). The punctuation marks are not encrypted. I suggest that you plot the value of the Pl (Plausability Function) at every iteration --- it should be trending up. All the computations should be done on the log scale.

You should be able to read the original text after about 2,000 iterations.

\[1\] Diaconis, P. (2009). The Markov chain Monte Carlo revolution. Bulletin of the American Mathematical Society, 46(2), 179--205. https://doi.org/10.1090/S0273-0979-08-01238-X
